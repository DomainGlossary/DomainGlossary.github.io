{
    "<AES encryption>": [],
    "<subprocedure>": [],
    "<cluster access>": [],
    "<txn>": [
        "For implicit txn, the stmt that triggered/started the txn is the first statement.",
        "Each statement writing data within a multi statement txn should have a unique WriteId."
    ],
    "<given label>": [],
    "<client instance>": [],
    "<single state>": [],
    "<RPC call, rpc call>": [
        "When the feature is enabled, RPC server will no longer block on the processing of RPC requests when RPC call queue is full."
    ],
    "<overwhelming number>": [],
    "<given scan>": [
        "Checks whether the given scan passes the Bloom filter (if present)."
    ],
    "<data resiliency>": [],
    "<Meetup, meetup>": [],
    "<apikey>": [],
    "<passed directory>": [],
    "<table already>": [
        "So the attempt to create the table is made even if the table already exists.",
        "-o (Optional) Overwrites the target table for the restore if the table already exists.",
        "Throw an exception if the table already has replication enabled on any of the column families."
    ],
    "<serialized bytes>": [],
    "<Deserializer, deserializer>": [
        "In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree."
    ],
    "<Max precision, max precision>": [
        "Max precision guaranteed to fit into a long."
    ],
    "<row matches>": [
        "A row matches if it has the same tag names and values as record, but it may also have additional tags."
    ],
    "<compression levels>": [],
    "<tag names>": [],
    "<expired keys>": [],
    "<BC size>": [],
    "<VLANs>": [
        "Clusters that span multiple subnets/VLANs will likely want to increase this."
    ],
    "<complex filter>": [],
    "<memory caching>": [],
    "<engine name>": [],
    "<Data Ingest>": [],
    "<serialized keys>": [],
    "<source cluster>": [
        "A source cluster is uniquely identified by the destination cluster using this id.",
        "Because we designed to write source cluster WALs and remote cluster WALs concurrently, so it's possible that the source cluster WALs has more data than the remote cluster, which result in data inconsistency.",
        "A source cluster might push changes to a destination cluster, which might also push its own changes back to the original cluster."
    ],
    "<Column Pruner>": [],
    "<retention size>": [],
    "<Retry Count, retry count>": [],
    "<key format>": [
        "The key format is <2 bytes rk len><rk><1 byte cf len><cf><qualifier><8 bytes timestamp><1 byte type>."
    ],
    "<cluster failure>": [],
    "<load cell>": [],
    "<history format>": [],
    "<Table replication, table replication>": [],
    "<unrecognized value>": [
        "An unrecognized value was silently assumed to mean authentication."
    ],
    "<JMS>": [],
    "<existing cluster>": [],
    "<Table instance, Table instances>": [
        "Table instances are not thread-safe."
    ],
    "<tgz>": [],
    "<autotools>": [],
    "<source level>": [],
    "<tag list>": [
        "Whether the tag list has a mob reference tag."
    ],
    "<mmaped>": [
        "mmaped files also use virtual address space."
    ],
    "<CPU frequency>": [],
    "<mutable number>": [
        "A mutable number optimized for high concurrency counting."
    ],
    "<Block size, block size, block sizes, blocks size>": [
        "The size of those indexes is a factor of the block size (64KB by default), the size of your keys and the amount of data you are storing.",
        "All blocks in a file except the last block are the same size, while users can start a new block without filling out the last block to the configured block size after the support for variable length block was added to append and hsync.",
        "Larger block size is preferred if files are primarily for sequential access.",
        "The block size and replication factor are configurable per file.",
        "The default blocks size prior to this change was 64MB.",
        "-p[rbugp] Preserve status r: replication number b: block size u: user g: group p: permission -p alone is equivalent to -prbugp."
    ],
    "<Oozie, oozie>": [],
    "<partition level>": [],
    "<data root>": [],
    "<Log directory, log directories, log directory, logs directory>": [
        "Fixed a bug in tasklog servlet which displayed wrong error message about job ACLs - an access control error instead of the expected log files gone error - after task logs directory is deleted."
    ],
    "<given properties>": [],
    "<CONF, Conf, Configs, conf, configration, configs, configuraiton, confs>": [
        "The conf is also passed to the file system for its configuration.",
        "If hive conf is manually created, metastore uri has to be set correctly.",
        "conf: contains the configuration files for the resource estimator service.",
        "Only configs matching any of the prefixes will be retrieved.",
        "Now when the job retires, the conf is deleted.",
        "These configs all need doc on when you'd change them.",
        "If the config is not set then default parallelism of 1 will be assumed.",
        "If confstoretrieve is specified, configs will be retrieved irrespective of whether CONFIGS is specified in fields query param or not.",
        "In cases of connection retries, conf will usually contain modified values.",
        "If that config is set, and it points to an address that is different then the RM web interface then a separate proxy server needs to be launched.",
        "This config can take values from 0 to 4.",
        "If this config is true, only pushed down filters remain in the operator tree, and the original filter is removed."
    ],
    "<skip key>": [],
    "<cell value>": [],
    "<maximum bytes>": [],
    "<mob files>": [
        "Checks if the mob file is expired.",
        "Currently, there are only two compact types: NORMAL means do store files compaction; MOB means do mob files compaction.",
        "Checks whether the referenced mob file exists.",
        "The MOB file name uses only the date part of the file creation time in it."
    ],
    "<Finish time, finish time>": [
        "Because job finish time will include the job scheduling overhead."
    ],
    "<empty response>": [],
    "<input batch>": [],
    "<attempt times>": [],
    "<byte allowed>": [],
    "<Input Key, Input key, Input keys, input key, input keys>": [
        "Input keys and values are swapped.",
        "Input keys must not be altered."
    ],
    "<symmetric key>": [],
    "<backticks>": [],
    "<Crypto, crypto>": [],
    "<Session identifier, session identifier>": [],
    "<memory sizes>": [],
    "<cmdline>": [],
    "<Table procedures>": [],
    "<partition list>": [],
    "<history store>": [],
    "<CPU>": [
        "Once resources (CPU, memory, etc.) have been obtained from YARN for a specific workload, the execution engine can choose to delegate these resources to LLAP, or to launch Hive executors in separate processes."
    ],
    "<Anonymizer>": [
        "Anonymizer takes a Rumen trace file and/or topology as input."
    ],
    "<log replication>": [],
    "<load generator>": [
        "The Synthetic Load Generator complements the extensive nature of SLS-native and RUMEN traces, by providing a distribution-driven generation of load."
    ],
    "<Typed bytes, typed bytes>": [
        "Typed bytes are sequences of bytes in which the first byte is a type code."
    ],
    "<tasktracker>": [
        "Get the tasktracker expiry interval for the cluster.",
        "of task failures exceeds this, the tasktracker is blacklisted for this job."
    ],
    "<output descriptor>": [],
    "<Delete family>": [],
    "<given objects>": [],
    "<client version>": [],
    "<Procedure code>": [
        "Execute may be called multiple times in the case of failure or restart, so Procedure code must be idempotent yielding the same result each time it run."
    ],
    "<ANDs>": [],
    "<disabled table, disabling table>": [],
    "<array size>": [],
    "<Hardware access>": [],
    "<rsgroup>": [
        "All rsgroups, but the default rsgroup, are static in that edits via the shell commands are persisted to the system hbase:rsgroup table.",
        "default rsgroup created implicitly doesn't have to be removed."
    ],
    "<Deleted files, deleted files>": [
        "Furthermore, the deleted files will continue to incur storage costs.",
        "Deleted files will remain encrypted and they will be moved to a \".Trash\" subdirectory under the root of the encryption zone, prefixed by $USER/current."
    ],
    "<Splits files>": [],
    "<Column Type, column type, column types>": [
        "Things can go wrong if the bucketing column type is different during the insert and on read, or if you manually cluster by a value that's different from the table definition."
    ],
    "<Thrift service, thrift service>": [],
    "<given capability>": [],
    "<rename files>": [],
    "<storage overhead>": [],
    "<reasonable time>": [],
    "<bits directory>": [],
    "<memory reserved>": [],
    "<memory info>": [],
    "<upgradable>": [],
    "<data replicated>": [],
    "<row lock>": [],
    "<health checker>": [
        "The health checker script is not supposed to give ERROR if only some of the local disks become bad."
    ],
    "<tmpfs>": [],
    "<key version>": [
        "Because keys can be rolled, a key can have multiple key versions, where each key version has its own key material (the actual secret bytes used during encryption and decryption)."
    ],
    "<single scan>": [],
    "<group level>": [],
    "<defined name>": [],
    "<Apriori>": [],
    "<stop row>": [],
    "<article code>": [],
    "<Max time, max time>": [],
    "<Exit codes, exit codes>": [
        "Previously the exit code was success.",
        "Non 0 exit code indicates failure.",
        "If unsuccessful, exit codes are.",
        "Exit code when a control-C, kill -3, signal was picked up: 3."
    ],
    "<HTTP Tools>": [],
    "<retry period>": [],
    "<data encryption key>": [],
    "<GUID, guid>": [],
    "<short version>": [],
    "<composite key>": [],
    "<bulkload>": [],
    "<cacheable>": [
        "SHARED means when this Cacheable is read back from cache it refers to the same memory area as used by the cache for caching it."
    ],
    "<local aggregation>": [],
    "<databind>": [],
    "<site files>": [],
    "<data resides>": [],
    "<state name>": [],
    "<short order, sorting order>": [],
    "<safemode>": [],
    "<header byte>": [
        "It uses 0x38 as the header byte, and is terminated by 0x00 in the DESCENDING case."
    ],
    "<integer metric>": [],
    "<block keys>": [],
    "<schemaless>": [],
    "<specified rack>": [],
    "<data processed>": [],
    "<compares size>": [],
    "<Token cache, token cache>": [],
    "<group key, grouping keys>": [
        "If the grouping key is a superset of the bucketing and sorting keys of the underlying table in the same order, the group by can be be performed on the map-side completely."
    ],
    "<imply order>": [],
    "<output specification>": [],
    "<Undefined state>": [],
    "<constuctor>": [],
    "<maximum delay>": [],
    "<ldd>": [],
    "<retrieved row>": [],
    "<dbname>": [],
    "<time ordering>": [],
    "<memory limit, memory limits>": [
        "Now, different memory limits can be set for map and reduce tasks of a job, in MB."
    ],
    "<specified qualifier>": [],
    "<Table Descriptor, Table descriptor, table descriptor, table descriptors>": [
        "Check that the table descriptor for the snapshot is a valid table descriptor.",
        "Construct a table descriptor by cloning the descriptor passed as a parameter."
    ],
    "<Node Cluster, node cluster>": [
        "Here is an example basic configuration for a distributed ten node cluster: * The nodes are named example0, example1, etc., through node example9 in this example."
    ],
    "<AES key>": [],
    "<memory shuffle>": [],
    "<alloc>": [],
    "<data directory>": [],
    "<AWS, Amazon Web Services, aws>": [
        "AWS S3 endpoint to connect to.",
        "When the V4 signing protocol is used, AWS requires the explicit region endpoint to be used \u2014hence S3A must be configured to use the specific endpoint.",
        "AWS access key ID used by S3A file system."
    ],
    "<local rate>": [],
    "<maximum count>": [],
    "<groupname>": [],
    "<column table>": [],
    "<long metric>": [],
    "<Primary Filter, Primary filter, Primary filters, primary filter, primary filters>": [],
    "<QA, qa>": [],
    "<Retry caller, retry caller>": [],
    "<side cache>": [
        "Both client side and server side cache needs to be a singleton and shared within the JVM.",
        "In the worst case, the caller may get up to (server-side cache size + client-side cache size) number of old EEKs, or until both caches expire."
    ],
    "<Moves files, Moving files, move files, moving files>": [],
    "<given List, given list>": [],
    "<encrypted zone>": [],
    "<Minimum period>": [
        "Minimum period of time (in milliseconds) to keep metadata (may only be applied when a prune command is manually run)."
    ],
    "<service state>": [
        "The service state is checked before the operation begins."
    ],
    "<LZO, Lzo, lzo>": [
        "LZO is not bundled as part of the hbase distribution.",
        "Snappy and LZO use fewer CPU resources than GZIP, but do not provide as high of a compression ratio.",
        "Before Snappy became available by Google in 2011, LZO was the default."
    ],
    "<Type requested>": [],
    "<empty memory>": [],
    "<associated directory>": [],
    "<error allowed>": [],
    "<double format>": [],
    "<ALTER TABLE, Alter Table, Alter table, alter table>": [
        "ALTER TABLE ADD|REPLACE COLUMNS with CASCADE command changes the columns of a table's metadata, and cascades the same change to all the partition metadata.",
        "For eg: alter table Tdependent add partition (ds='1') depends on table T1 partition (ds='1'); Something that can be achieved by external tables currently.",
        "By signaling an error via this message, the table is left in a good state and the incorrect value can be corrected with a call to alter table T set TBLPROPERTIES."
    ],
    "<keys list>": [
        "If its keys list is size = 0, then this is a cross product."
    ],
    "<Table cardinality>": [],
    "<maximum put>": [],
    "<table indexes>": [],
    "<Checkstyle, checkstyle>": [],
    "<Token Identifier, token identifier>": [],
    "<eviction time>": [],
    "<source paths>": [],
    "<separate directory>": [],
    "<table permission, table permissions>": [
        "Updates the internal table permissions cache for specified table."
    ],
    "<memory lock>": [],
    "<compacted files>": [],
    "<retry forever>": [],
    "<less number>": [],
    "<Memory load, memory load>": [],
    "<metric level>": [],
    "<results list>": [
        "The results list can contain expressions based on the input columns and also the matched Path."
    ],
    "<given subject>": [],
    "<ANTLR, antlr>": [],
    "<small blocks>": [],
    "<REST call>": [],
    "<Encode values, encoded values>": [],
    "<CORS>": [],
    "<HRI>": [],
    "<share files>": [],
    "<bytes key>": [
        "When a bytes key appears in the small table results area, instead of copying the bytes key we reference the big table key."
    ],
    "<Rack name, rack name>": [],
    "<cache usage>": [],
    "<encryption status>": [],
    "<root directory>": [
        "The root directory, \"/\", is always a directory, and cannot be overwritten by a file write operation.",
        "The root directory, \"/\", always exists, and cannot be renamed."
    ],
    "<Multi Table>": [],
    "<running cluster>": [],
    "<table compaction>": [],
    "<Predicate Pushdown, Predicate pushdown, predicate pushdown>": [
        "Predicate pushdown is a term borrowed from relational databases even though for Hive it is predicate pushup."
    ],
    "<Memory needed, memory needed>": [],
    "<group row>": [],
    "<remote node, remote nodes>": [],
    "<servlet filter>": [],
    "<Cached list>": [],
    "<Local metastore>": [],
    "<analysis time>": [],
    "<Local memory, local memory>": [],
    "<checkout directory>": [],
    "<entire filter>": [],
    "<table state>": [
        "Set table state to provided.",
        "Table locking from master prevents concurrent schema modifications to corrupt table state."
    ],
    "<asynchronous table>": [],
    "<lock table>": [
        "Set table state to provided but only if table in specified states Caller should lock table on write."
    ],
    "<stripe store>": [],
    "<Rackspace>": [],
    "<memory statistics>": [],
    "<local source>": [],
    "<uber>": [],
    "<remote service>": [],
    "<given servers>": [],
    "<output name>": [
        "The names of the counters are the same as the output name.",
        "Set the base output name for output file to be created."
    ],
    "<skipped files>": [],
    "<xdoc, xdocs>": [],
    "<connection configuration>": [],
    "<PPID>": [],
    "<HTTP equivalent>": [],
    "<given metastore>": [],
    "<Add value, add value>": [],
    "<HTTP Operations>": [],
    "<single location>": [],
    "<client scanner>": [
        "A client scanner for a region opened for read-only on the client side."
    ],
    "<Patch name>": [
        "Patch name should be as follows to adhere to Yetus' naming convention.",
        "Patch name is formatted as (JIRA).(branch name).(patch number).patch to follow Yetus' naming rules."
    ],
    "<Hash aggregation, hash aggregation>": [
        "Hash aggregation is an optimization in hive to reduce the number of rows shuffled between map and reduce stage.",
        "The number of rows emitted from map-side will vary if hash aggregation is enabled throughout execution or disabled."
    ],
    "<Decrease rate>": [],
    "<Access restriction, access restriction>": [],
    "<Table Creation, table creation>": [
        "Unpartitioned tables effectively have one default partition that must be created at table creation time."
    ],
    "<given peer>": [],
    "<machine cluster>": [],
    "<sequential number>": [],
    "<service length>": [],
    "<Apache Software License>": [],
    "<Compare key, compare key, compare keys>": [],
    "<raw scan>": [],
    "<Table basis, table basis>": [],
    "<LevelDB, leveldb>": [
        "Default leveldb read cache size if no configuration is specified."
    ],
    "<NFS, nfs>": [
        "When the user on NFS client accesses the mount point, NFS client passes the UID to NFS gateway.",
        "NFS gateway does a lookup to find user name from the UID, and then passes the username to the HDFS along with the HDFS requests.",
        "NFS client often reorders writes, especially when the export is not mounted with \"sync\" option.",
        "By default, NFS gateway supports 1MB as the maximum transfer size."
    ],
    "<REST>": [
        "REST itself is out of the scope of this documentation, but in general, REST allows client-server interactions via an API that is tied to the URL itself.",
        "Representational State Transfer (REST) was introduced in 2000 in the doctoral dissertation of Roy Fielding, one of the principal authors of the HTTP specification.",
        "(REST stands for \"representational state transfer\", a style of API based on HTTP verbs)."
    ],
    "<timer value>": [],
    "<single domain>": [],
    "<button name>": [],
    "<index path>": [],
    "<GPG, gpg>": [
        "In case the GPG is not-available, cluster operations will continue as of the last time the GPG published policies, and while a long-term unavailability might mean some of the desirable properties of balance, optimal cluster utilization and global invariants might drift away, compute and access to data will not be compromised.",
        "The GPG operates continuously but out-of-band from all cluster operations, and provide us with a unique vantage point, that allows to enforce global invariants, affect load balancing, trigger draining of sub-clusters that will undergo maintenance, etc."
    ],
    "<key deletion>": [],
    "<JVM, JVMs, jvms>": [
        "The usual cause is that their JVM is untuned and they are running into long GC pauses.",
        "More data is being generated than in the JVM than it can upload to S3 \u2014and so much data has been buffered that the JVM has run out of memory.",
        "Symptoms of this problem include JVM crashes with a stack trace inside these functions.",
        "Amount of time the JVM spent in garbage collection while executing tasks.",
        "A JVM without the Java Cryptography Extensions installed does not support such a key length.",
        "JVM metrics published to Ganglia now include the process name as part of the gmetric name.",
        "Make sure you don't swap, the JVM never behaves well under swapping.",
        "If the JVM does not support this length, the command will fail."
    ],
    "<Batch size, batch size>": [
        "The batch size and decaying factor are provided with the constructor.",
        "The rule of thumb is that, the smaller the number of cells out of sync (lower probability of finding a diff), larger batch size values can be determined.",
        "During hive streaming connection creation, transaction batch size can be specified via builder API."
    ],
    "<service key>": [],
    "<Resource reserved>": [
        "Resource reserved for the allocation."
    ],
    "<NULL fields, null fields>": [],
    "<access result>": [],
    "<RTRIM>": [],
    "<cluster level>": [],
    "<TABLE T1, table T1, table t1>": [],
    "<key prefix>": [
        "The key prefix is defined as everything preceding the task ID in the key.",
        "During stats publishing phase, this key prefix will be appended with the optional dynamic partition spec and the task ID."
    ],
    "<metrics reporting>": [],
    "<real cluster>": [],
    "<pentaho>": [],
    "<storage media>": [],
    "<disk blocks>": [],
    "<max period>": [
        "Also note that the configured max period must be divisible by the recurrence expression if expressed as a long."
    ],
    "<parametrized>": [],
    "<update request>": [
        "If a new update request is sent for a container (in a subsequent allocate call) before the first one is satisfied by the Scheduler, it will overwrite the previous request."
    ],
    "<DFS, dfs, dfsadmin>": [
        "If HDFS is being used, hdfs dfs is a synonym.",
        "The dfsadmin -metasave command has been changed to overwrite the output file.",
        "Such issues can occur when DFS clients were closed etc.",
        "If it is set to zero, DFS ZKFC won't get local NN thread dump."
    ],
    "<intermediate key>": [],
    "<context state>": [],
    "<table definition>": [
        "Called before a new table definition is added to the metastore during CREATE TABLE.",
        "Called before a table definition is removed from the metastore during DROP TABLE."
    ],
    "<token store>": [],
    "<Metastore key>": [],
    "<right number>": [
        "The first byte is interpreted as a type code, and then the right number of subsequent bytes are read depending on the obtained type."
    ],
    "<proper values>": [],
    "<Export Table, Export table, export table>": [],
    "<specified Cell, specified cell, specified cells>": [],
    "<given metric>": [],
    "<table pool>": [],
    "<CFD>": [],
    "<ALTER>": [
        "ALTER TABLE ADD|REPLACE COLUMNS with CASCADE command changes the columns of a table's metadata, and cascades the same change to all the partition metadata.",
        "However, ALTER INDEX requires an index name that was created with lowercase letters (see HIVE-2752)."
    ],
    "<input vectors>": [],
    "<Templeton name>": [
        "The Templeton name is taken from a character in the award-winning children's novel Charlotte's Web, by E."
    ],
    "<Retry interval, retry interval>": [],
    "<high memory>": [],
    "<Open call, open call>": [],
    "<skippable>": [],
    "<WAL, WALs>": [
        "Register WAL files as eligible for deletion.",
        "Skips WAL edits for all System tables including META.",
        "This causes the WAL to be a performance bottleneck.",
        "If deferred log flush is used, WAL edits are kept in memory until the flush period.",
        "By default, WAL tag compression is turned on when WAL compression is enabled.",
        "As bulk loading bypasses the write path, the WAL doesn't get written to as part of the process.",
        "WALs are retained when enabling or disabling replication as long as peers exist.",
        "The WALs for each region server must be kept in HDFS as long as they are needed to replicate data to any slave cluster.",
        "The WAL is filtered to this set of tables.",
        "Already-processed WALs are stored in /hbase/oldWALs/ and corrupt WALs are stored in /hbase/.corrupt/ for examination.",
        "The WAL can be replayed for a set of tables or all tables, and a timerange can be provided (in milliseconds)."
    ],
    "<side files>": [
        "When file permissions are thwarted, unlike \"side files\", there are no standard tools that can expose the protected credentials - even with the password known."
    ],
    "<Metrics related, metrics related>": [
        "The following metrics have been removed: Metrics related to the Distributed Log Replay feature are no longer present."
    ],
    "<Backport, Backported, Backports, backport, backported, backports>": [],
    "<Service instances, service instances>": [
        "A service instance is running if the instances the components which for the service are running.",
        "Core Hadoop service instances must be able to register their service endpoints.",
        "The registry will allow a service instance can only be registered under the path where it has permissions.",
        "Since coprocessor Service instances are associated with individual regions within the table, the client RPC calls must ultimately identify which regions should be used in the Service method invocations.",
        "These service instances need to be discovered by clients; traditionally their IP added is registered in DNS or in some configuration file \u2014but that is not feasible in YARN-deployed applications when neither the hostname nor network ports can be known in advance.",
        "Each Hadoop Service instance must be configured with its Kerberos principal and keytab file location.",
        "The service instance could not be created: 57."
    ],
    "<Checksum verification, checksum verification>": [
        "Checksum verification by HDFS will be internally disabled on hfile streams when this flag is set."
    ],
    "<expiry interval>": [],
    "<Split identifier>": [],
    "<Master Key, master key, master keys>": [],
    "<restore request>": [],
    "<length block>": [
        "All blocks in a file except the last block are the same size, while users can start a new block without filling out the last block to the configured block size after the support for variable length block was added to append and hsync."
    ],
    "<hash key>": [],
    "<given family>": [],
    "<Protos, protoc, protos>": [
        "Protos are hosted by the module that makes use of them."
    ],
    "<FLOAT code>": [],
    "<DistCP, Distcp, distcp>": [
        "DistCp now has a \"-basedir\" option that allows you to set the sufix of the source path that will be copied to the destination.",
        "Distcp will launch a mapreduce job to handle copying the files in a distributed fashion.",
        "Legacy DistCp works by figuring out what files need to be actually copied to target before the copy-job is launched, and then launching as many maps as required for copy.",
        "Note: Distcp works in this situation because the cluster is down and there are no in-flight edits to files.",
        "DistCp may alternatively be sub-classed to fine-tune behaviour.",
        "Rather than to permit this conflict, DistCp will abort.",
        "DistCp is the main driver-class for DistCpV2.",
        "(Also, in this example, the distcp should be run using the configuraton of the new filesystem.).",
        "Distcp will no longer start jobs that move no data.",
        "By default, DistCp makes an attempt to size each map comparably so that each copies roughly the same number of bytes.",
        "When DistCp -update is used with object stores, generally only the modification time and length of the individual files are compared, not any checksums.",
        "DistCp can be used to upload data."
    ],
    "<Puzzle files, puzzle files>": [
        "Puzzle files have a line per a row and columns separated by spaces."
    ],
    "<next cell>": [],
    "<Secure Cluster, secure cluster, secure clusters>": [],
    "<given types>": [
        "Check if the given type is numeric."
    ],
    "<progress fields>": [],
    "<FilterPPR filter>": [],
    "<large table, larger table, largest table>": [
        "If the larger table is huge, there will be thousands of Mapper launched to read different records of the larger table."
    ],
    "<tracking files>": [],
    "<integer size>": [],
    "<total memory>": [],
    "<Partition cluster>": [
        "Partition cluster - each node can be assigned one label, so the cluster will be divided to several smaller disjoint partitions."
    ],
    "<Storefile size>": [
        "Archived Storefile Size is the Storefile size in Archive.",
        "Shared Storefile Size is the Storefile size shared between snapshots and active tables."
    ],
    "<HTTP connections>": [],
    "<memory attribute>": [],
    "<signal table>": [],
    "<fileID>": [],
    "<Execution Types>": [
        "If there are available resources, the execution of the container starts immediately, irrespective of its execution type."
    ],
    "<cluster layout>": [],
    "<specified keys>": [
        "Check if the specified key is contained in the bloom filter.",
        "Determines wether a specified key belongs to this filter.",
        "Invariant: nothing happens if the specified key does not belong to this counter Bloom filter."
    ],
    "<SessionID, sessionid>": [],
    "<Management Committee>": [],
    "<checksum type>": [
        "The checksum type is automatically inferred by issuing a read of the first byte of each block."
    ],
    "<lock request>": [
        "The lock request is queued with other lock requests.",
        "Note that instead of waiting, the lock request will be denied."
    ],
    "<quickstart>": [
        "The architecture will be as follows: This quickstart assumes that each node is a virtual machine and that they are all on the same network."
    ],
    "<Web link, web link>": [],
    "<TTLs>": [
        "Cell TTLs are submitted as an attribute on mutation requests (Appends, Increments, Puts, etc.) using Mutation#setTTL."
    ],
    "<JSP, jsp>": [
        "Fixed null pointer error when quoting HTML in the case JSP has no parameters."
    ],
    "<output directories, output directory>": [
        "Output is written to the given output directory.",
        "check that the output directory doesn't already exist.",
        "Changed Map-Reduce framework to no longer create temporary task output directories for staging outputs if staging outputs isn't necessary."
    ],
    "<threshold size>": [],
    "<end store>": [
        "The back end store is responsible for maintaining and persisting metadata about the shared cache."
    ],
    "<Kerberized, kerberized>": [],
    "<cache directory>": [
        "Ensure that the shared cache directory is owned by the user that runs the shared cache manager daemon and the node manager."
    ],
    "<storage table>": [],
    "<read errors>": [
        "If the configuration is false, read errors are treated similar to connection errors."
    ],
    "<DFW>": [],
    "<increased number>": [],
    "<violation message>": [],
    "<Active state, active state>": [],
    "<cluster specified>": [],
    "<grouping columns>": [],
    "<automake>": [],
    "<access key, access keys>": [
        "The access key is a secret that protects access to your storage account."
    ],
    "<hlog>": [],
    "<dryrun>": [],
    "<created time>": [],
    "<data encoded>": [],
    "<Structural Filters>": [
        "Structural Filters contain other Filters."
    ],
    "<ASF, Apache Software Foundation, asf>": [
        "The Apache Software Foundation defines generic guidelines for what it means to be a committer.",
        "See ASF board reporting for more information."
    ],
    "<HEAD request>": [],
    "<path array>": [],
    "<hcat, hcatalog>": [],
    "<local repository>": [],
    "<input folders>": [],
    "<particular table>": [],
    "<Cloudera>": [
        "Cloudera provides some AMIs that bundle Hive with Hadoop - although the choice in terms of Hive and Hadoop versions may be restricted.",
        "Cloudera will take the lead on that.",
        "Cloudera will host the next meeting."
    ],
    "<repeating values>": [],
    "<Data Node, Data Nodes, data node, data nodes>": [
        "Failure and recovery is simplified because any data node can still be used to process any fragment of the input data.",
        "This is useful when data node decommissioning is blocked by slow writers.",
        "A stale data node is avoided during lease/block recovery.",
        "The Data Nodes will flush in-memory data to disk asynchronously thus removing expensive disk IO and checksum computations from the performance-sensitive IO path, hence we call such writes Lazy Persist writes.",
        "Following a scheduled reboot, one data node began exhibiting unusual behavior."
    ],
    "<snapshot files>": [
        "The snapshot should not be deleted while there are jobs reading from snapshot files.",
        "Blocks in datanodes are not copied: the snapshot files record the block list and the file size."
    ],
    "<purge time>": [],
    "<encoding time>": [],
    "<ppr>": [],
    "<wait period>": [
        "Such wait period gives RM a chance to settle down resyncing with NMs in the cluster on recovery, before assigning new containers to applications."
    ],
    "<executable directory>": [],
    "<jruby>": [],
    "<FLOAT>": [],
    "<data columns>": [],
    "<disk storage>": [],
    "<failure number>": [],
    "<local node>": [],
    "<single constant>": [],
    "<retry cache>": [],
    "<mongodb>": [],
    "<long identifier>": [],
    "<table snapshot>": [],
    "<estimation size>": [],
    "<WASB>": [
        "WASB does not enforce this guarantee internally.",
        "WASB can operate in secure mode where the Storage access keys required to communicate with Azure storage does not have to be in the same address space as the process using WASB.",
        "WASB passes User-Agent header to the Azure back-end.",
        "The Azure Blob Storage file system (WASB) now includes optional support for use of the append API by a single writer on a path."
    ],
    "<cgroup, cgroups>": [],
    "<datablock>": [],
    "<DATA, Datanode, Datanodes, datanode, datanodes>": [
        "Setting this to 0 fails requests right away if the datanode is not yet registered with the namenode.",
        "Any other value will throw an exception when namenode and datanodes are starting up.",
        "the datanode will be transitioned to DECOMMISSIONED state.",
        "HDFS datanodes simply see a stream of encrypted bytes.",
        "Check if all datanodes are healthy after restart.",
        "Repeat the above steps until all datanodes in the cluster are upgraded.",
        "Previously a datanode resigned if any volume failed.",
        "HDFS Rolling Upgrade document explains how the datanodes can be upgraded in a rolling fashion without downtime.",
        "Each element maps to one datanode and each datanode can have multiple properties.",
        "After the timeout, the datanode will be transitioned out of maintenance state automatically by HDFS without human intervention.",
        "The datanode now performs 4MB readahead by default when reading data from its disks, if the native libraries are present.",
        "Otherwise, datanode will be registered and the default rack will be assigned as the topology path.",
        "This means all datanodes belonging to a specific upgrade domain collectively won't store more than one replica of any block.",
        "On startup, the datanode will automatically upgrade it's storages to this new layout.",
        "Datanodes send periodic heartbeats and block reports.",
        "Datanode can continue if a volume for replica storage fails.",
        "Upon startup, the datanode will automatically change the permissions to match the configured value.",
        "Stale datanodes are avoided, and marked as the last possible target for a read or write operation.",
        "This command can be used for checking if a datanode is alive like the Unix ping command.",
        "The Datanodes are started on the nodes specified in the slaves file.",
        "Simulated Datanodes should not include blocks that are still being written in their block report.",
        "If the DATA blocks fit inside fscache, this alternative may make sense when access is completely random across a very large dataset.",
        "When all the Namenodes finish decommissioning a Datanode, the Datanode is considered decommissioned.",
        "Datanode will report to all the nameservices in this list."
    ],
    "<Findbugs, findbugs>": [
        "Findbugs is used to detect common bugs pattern."
    ],
    "<byte space>": [],
    "<data copies>": [],
    "<output filter>": [],
    "<status entries>": [],
    "<data accessed>": [],
    "<Join keys, join keys>": [
        "Join keys are expressions based on the select operator."
    ],
    "<Write table>": [],
    "<memory errors>": [],
    "<Data moved>": [
        "Data moved to the .Trash directory can be purged using the expunge command."
    ],
    "<mkdirs>": [],
    "<home path>": [],
    "<replication number>": [
        "-p[rbugp] Preserve status r: replication number b: block size u: user g: group p: permission -p alone is equivalent to -prbugp."
    ],
    "<PCI>": [],
    "<Start time, start time, start times>": [
        "Suppose time T is the rolling upgrade start time and the upgrade is terminated by downgrade."
    ],
    "<maximum bandwidth>": [],
    "<export numbers>": [],
    "<final name>": [],
    "<filter type>": [],
    "<single Region>": [],
    "<long counter>": [],
    "<single stripe>": [],
    "<flush time>": [],
    "<metric size>": [],
    "<POJO>": [],
    "<replication state>": [],
    "<YARN Service, YARN service>": [
        "The YARN service registry is built on top of Apache Zookeeper.",
        "Any YARN service intended to run for an extended period of time must have a strategy for renewing credentials."
    ],
    "<error property>": [],
    "<concurrent requests>": [],
    "<table lookup>": [],
    "<typed fields>": [],
    "<storage nodes>": [],
    "<entire cluster>": [
        "Thus if you have 2 regions for 16GB data, on a 20 node machine your data will be concentrated on just a few machines - nearly the entire cluster will be idle."
    ],
    "<YARN Registry, YARN registry, Yarn Registry, yarn registry>": [],
    "<sorting keys>": [],
    "<sequence response>": [],
    "<sname>": [],
    "<ALTER INDEX>": [
        "However, ALTER INDEX requires an index name that was created with lowercase letters (see HIVE-2752)."
    ],
    "<byte overhead>": [],
    "<relational store>": [],
    "<Storage Formats, Storage formats, storage format, storage formats>": [],
    "<global number>": [],
    "<resource value>": [
        "This method assumes resource value is positive."
    ],
    "<Reduce number>": [],
    "<Table Discovery, table discovery>": [],
    "<column comparison>": [],
    "<prefix tree>": [
        "Prefix Tree may be appropriate for applications that have high block cache hit ratios."
    ],
    "<given info>": [],
    "<chgrp>": [
        "Regardless of whether permissions are on or off, chmod, chgrp, chown and setfacl always check permissions."
    ],
    "<input trace>": [
        "Both generation of Distributed Cache files and emulation of Distributed Cache load are disabled if: input trace comes from the standard input-stream instead of file, or."
    ],
    "<TRACE>": [],
    "<IAM>": [],
    "<Field numbers>": [
        "Field numbers are cheap and changing and reusing is not a good idea."
    ],
    "<YARN Application, YARN application, yarn application>": [
        "A YARN application is deployed.",
        "An YARN application may be passed the token to connect with the ZK service when launched.",
        "YARN supports a rudimentary registry which allows YARN Application Masters to register a web URL and an IPC address.",
        "To execute code in the cluster, a YARN application must: A list of files in the cluster's filesystem to be \"localized\"."
    ],
    "<attempt directory>": [],
    "<Data Lake, data lake>": [
        "Azure data lake does not support user configuration for data replication hence not leaving system to query on azure data lake."
    ],
    "<emps>": [],
    "<data aggregation>": [],
    "<Joda, joda>": [],
    "<exclude pattern>": [],
    "<Queue name, queue name>": [
        "In the examples below, <queue> is the queue name.",
        "Name of the queue to which jobs will be submitted, if no queue name is mentioned."
    ],
    "<cell level>": [],
    "<umount>": [],
    "<Policy Store>": [],
    "<standy>": [],
    "<machine level>": [],
    "<Huai>": [],
    "<local cluster>": [],
    "<Dist, dist>": [],
    "<inline code>": [],
    "<del files>": [],
    "<generation time>": [],
    "<column range>": [],
    "<start row>": [],
    "<distant cluster>": [],
    "<key range>": [],
    "<area directory>": [],
    "<Unspecified values>": [],
    "<output storage>": [],
    "<monitor interval>": [],
    "<LZ4, lz4>": [
        "LZ4 support is bundled with Hadoop."
    ],
    "<Bloom Block, Bloom block>": [],
    "<OLAP>": [],
    "<data age>": [],
    "<statistics values>": [],
    "<bytes arrays>": [],
    "<global filters>": [],
    "<Junit, junit>": [],
    "<storage efficiency>": [],
    "<cluster manager>": [],
    "<correct format>": [],
    "<join order>": [],
    "<MYSQL, MySQL, mysql>": [
        "However, many DBMS's ignore this rule; for example, MySQL allows ORDER BY, but ignores it in the case where it is superceded by an ORDER BY in the query."
    ],
    "<Storage service, storage service>": [],
    "<service version>": [],
    "<per time>": [],
    "<storage mechanisms>": [],
    "<Input Table, Input table, input table, input tables>": [
        "The other input tables will be recognize as the small tables during the execution stage and these tables need to be held in the memory."
    ],
    "<data grows>": [],
    "<group value>": [],
    "<anchor key>": [],
    "<retry counter>": [],
    "<Cached bytes>": [],
    "<YARN Security>": [],
    "<key retrieval>": [],
    "<Delete Table, Delete table, delete Table, delete table>": [],
    "<web response>": [],
    "<Mount table>": [
        "The mount table entries are pretty much the same as in ViewFs.",
        "Mount Table: This table hosts the mapping between folders and subclusters.",
        "Mount table permission can be set by following command: The option mode is UNIX-style permissions for the mount table.",
        "The mount table is read when the job is submitted to the cluster.",
        "It is recommanded that the mount table of a cluster should be named by the cluster name.",
        "Mount table have UNIX-like permissions, which restrict which users and groups have access to the mount point.",
        "Mount tables support ACL, The users won't be able to modify their own entries (we are assuming these old (no-permissions before) mount table with owner:superuser, group:supergroup, permission:755 as the default permissions)."
    ],
    "<deprecated name>": [
        "If name is deprecated or there is a deprecated name associated to it, it sets the value to both names."
    ],
    "<output column>": [
        "All the input parameters and output column types are string.",
        "In this variation, column information is ordered by the output column number."
    ],
    "<maximum heap>": [],
    "<nameservice>": [],
    "<specific Table, specific table>": [],
    "<lexical error>": [],
    "<minimum allocation>": [
        "Allocations below that are padded to minimum allocation."
    ],
    "<Heap usage, heap usage>": [
        "Adds cumulative cpu usage and total heap usage to task counters.",
        "As a result, the actual heap usage of the memstore before being flushed may increase by up to 100%.",
        "Heap usage emulator is designed in such a way that it only emulates at specific progress boundaries of the task."
    ],
    "<Column Statistics, Column statistics, column statistics>": [
        "Column statistics are fetched from the metastore.",
        "Based on the provided column statistics and number of rows, this method infers if the column can be primary key.",
        "However, the base column statistics is not cached and needs to fetch from SQL database everytime needed.",
        "Basic and column statistics can have one of the following states COMPLETE, PARTIAL, NONE."
    ],
    "<web filters>": [],
    "<rebase>": [],
    "<data irrespective>": [],
    "<Group information, group info, group information>": [],
    "<SVN, svn>": [
        "Run svn add for whatever new files you have created.",
        "(The svn workflow does not support the --preview option.).Updating Patches (svn).At this point, reviewers can add comments and request changes."
    ],
    "<hsync>": [],
    "<Storage information, storage information>": [
        "Storage information includes location of the underlying data, file inout and output formats and bucketing information."
    ],
    "<Heartbeat message, Heartbeat messages, heartbeat message, heartbeat messages>": [
        "Heartbeat checks occur during the processing of scans to determine whether or not the server should stop scanning in order to send back a heartbeat message to the client.",
        "Heartbeat messages are used to keep the client-server connection alive during long running scans."
    ],
    "<flush request>": [],
    "<snapshot information>": [],
    "<striped files>": [],
    "<Metrics Subsystem, metrics subsystem>": [],
    "<diagnositic>": [],
    "<temp table>": [
        "This has to be done after the temp table is populated and all necessary Partition objects exist in the metastore."
    ],
    "<limit count>": [],
    "<Reference files, reference files, referenced files, referencing files>": [
        "Those reference files will point to the parent region's files.",
        "Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further.",
        "The reference file is used just like a regular data file, but only half of the records are considered."
    ],
    "<output privileges>": [],
    "<topic name>": [],
    "<untar>": [],
    "<checksum information>": [],
    "<check time>": [],
    "<key fields>": [],
    "<failure status>": [],
    "<chunked value>": [],
    "<cluster state>": [],
    "<metrics filter>": [],
    "<Apps, appID, appid, apps>": [
        "Such apps will not be retried by the RM on app attempt failure.",
        "Once app is created, note down the \"Appplication ID\" of the app.",
        "This is useful when the app only wants to aggregate logs of a subset of containers.",
        "When other apps are submitted, resources that free up are assigned to the new apps, so that each app eventually on gets roughly the same amount of resources.",
        "user: the app is placed into a queue with the name of the user who submitted it.",
        "Once app is created, go to \"keys\" under \"settings\" for the app.",
        "This could be null if no app is associated.",
        "If \"fifo\", apps with earlier submit times are given preference for containers, but apps submitted later may run concurrently if there is leftover space on the cluster after satisfying the earlier app's requests.",
        "You can confirm that the app is killed by repeating the PUT request until you get a 200, querying the state using the GET method or querying for app information and checking the state.",
        "Valid rules are: specified: the app is placed into the queue it requested.",
        "Unlike the default Hadoop scheduler, which forms a queue of apps, this lets short apps finish in reasonable time while not starving long-lived apps.",
        "If an app specifically lists a queue in a container resource request, the request is submitted to that queue.",
        "When there is a single app running, that app uses the entire cluster.",
        "default: the app is placed into the queue specified in the 'queue' attribute of the default rule.",
        "App should not make concurrent allocate requests.",
        "If the app cannot use the container or wants to give up the container then it can release them.",
        "So even after the remove request the app must be prepared to receive an allocation for the previous request even after the remove request."
    ],
    "<changed files>": [
        "Otherwise, changed files may be missed/copied too often."
    ],
    "<side metrics>": [],
    "<regular time>": [],
    "<match code>": [],
    "<NLM>": [
        "NLM is not supported so mount option \"nolock\" is needed."
    ],
    "<Column Prefixes, column prefix, column prefixes>": [],
    "<output result, output results>": [
        "At the same time, some output results need to be formed in the overflow batch."
    ],
    "<Table Sample>": [],
    "<expiry time>": [],
    "<constant true>": [],
    "<HTTP, Httpclient, httpclient>": [
        "HTTP POST is used for configuring the loggers.",
        "The HTTP result code, response headers, and body of a HTTP response.",
        "HttpFS can be used to access data in HDFS using HTTP utilities (such as curl and wget) and HTTP libraries Perl from other languages than Java.",
        "This HTTP Cookie has an expiration time, after which it will trigger a new authentication sequence."
    ],
    "<space image>": [],
    "<data per>": [],
    "<quota size>": [],
    "<ACL, ACLS, ACLs, Acl, Acls, acl, acls>": [
        "ACLs are NOT enforced hierarchically.",
        "An ACL consists of a set of ACL entries.",
        "If no ACL is configured for a specific key AND no default ACL is configured AND no whitelist key ACL is configured for the requested operation, then access will be DENIED.",
        "This means that cell ACLs do not override ACLs at less granularity.",
        "ACLs are disabled by default.",
        "ACLs are evaluated from least granular to most granular, and when an ACL is reached that grants permission, evaluation stops.",
        "Job Tracker queue ACLs can be changed without restarting Job Tracker.",
        "Determine if cell ACLs covered by the operation grant access.",
        "Otherwise, the ACL of the existing cell you are appending to or incrementing is preserved.",
        "An ACL contains two components, the authentication method and the principal.",
        "ACLs can specify fine-grained file permissions for specific named users or named groups.",
        "If reservation ACLs are enabled but not defined, everyone will have access.",
        "NOTE: The default and whitelist key ACL does not support ALL operation qualifier.",
        "Every ACL must have a mask.",
        "ACL of node-labels on queues - user can set accessible node labels on each queue so only some nodes can only be accessed by specific queues.",
        "By default, ACLs are disabled.",
        "Runtime Configuration - The queue definitions and properties such as capacity, ACLs can be changed, at runtime, by administrators in a secure manner to minimize disruption to users.",
        "ACL list for users allowed to run privileged containers.",
        "ACL used in case none is found.",
        "Queue Access Control Lists (ACLs) allow administrators to control who may take actions on particular queues.",
        "ACLs are discussed in greater detail later in this document.",
        "An ACL provides a way to set different permissions for specific named users or named groups, not only the file's owner and the file's group.",
        "If the ACL itself contains secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within."
    ],
    "<Bigtable, bigtable>": [],
    "<replication count>": [
        "The replication count of all files is \"1\"."
    ],
    "<snapshot directory>": [],
    "<space quota>": [
        "When a space quota is set on a namespace, the quota's limit applies to the sum of usage of all tables in that namespace.",
        "However, overall space quota is recommended in this case as the storage type information is either unavailable or inaccurate for storage type quota enforcement.",
        "By default, if a table or namespace is deleted that has a space quota, the quota itself is also deleted.",
        "With the -q option, also report the name quota value set for each directory, the available name quota remaining, the space quota value set, and the available space quota remaining.",
        "The space quota is a hard limit on the number of bytes used by files in the tree rooted at that directory.",
        "In some cases, it may be desirable for the space quota to not be automatically deleted."
    ],
    "<classpaths>": [
        "If called without arguments, then prints the classpath set up by the command scripts, which is likely to contain wildcards in the classpath entries.",
        "If named by a String, then the classpath is examined for a file with that name.",
        "Classpath is usually the first problem."
    ],
    "<memory requirement>": [],
    "<OI, open instance>": [],
    "<leaf name>": [],
    "<encryption algorithms>": [
        "Encryption algorithm and key used."
    ],
    "<backing files>": [],
    "<compactable>": [],
    "<threshold level>": [],
    "<wordcount>": [],
    "<cache access>": [],
    "<byte quotas>": [],
    "<Sqoop>": [],
    "<block level>": [],
    "<Partition files>": [],
    "<DPP>": [
        "If DPP enabled only for mapjoin and join is not a map join."
    ],
    "<query time>": [],
    "<store sizes>": [],
    "<Table metadata, table metadata>": [],
    "<table monitor>": [],
    "<output processor>": [],
    "<dmesg>": [],
    "<configured number>": [],
    "<Master Service, master service>": [],
    "<Latency metrics, latency metrics>": [
        "By default, percentile latency metrics are disabled."
    ],
    "<cell bytes>": [],
    "<Driver code>": [],
    "<input row, input rows>": [
        "Map outputs table rows IF the input row has columns that have content."
    ],
    "<cluster ID, cluster identifier>": [],
    "<Compactor, compactor>": [
        "A compactor is a compaction algorithm associated a given policy.",
        "When the compactor kicks in, these delta files get rewritten into read- and storage-optimized ORC format (enable dictionary encoding, indexes and compression)."
    ],
    "<Data generated, data generated, data generator>": [],
    "<Metrics names, metric name, metric names, metrics names>": [
        "All metrics names start with capitals."
    ],
    "<labels table>": [],
    "<LLAP>": [
        "Obviously, LLAP level of support depends on each individual execution engine (starting with Tez)."
    ],
    "<exec call>": [],
    "<data tables>": [],
    "<local resource>": [],
    "<memory replicas>": [],
    "<end key, end keys>": [],
    "<double cost>": [],
    "<replication streams>": [],
    "<Flajolet>": [],
    "<memory copies>": [
        "Indicates which memory copy is used in building cell."
    ],
    "<YARN Resource>": [
        "The YARN Resource Manager gets a new token for the node managers, if needed.",
        "YARN Resource Managers (RMs) and Node Managers (NMs) co-operate to execute the user's application with the identity and hence access rights of that user."
    ],
    "<HTTP verbs>": [
        "They also strive for simple non-POSIX APIs: the HTTP verbs are the operations allowed."
    ],
    "<Write key, write key, writes keys>": [],
    "<vcore>": [],
    "<Specify table, specified table>": [
        "Get if the specified table is partitioned table or not.",
        "Checks if the specified table exists.",
        "Suspend the procedure if the specified table is already locked."
    ],
    "<ODBC, odbc>": [],
    "<threshold time>": [],
    "<maximal time>": [],
    "<particular block>": [],
    "<numeric order>": [],
    "<Generates name>": [],
    "<groups list>": [],
    "<Storage Account, Storage account, Storage accounts, storage account>": [
        "A storage account may have multiple containers."
    ],
    "<shared store>": [],
    "<Physical size>": [],
    "<port search>": [],
    "<cluster key>": [],
    "<column reference, column references>": [
        "In such cases the column references are replaced by the corresponding expression in the input data."
    ],
    "<HBASE, Hbase, hbase>": [
        "We do this indirection so hbase core can evolve its protobuf version independent of whatever our dependencies rely on.",
        "In this case, you would do the following to link the hadoop native lib so hbase could find them.",
        "The hbase:meta table (previously called .META.) keeps a list of all regions in the system.",
        "Here are others that you may have to take into account: The hbase:meta table is forced into the block cache and have the in-memory priority which means that they are harder to evict."
    ],
    "<TaskID, taskid>": [
        "TaskID represents the immutable and unique identifier for a Map or Reduce Task.",
        "TaskID consists of 3 parts."
    ],
    "<huge size>": [],
    "<Coprocessor, Coprocessors, coprocessor, coprocessors>": [
        "Coprocessors can optionally abort actions.",
        "Coprocessor framework will automatically load the configured classes as default coprocessors.",
        "Coprocessors can piggyback this event.",
        "Thrown if a coprocessor encounters any exception.",
        "Coprocessors act like RDBMS triggers.",
        "Coprocessors can piggyback or fail this process.",
        "Whether a coprocessor is loadable or not will be determined when a region is opened.",
        "Coprocessor Endpoint whose API is marked private given this is an evolving feature; the Coprocessor API is not for public consumption.",
        "Coprocessors are executed Load from table attribute Coprocessor classes can also be configured at table attribute.",
        "Coprocessor classes can be loaded either from local Multiple types of coprocessors are provided to provide sufficient flexibility for potential use cases.",
        "Coprocessors are executed in order according to the natural ordering of the int.",
        "Please note that Coprocessors are covered in the operational section.",
        "Observer coprocessors are triggered either before or after a specific event occurs.",
        "Coprocessors which are loaded in this way will be active on all regions of all tables.",
        "Coprocessor classes can be configured in any order, based on that priority is set and chained in a sorted order.",
        "In this version, the coprocessor is loaded dynamically (table coprocessor for the flowrun table).",
        "In this scenario, coprocessors might make sense."
    ],
    "<table archive>": [],
    "<Retrieve list>": [],
    "<root queue>": [
        "All queues in the system are children of the root queue."
    ],
    "<bit vector>": [],
    "<HTTP policy, http policy>": [
        "When the deprecated configuration properties are still configured, currently http policy is decided based on the following rules: 1."
    ],
    "<replication tree>": [],
    "<total values>": [],
    "<statistics stored>": [],
    "<keypairs>": [],
    "<Data Migration, data migration>": [],
    "<status description>": [],
    "<long period>": [],
    "<SHOW>": [
        "SHOW VIEWS lists all the views in the current database (or the one explicitly named using the IN or FROM clause) with names matching the optional regular expression.",
        "SHOW FUNCTIONS lists all the user defined and builtin functions matching the regular expression.",
        "SHOW PARTITIONS lists all the existing partitions for a given base table.",
        "SHOW TRANSACTIONS is for use by administrators when Hive transactions are being used."
    ],
    "<Apply order, apply ORDER>": [],
    "<data producer>": [],
    "<Table alias, table alias, table aliases>": [],
    "<row bytes>": [],
    "<Simple time>": [],
    "<emtpy>": [],
    "<error indicates>": [],
    "<source checksum>": [],
    "<Failure info, failure info>": [],
    "<JAAS configuration>": [],
    "<search time>": [],
    "<cloned list>": [],
    "<checks size>": [],
    "<input description>": [],
    "<autoconf>": [],
    "<range list>": [],
    "<templatized>": [],
    "<sorting files>": [],
    "<nagios>": [],
    "<shutdown time>": [],
    "<allocation request>": [],
    "<sym>": [],
    "<periodic key>": [],
    "<write requests>": [
        "If issued against a non-decommissioing data node, all current writers will be stopped, but new write requests will continue to be served.",
        "As write requests are handled by the region server, they accumulate in an in-memory storage system called the memstore."
    ],
    "<Create call, create call>": [],
    "<key area>": [],
    "<log key>": [],
    "<binary tree>": [],
    "<data decompression>": [],
    "<single transaction>": [],
    "<primitive field>": [],
    "<particular prefix>": [],
    "<data reads>": [],
    "<Active Directory, active directory>": [
        "Azure Active Directory (Azure AD) is Microsoft's multi-tenant cloud based directory and identity management service."
    ],
    "<Local Reads, local read, local reads>": [],
    "<table node>": [],
    "<NN, NNs>": [
        "On start, this NN will not enter the standby state as usual in an HA setup.",
        "The active NN at the time this happens will perform the finalization of the shared log, and the NN whose local storage directories contain the previous FS state will delete its local state.",
        "To perform a rollback of an upgrade, both NNs should first be shut down.",
        "Rather, this NN will immediately enter the active state, perform an upgrade of its local storage dirs, and also perform an upgrade of the shared edit log."
    ],
    "<separate row>": [],
    "<output metadata>": [],
    "<memory token>": [],
    "<partiton>": [],
    "<gen size>": [],
    "<Datastore, datastore>": [],
    "<split table>": [],
    "<storage cluster>": [],
    "<parent row>": [],
    "<Compressed blocks, compressed blocks>": [
        "Compressed blocks can be jumped over without first having to be decompressed for scanning.",
        "Each compressed block requires one compression/decompression codec for I/O."
    ],
    "<NDV>": [],
    "<Widom>": [],
    "<BYTES>": [],
    "<OOM>": [],
    "<available size>": [],
    "<storage location, storage locations>": [
        "Then, if one storage location is corrupt, you can read the metadata from one of the other storage locations.",
        "A datanode storage location /grid/dn/disk0 on DISK should be configured with [DISK]file:///grid/dn/disk0 A datanode storage location /grid/dn/archive0 on ARCHIVE should be configured with [ARCHIVE]file:///grid/dn/archive0 The default storage type of a datanode storage location will be DISK if it does not have a storage type tagged explicitly."
    ],
    "<Map files>": [
        "Map files are created by adding entries in-order."
    ],
    "<core size>": [],
    "<factors number>": [],
    "<http services>": [
        "Conf HTTP service should set response's content type according to the Accept header in the request."
    ],
    "<filter operates>": [
        "When only include patterns are specified, the filter operates in the white listing mode, where only matched sources are included."
    ],
    "<Resource Request, resource request, resource requests>": [
        "If an app specifically lists a queue in a container resource request, the request is submitted to that queue.",
        "If no node label requirement specified, such Resource Request will only be allocated on nodes belong to DEFAULT partition."
    ],
    "<Namenode, namenode>": [
        "Each namenode has its own namespace.",
        "A namenode prior to this change will not be able to communicate with a namenode after this change.",
        "Each Namenode decommissions its Block Pool.",
        "Any other value will throw an exception when namenode and datanodes are starting up.",
        "Specifies a rolling upgrade already started so that the namenode should allow image directories with different layout versions during startup.",
        "If the namenode ID is not configured it is determined automatically by matching the local node's address with the configured address.",
        "A namenode belongs to one and only one cluster.",
        "The issue HBASE-8354 forces Namenode into loop with lease recovery requests is messy but has a bunch of good discussion toward the end on low timeouts and how to cause faster recovery including citation of fixes added to HDFS.",
        "If set to false, the Namenode uses the traditional synchronous edit logs.",
        "Any Namenode in the cluster can be used to access this web page."
    ],
    "<allocation size>": [],
    "<resource estimator>": [],
    "<Bloom Filter, Bloom Filters, Bloom filter, Bloom filters, bloom filter, bloom filters>": [
        "Bloom filters were introduced in HBASE-1200.",
        "Large Bloom filters produce a different performance problem: the first get request that requires a Bloom filter lookup will incur the latency of loading the entire Bloom filter bit array.",
        "A counting Bloom filter is an improvement to standard a Bloom filter as it allows dynamic additions and deletions of set membership information.",
        "A Bloom filter chunk enqueued for writing.",
        "Bloom filters are enabled on a Column Family.",
        "A dynamic Bloom filter (DBF) makes use of a s * m bit matrix but each of the s rows is a standard Bloom filter.",
        "Bloom filter type specified in column family configuration.",
        "If an active Bloom filter is found, the key is inserted and nr is incremented by one.",
        "(A bit field or bloom filter could also be included.) Row index entries provide offsets that enable seeking to the right compression block and byte within a decompressed block.",
        "Enabling Bloom Filters can save your having to go to disk and can help improve read latencies.",
        "A Bloom filter, named for its creator, Burton Howard Bloom, is a data structure which is designed to predict whether a given element is a member of a set of data.",
        "The Bloom filter is a data structure that was introduced in 1970 and that has been adopted by the networking research community in the past decade thanks to the bandwidth efficiencies that it offers for the transmission of set membership information between networked hosts.",
        "Unless you have one column per row, row+column Bloom filters require more space, in order to store more keys.",
        "Bloom filters are designed to be \"accurate enough\" for sets of data which are so large that conventional hashing mechanisms would be impractical.",
        "A Bloom filter is active when the number of recorded keys, nr, is strictly less than the current cardinality of A, n.",
        "Bloom filters need to be rebuilt upon deletion, so may not be appropriate in environments with a large number of deletions.",
        "Bloom filter blocks and index blocks (we call these \"inline blocks\") become interspersed with data blocks, and as a side effect we can no longer rely on the difference between block offsets to determine data block length, as it was done in version 1."
    ],
    "<Table directory, table directories, table directory>": [],
    "<keyvalue>": [],
    "<local program>": [],
    "<capacity usage>": [],
    "<RPC connection>": [],
    "<RCN>": [],
    "<creating cells>": [],
    "<PMOD>": [],
    "<group type>": [],
    "<data fails>": [],
    "<big cluster>": [],
    "<serialized value, serialized values>": [
        "All serialized values are 9 bytes in length.",
        "All serialized values are 5 bytes in length."
    ],
    "<end state>": [],
    "<Error rate, error rate>": [],
    "<row size>": [
        "Average row size is computed from average column size of all columns in the row."
    ],
    "<time low>": [],
    "<SHOW TABLE, Show Table>": [
        "SHOW TABLE EXTENDED will list information for all tables matching the given regular expression."
    ],
    "<Storage Properties, storage properties>": [],
    "<ngrams>": [
        "(ngrams) Find trending topics in text.",
        "(ngrams) Find important topics in text in conjunction with a stopword list."
    ],
    "<Metrics system, metrics system>": [
        "A default metrics system is provided to marshal metrics from sources to sinks based on (per source/sink) configuration options."
    ],
    "<filter initialization>": [],
    "<Object Store, object store>": [
        "If the operation is interrupted, the object store will be in an undefined state.",
        "An object store is a data storage service, usually accessed over HTTP/HTTPS.",
        "If an object store is eventually consistent, then any operation which overwrites existing objects may not be immediately visible to all clients/queries.",
        "Some object store connectors offer an option for in-memory buffering of output \u2014for example the S3A connector.",
        "Different object store clients may support these commands: do consult the documentation and test against the target store.",
        "Smaller values results in faster test runs, especially when the object store is a long way away."
    ],
    "<Cluster connection, cluster connection>": [
        "Cluster connection to be shared by services."
    ],
    "<access type, access types>": [],
    "<short amount>": [],
    "<created directories>": [
        "Newly created directories have no associated quota."
    ],
    "<diffed>": [],
    "<data sampled>": [],
    "<whole cluster>": [],
    "<filepath>": [],
    "<Procedure Types>": [],
    "<reading keys>": [],
    "<extant files>": [],
    "<heartbeat code>": [],
    "<filesink>": [],
    "<Started cluster>": [],
    "<Storage Policies, Storage Policy, storage policies, storage policy>": [
        "Ensure Storage Policies are enabled.",
        "A storage policy specifies the placement of block replicas on specific storage types.",
        "To set storage type quota on a directory, storage policies must be configured on the directory in order to allow files to be stored in different storage types according to the storage policy.",
        "More formally, a storage policy consists of the following fields: Policy ID."
    ],
    "<write queue>": [],
    "<unarchived>": [],
    "<Mapreduce, mapred, mapreduce>": [],
    "<textual format>": [],
    "<Local directory, local directories, local directory>": [],
    "<cache limit>": [],
    "<instant name>": [],
    "<syslogs>": [],
    "<intra row>": [],
    "<hierarchy level>": [],
    "<WARN>": [],
    "<Maximum capacity, maximum capacity>": [],
    "<cluster name>": [],
    "<timedout>": [],
    "<Row Format, row format>": [
        "The delimited row format specifies how the rows are stored in the hive table.",
        "A native SerDe is used if ROW FORMAT is not specified or ROW FORMAT DELIMITED is specified."
    ],
    "<storer>": [],
    "<JavaCC>": [],
    "<AVL>": [],
    "<partition number>": [],
    "<potential size>": [],
    "<change number>": [],
    "<physical location>": [],
    "<cell number>": [],
    "<encryption information>": [],
    "<cache value, caching values>": [
        "Higher caching values will enable faster scanners but will use more memory."
    ],
    "<data private>": [],
    "<log status>": [],
    "<optimize number>": [],
    "<int8>": [],
    "<Single Table, Single table, single table>": [
        "List of tables (or a single table) to include in the backup set.",
        "This allows for a scenario where a large limit can be placed on a collection of tables, but a single table in that collection can have a fine-grained limit set."
    ],
    "<binary fields>": [],
    "<Scheduler state>": [
        "Fair Scheduler state dumps can potentially generate large amount of log data."
    ],
    "<destination directories>": [
        "When a file is renamed, its modification time is not changed, but the source and destination directories have their modification times updated."
    ],
    "<table status>": [],
    "<store compaction>": [
        "Called after the store compaction has completed."
    ],
    "<max count>": [
        "There are lazily created buffers and the count is the max count to be pooled."
    ],
    "<See store>": [],
    "<resync>": [],
    "<completion service>": [],
    "<service ID>": [],
    "<status message>": [],
    "<Registry Security>": [],
    "<fact table>": [],
    "<external table>": [
        "Determines whether a table is an external table.",
        "External table files can be accessed and managed by processes outside of Hive.",
        "Classic table type mapping : Managed Table ==> Table External Table ==> Table Virtual View ==> View."
    ],
    "<Region state>": [
        "Region state is distributed and hard to reason about and test.",
        "Let a region state have some vintage before we act on it (one second currently)."
    ],
    "<Lock times>": [
        "Lock times out on master: Can happen because of network issues, GC pauses, etc."
    ],
    "<memory merge>": [],
    "<colname>": [
        "colname indicates the column on which to sample each row in the table."
    ],
    "<memstore>": [
        "Called before the memstore is flushed to disk.",
        "Called after the memstore is flushed to disk.",
        "Number of milliseconds updates have been blocked so the memstore can be flushed.",
        "When it is enabled, memstores will step allocate memory in MSLAB 2MB chunks even if the memstore has zero or just a few small elements.",
        "For write-heavy workload, memstore fraction can be increased in configuration at the expense of block cache; this will also allow one to have more regions."
    ],
    "<toJSON>": [],
    "<localized files>": [],
    "<shared cluster>": [],
    "<aggregation request>": [],
    "<lock limit>": [],
    "<Capacity Unit, capacity unit>": [],
    "<cluster services>": [],
    "<storage descriptor, storage descriptors>": [],
    "<long constant>": [],
    "<Cron, cron>": [],
    "<CRCs>": [],
    "<delta directory>": [
        "For every transaction batch a delta directory will be created which will impact when compaction will trigger."
    ],
    "<entry number>": [],
    "<system access>": [],
    "<key space>": [],
    "<heapsize>": [],
    "<complete row>": [
        "If the version you specified when deleting a row is larger than the version of any value in the row, then you can consider the complete row to be deleted."
    ],
    "<Master Info, master info>": [],
    "<filter pattern>": [],
    "<Meta Block, Meta block, meta block>": [
        "Exception - Meta Block with the same name already exists."
    ],
    "<rmr>": [],
    "<PDU>": [],
    "<cluster status>": [],
    "<failure types>": [],
    "<group hierarchy>": [],
    "<table location>": [],
    "<specified root>": [],
    "<memory pressure>": [],
    "<Attempt State>": [],
    "<signal request>": [],
    "<TSV, tsv>": [],
    "<output compressed>": [],
    "<Data restatement>": [],
    "<Deafult value>": [],
    "<submission code>": [
        "If \"disabled\" is specified then the job submission code will not use the shared cache."
    ],
    "<disk bytes>": [],
    "<compaction request>": [
        "Called if the compaction request is failed for some reason.",
        "if compaction request should block until completion."
    ],
    "<archive directory>": [],
    "<checksum error, checksum errors>": [],
    "<mountpoints>": [],
    "<positive rate>": [],
    "<specified family>": [],
    "<Metrics information, metric info, metrics info>": [
        "More metrics info can see Router RPC Metrics and State Store Metrics."
    ],
    "<PoV>": [],
    "<dump directory>": [],
    "<table information>": [],
    "<range request>": [],
    "<memory segment>": [],
    "<leaf level>": [
        "Intermediate levels can only be present if leaf level blocks are present Optionally, version 2 leaf levels, stored in the non%root format inline with data blocks."
    ],
    "<hflush>": [],
    "<service store>": [],
    "<column alias>": [],
    "<entire row>": [
        "Get whether entire row should be filtered if column is not found.",
        "If true, the entire row will be skipped if the column is not found.",
        "Otherwise, if the column is found, the entire row will be emitted only if the value passes."
    ],
    "<ORDER table>": [],
    "<storage statistics>": [
        "Stores global storage statistics objects."
    ],
    "<bytes referred>": [],
    "<heartbeat interval>": [],
    "<resource contention>": [],
    "<table function>": [],
    "<source state>": [],
    "<Estimate size>": [],
    "<Http address, http address>": [],
    "<error heuristic>": [],
    "<simple array>": [],
    "<connection creation>": [
        "Streaming connection creation, begin/commit/abort transactions, write and close has to be called in the same thread."
    ],
    "<bucket name>": [
        "The same bucket name can be used for all tests."
    ],
    "<regular row>": [],
    "<Vaibhav>": [],
    "<Corrupt files, Corrupted Files, corrupt files, corrupted files>": [
        "Corrupt files should probably be removed."
    ],
    "<statistic entries>": [],
    "<portmap>": [],
    "<service definition>": [
        "A reference to this service would use the rackspaceuk service name: Because the public endpoint is used, if this service definition is used within the London datacenter, all accesses will be billed at the public upload/download rates, irrespective of where the Hadoop cluster is.",
        "This service definition is for use in a Hadoop cluster deployed within Rackspace's US infrastructure."
    ],
    "<Filesystem, filesystem>": [
        "Other filesystems are skipped unless there is a specific configuration to the remote server providing the filesystem.",
        "If the filesystem has multiple partitions, the use and capacity of the root partition is reflected.",
        "Case: Underlying filesystem doesn't behave in a way that matches Hadoop's expectations.",
        "New filesystem shell command -df reports capacity, space used and space free.",
        "It can then be declared that a path has no parent in which case it is the root directory, or it MUST have a parent that is a directory: Because the parent directories of all directories must themselves satisfy this criterion, it is implicit that only leaf nodes may be files or symbolic links: Furthermore, because every filesystem contains the root path, every filesystem must contain at least one directory.",
        "It also checks that the filesystem isn't in a read-only state.",
        "If set, when the filesystem is instantiated then all outstanding uploads older than the purge age will be terminated -across the entire bucket.",
        "If the operations are interrupted, the filesystem is left in an intermediate state.",
        "Filesystem commands which list permission and user/group details, usually simulate these details."
    ],
    "<cached table>": [],
    "<Block Pool>": [
        "Each Block Pool is managed independently.",
        "A Namespace and its block pool together are called Namespace Volume.",
        "A Block Pool is a set of blocks that belong to a single namespace.",
        "When a Namenode/namespace is deleted, the corresponding block pool at the Datanodes is deleted."
    ],
    "<SCM>": [
        "The SCM runs as a separate daemon process that can be placed on any node in the cluster."
    ],
    "<Temporary space>": [],
    "<lsr>": [],
    "<Container request, container request>": [
        "The execution types are the following: GUARANTEED - this container is guaranteed to start its execution, once the corresponding start container request is received by an NM."
    ],
    "<output row>": [],
    "<output fields>": [],
    "<subproject, subprojects>": [],
    "<Vaidya>": [
        "Introduced Vaidya rule based performance diagnostic tool for Map/Reduce jobs."
    ],
    "<client call>": [],
    "<GB size>": [],
    "<Flush interval, flush interval>": [],
    "<HTTP endpoint>": [],
    "<TRACE level, trace level>": [],
    "<time vector>": [],
    "<partition filter>": [],
    "<table character>": [],
    "<Resource size>": [],
    "<output read>": [],
    "<program names>": [],
    "<Data Processing>": [],
    "<maximum threshold>": [],
    "<access path>": [],
    "<CPU limit>": [],
    "<feature requests>": [],
    "<swappiness>": [],
    "<store blocks>": [],
    "<Resource Allocation, Resource allocation, resource allocation>": [
        "However for this beta only static resource allocation can be used.",
        "If this is true, the full resource allocation will be included in the response."
    ],
    "<underlying bytes>": [],
    "<integer list>": [],
    "<storage permissions>": [],
    "<counter format>": [],
    "<given identifier>": [],
    "<remote name>": [],
    "<output trace>": [],
    "<cluster cost>": [],
    "<access token>": [
        "Obtain the access token that should be added to https connection's header."
    ],
    "<Memory state, memory state, memory states>": [],
    "<mob store>": [],
    "<data schema>": [],
    "<replication peer, replication peers>": [
        "Thrown when a replication peer can not be found.",
        "Test whether a replication peer is enabled.",
        "Get the replication peer instance."
    ],
    "<Single Cluster>": [],
    "<Resource capability>": [],
    "<immutable table>": [],
    "<certain time>": [],
    "<remote context>": [
        "Indicates whether the remote context is active.",
        "Check if remote context is still active.",
        "Any pending jobs will be cancelled, and the remote context will be torn down.",
        "Normally, the remote context will queue jobs and execute them based on how many worker threads have been configured."
    ],
    "<Compress key, compress key>": [],
    "<performance evaluation>": [],
    "<traffic control>": [],
    "<memory per>": [],
    "<Throttle Type>": [],
    "<Processes block>": [],
    "<REST server>": [
        "The included REST server can run as a daemon which starts an embedded Jetty servlet container and deploys the servlet into it.",
        "The thread pool always has at least these number of threads so the REST server is ready to serve incoming requests."
    ],
    "<Timeline store, timeline store>": [],
    "<Compressed Value, Compressed value, compressed value>": [],
    "<true size>": [],
    "<unconfigured>": [],
    "<key comparison>": [],
    "<Column Families, Column Family, Column families, Column family, column families, column family>": [
        "If the column family is omitted, all MOB-enabled column families are compacted.",
        "If only one column family is busy with writes, only that column family accomulates memory.",
        "The column family prefix must be composed of printable characters.",
        "Because tunings and storage specifications are done at the column family level, it is advised that all column family members have the same general access pattern and size characteristics.",
        "When a column family Store is created, it says what memstore type is in effect.",
        "Physically, all column family members are stored together on the filesystem.",
        "same row, column family, qualifier and timestamp \u2014 regardless of which arrived first.",
        "The column family must already exist in your table // schema.",
        "The column family names are stored for every value (ignoring prefix encoding).",
        "When many column families exist the flushing and compaction interaction can make for a bunch of needless i/o (To be addressed by changing flushing and compaction to work on a per column family basis).",
        "If multiple column families are involved, the columns may be spread across them.",
        "Serialize column family to block size map to configuration.",
        "Serialize column family to data block encoding map to configuration.",
        "To enable this setup, alter your table and for each column family set BLOCKCACHE \u21d2 'false'.",
        "Column families must be declared up front at schema definition time whereas columns do not need to be defined at schema time but can be conjured on the fly while the table is up and running.",
        "Each column family has a set of storage properties, such as whether its values should be cached in memory, how its data is compressed or its row keys are encoded, and others.",
        "To check which column families are using incompatible data block encoding you can use Pre-Upgrade Validator.",
        "Per Column Family keys facilitate low impact incremental key rotation and reduce the scope of any external leak of key material.",
        "Column Family time ranges take precedence over the global time range."
    ],
    "<temp directory>": [],
    "<ACKs, Ack, ack>": [
        "Delayed ACKs can add up to ~200ms to RPC round trip time."
    ],
    "<TABLE, Tablename, tablename>": [
        "hive> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING); creates a table called invites with two columns and a partition column called ds.",
        "DROP TABLE removes metadata and data for this table.",
        "ALTER TABLE ADD|REPLACE COLUMNS with CASCADE command changes the columns of a table's metadata, and cascades the same change to all the partition metadata.",
        "DB and TABLENAME are DOT-separated."
    ],
    "<Large Clusters, large cluster, large clusters>": [],
    "<Service Registry, service registry>": [],
    "<fsimage format>": [],
    "<Cluster Reservation>": [],
    "<mega bytes>": [],
    "<chain order>": [],
    "<given Mutation>": [],
    "<data twice>": [],
    "<privilege level>": [],
    "<filesize>": [],
    "<RDL>": [
        "If the RDL is different, the reservation will be rejected, and the request will be unsuccessful."
    ],
    "<Cluster registry>": [],
    "<ARN, arn>": [],
    "<Recursive version>": [],
    "<operation level>": [],
    "<Table Scan, table scan>": [
        "But, more things will be added here as table scan is invoked as part of local work."
    ],
    "<permission values>": [
        "The exact permission values in the new child's access ACL are subject to filtering by the mode parameter."
    ],
    "<Request size, request size>": [],
    "<data analytics>": [],
    "<filter cells>": [
        "This filter is used to filter cells based on value."
    ],
    "<encryption key, encryption keys>": [],
    "<SCP, scp>": [],
    "<gang size>": [],
    "<running time>": [],
    "<intermediate files>": [
        "The intermediate files in the temporary directory will not be cleaned up."
    ],
    "<given block>": [],
    "<resolvers>": [
        "Note that the resolver is stored in the UDF class as an instance variable.",
        "The resolver handles type checking and operator overloading for UDAF queries.",
        "Each resolver has its own set of optimization rule.",
        "This resolver is used for resolving the UDF method that is to be used for evaluation given the list of the argument types.",
        "This resolver is used for resolving the UDAF methods are used for partial and final evaluation given the list of the argument types.",
        "The resolver handles type checking and operator overloading (if you want it), and helps Hive find the correct evaluator class for a given set of argument types."
    ],
    "<input size>": [],
    "<UGI, Ugi, ugi>": [
        "UGI handles the login, and schedules a background thread to relogin the user periodically.",
        "Loading in the token file is automatic: UGI does it during user login."
    ],
    "<datanucleus>": [],
    "<storage system, storage systems>": [],
    "<quota policy>": [],
    "<Fetch table, fetch table>": [],
    "<Compare bytes>": [],
    "<SSH, ssh>": [
        "The argument 1 (%1$s) is SSH options set the via opts setting or via environment variable, 2 is SSH user name, 3 is \"@\" if username is set or \"\" otherwise, 4 is the target host name, and 5 is the logical command to execute (that may include single quotes, so don't use them)."
    ],
    "<discrete value>": [],
    "<log roll>": [
        "Sync slots after log roll failed, abort."
    ],
    "<INDEX>": [
        "An index can be dropped at any time with DROP INDEX.",
        "However, ALTER INDEX requires an index name that was created with lowercase letters (see HIVE-2752)."
    ],
    "<Log4J, Log4j, log4J, log4j>": [],
    "<RDBMS, RDBMSs, rdbms>": [
        "RDBMS products are more advanced in this regard to handle alternative index management out of the box.",
        "Whether the RDBMS has a bug in join and filter operation order described in DERBY-6358.",
        "An RDBMS can scale well, but only up to a point - specifically, the size of a single database server - and for the best performance requires specialized hardware and storage devices.",
        "Whether the RDBMS has restrictions on IN list size (explicit, or poor perf-based)."
    ],
    "<statistics table>": [],
    "<web proxy>": [],
    "<approximate key>": [],
    "<Output Key, output Key, output key, output keys>": [
        "The key in an output key/value pair encode two pieces of information: aggregation type and aggregation id.",
        "The map output keys of the above Map/Reduce job normally have four fields separated by \".\".",
        "In this case, the map output key will consist of fields 6, 5, 1, 2, and 3."
    ],
    "<lock queues>": [],
    "<DynamoDB>": [
        "See DynamoDB documents for more information.",
        "The IO load of clients of the (shared) DynamoDB table was exceeded."
    ],
    "<Client policy, client policy>": [],
    "<given results>": [],
    "<Failover, failover, failovers>": [
        "Even if automatic failover is configured, you may initiate a manual failover using the same hdfs haadmin command.",
        "RM Failover Recovering previous active-RM's state."
    ],
    "<approximate count>": [],
    "<single qualifier>": [],
    "<POM>": [],
    "<disk services>": [],
    "<column key, column keys>": [
        "While rows and column keys are expressed as bytes, the version is specified using a long integer."
    ],
    "<Maximum lifetime, maximum lifetime>": [],
    "<JDBM>": [],
    "<memory boundary>": [],
    "<Side Code, side code>": [],
    "<persistent state>": [],
    "<Appender, appender, appenders>": [],
    "<JCODE>": [],
    "<keyspace>": [
        "Also, if you are pre-splitting regions and all your data is still winding up in a single region even though your keys aren't monotonically increasing, confirm that your keyspace actually works with the split strategy."
    ],
    "<Directory specification>": [],
    "<Zoltan>": [],
    "<key per>": [],
    "<copy code>": [],
    "<root block>": [],
    "<disk writes>": [],
    "<confidence level>": [],
    "<failure count>": [],
    "<production cluster>": [],
    "<HTTP access>": [],
    "<RDD, RDDs>": [
        "While RDD extension seems easy in Scala, this can be challenging as Spark's Java APIs lack such capability."
    ],
    "<byte per>": [],
    "<encoding state>": [],
    "<local copies>": [
        "This local copy is required to display on the webui.",
        "Your local copy of Hive should work by running build/dist/bin/hive from the Hive root directory, and you should have some tables of data loaded into your local instance for testing whatever UDAF you have in mind."
    ],
    "<Storage Type, Storage Types, Storage type, storage type, storage types>": [
        "Storage type quota on DISK are of limited use except when DISK is not the dominant storage medium.",
        "Fallback storage types are used if the preferred storage types are not available.",
        "The default storage type will be DISK if the directory does not have a storage type tagged explicitly.",
        "Storage type quota can be configured even though the specific storage type is unavailable (or available but not configured properly with storage type information).",
        "However, overall space quota is recommended in this case as the storage type information is either unavailable or inaccurate for storage type quota enforcement.",
        "To set storage type quota on a directory, storage policies must be configured on the directory in order to allow files to be stored in different storage types according to the storage policy.",
        "Remove storage type quota specified for each directory.",
        "If specific storage types are given after -t option, only quota and remaining quota of the types specified will be displayed.",
        "These storage types are used sequentially for successive block replicas.",
        "The storage type quota is a hard limit on the usage of specific storage type (SSD, DISK, ARCHIVE) by files in the tree rooted at the directory."
    ],
    "<cell key>": [],
    "<HFDS>": [],
    "<load balancer>": [
        "The Master runs several background threads: Periodically, and when there are no regions in transition, a load balancer will run and move regions around to balance the cluster's load.",
        "The Load Balancer ensures that the region replicas are not co-hosted in the same region servers and also in the same rack (if possible)."
    ],
    "<high latency>": [
        "This can cause high latency in job submission as well as incur some AWS network transmission costs."
    ],
    "<Hive Metastore, Hive metastore, hive metastore>": [
        "Hive metastore has no knowledge of this dependency (view partition -> table partition), and it is maintained in multiple places (for possibly different teams)."
    ],
    "<key boundaries>": [],
    "<stores table>": [],
    "<byte copy>": [],
    "<compatibility level>": [],
    "<binary distribution>": [],
    "<transfer size>": [],
    "<attempt number>": [],
    "<output structure>": [],
    "<CIDR>": [],
    "<given collation>": [],
    "<key storage>": [],
    "<cluster capacity>": [],
    "<denormalized>": [],
    "<precision number>": [],
    "<MVCC, mvcc>": [
        "Set the mvcc read point to -1 which means do not use it."
    ],
    "<scan range>": [],
    "<benchmark cluster>": [],
    "<table files>": [
        "External table files can be accessed and managed by processes outside of Hive."
    ],
    "<stability level>": [],
    "<table regardless>": [],
    "<data scanned, data scans>": [],
    "<Netty, netty>": [
        "Netty is used to serve requests, so a thread is not needed for each connection."
    ],
    "<JPOX>": [],
    "<threshold number>": [
        "When threshold number of the nodemanager-local-directories or threshold number of the nodemanager-log-directories become bad."
    ],
    "<byte split>": [],
    "<reported values>": [
        "If the directory does not have a quota set, the reported values are none and inf."
    ],
    "<summary message>": [],
    "<output Partition>": [],
    "<TODO, todo>": [
        "TODO: remove once HBASE-11555 is fixed.",
        "TODO: need to turn on rules that's commented out and add more if necessary.",
        "TODO: rename files \"execute\" option may be supplied in both modes to have the utility automatically execute the equivalent of the generated commands \"location\" option may be supplied followed by a path to set the location for the generated scripts.",
        "TODO Currently List type only support non nested case.",
        "TODO: this method is relied upon by custom input formats to set jobconf properties.",
        "The client should only hold one open transaction at any given time (TODO: enforce this)."
    ],
    "<Composite service, composite service>": [],
    "<Flow name, flow name>": [
        "If the flow context is not specified, defaults are supplied for these attributes: Flow name: the YARN application name (or the application id if the name is not set).",
        "user, flow name and run id are not mandatory but if specified in query param can preclude the need for an additional operation to fetch flow context information based on cluster and app id."
    ],
    "<consistency level>": [
        "Consistency defines the expected consistency level for an operation."
    ],
    "<root value>": [
        "The root value is prepended to all registry paths so as to create the absolute path."
    ],
    "<disable time>": [],
    "<retry logic>": [],
    "<store level>": [],
    "<disk format>": [],
    "<double colon>": [],
    "<notifier>": [],
    "<Hadooop, Hadoop, hadoop>": [
        "Hadoop streaming is a utility that comes with the Hadoop distribution.",
        "Hadoop YARN allows applications to run on the Hadoop cluster.",
        "Hadoop now requires Java 6.",
        "The date that Hadoop was compiled.",
        "Hadoop has a library package called Aggregate.",
        "Unlike the released documentation, which is part of Hadoop source tree, Hadoop Wiki is regularly edited by Hadoop Community.",
        "Hadoop now supports integration with Azure Storage as an alternative Hadoop Compatible File System.",
        "Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes.",
        "Once the provider is set in the Hadoop configuration, hadoop commands work exactly as if the secrets were in an XML file.",
        "This is the hadoop copied local so can fix bugs and make hbase-specific optimizations.",
        "Hadoop supports shell-like commands to interact with HDFS directly.",
        "You must choose which Hadoop to build against.",
        "Finally: Apache Hadoop is an open source project.",
        "Hadoop will send multiple IP addresses to ARGV when forking the topology script.",
        "Hadoop uses URIs to refer to files within a filesystem.",
        "Hadoop components are rack-aware.",
        "Hadoop has an option parsing framework that employs parsing generic options as well as running classes.",
        "Hadoop is written in Java and is supported on all major platforms.",
        "When Hadoop is configured to run in secure mode, each Hadoop service and each user must be authenticated by Kerberos.",
        "Hadoop should be capable of generating serialization code in multiple target languages and should be C++ and Java.",
        "Hadoop daemons obtain the rack information of the slaves in the cluster by invoking an administrator configured module.",
        "Hadoop has upgraded its dependency so that this class is deprecated.",
        "Then Hadoop system will look for a mount table with the name \"clusterX\" in the Hadoop configuration files.",
        "This .20S workaround should cease to exist when Hadoop supports token store.",
        "Hadoop Auth is a Java library consisting of a client and a server components to enable Kerberos SPNEGO authentication for HTTP.",
        "By doing this, Hadoop will reuse the entries in the distributed cache.",
        "Over last few years Apache Hadoop has become the de facto platform for distributed data processing using commodity hardware.",
        "In this configuration, Hadoop jobs run with the privileges of the invoking user.",
        "Thus Hive and Hive Web Interface cannot enforce more stringent security then Hadoop can.",
        "Hadoop is able to read/write such files using the S3N filesystem.",
        "check whether current hadoop supports sticky bit.",
        "hadoop trace -list shows list of loaded span receivers associated with the id.",
        "Hadoop metrics sent to Ganglia over multicast now support optional configuration of socket TTL.",
        "To facilitate this, Hadoop supports a notion of a default file system.",
        "Hadoop is setup in secure mode with the authentication type set to kerberos.",
        "Hadoop, including HDFS, is well suited for distributed storage and distributed processing using commodity hardware.",
        "Unit tests are only flagged as necessary with native or Java code, since Hadoop has no framework in place yet for other types of unit tests.",
        "For each target language Hadoop defines very simple input and output stream interfaces.",
        "JDK classes, hadoop classes and resources, and some select third-party classes are considered system classes, and are not loaded by the application classloader.",
        "In particular, a user wishing to interact with Hadoop or Hive requires access to many ports.",
        "Hadoop users have to describe their data in a simple data description language.",
        "Optionally, you may now configure the default path for Hadoop clients to use the new HA-enabled logical URI.",
        "Hadoop Auth also supports additional authentication mechanisms on the client and the server side via 2 simple interfaces.",
        "Hadoop should include as primitives commonly used builtin types from programming languages we intend to support.",
        "Hadoop generates code for serializing and deserializing record types to abstract streams.",
        "Hadoop provides an optional mode of execution in which the bad records are detected and skipped in further attempts.",
        "If Hadoop cannot authenticate with the S3 service endpoint, the client retries a number of times before eventually failing.",
        "By default, Hadoop is configured to run in a non-distributed mode, as a single Java process.",
        "Hadoop doesn't have a Company-Private classification, which is meant for APIs which are intended to be used by other projects within the company, since it doesn't apply to opensource projects.",
        "But Hadoop also supports special group mapping mechanisms through LDAP and composition of LDAP and operating system group name resolution, which require additional configurations.",
        "Alternatively Hadoop provides a block based file system using S3 as a backing store.",
        "Hadoop should support widely used composite types such as structs and vectors.",
        "Hadoop 2 added iterative listing to handle the challenge of listing directories with millions of entries without buffering at the cost of consistency.",
        "Hadoop 2 is strongly encouraged (faster but also has fixes that help MTTR).",
        "hadoop s3guard destroy -meta dynamodb://ireland-team -region eu-west-1 Clean up a table, s3guard prune.",
        "By default, Hadoop makes two LDAP queries per user if this value is empty.",
        "Kosmos FS (KFS) is no longer maintained and Hadoop support has been removed.",
        "Required command line arguments: Optional command line arguments: Hadoop offline edits viewer.",
        "Hadoop provides an option where a certain set of bad input records can be skipped when processing map inputs.",
        "Hadoop will call us multiple times in the event of failure.",
        "Hadoop recommends that this value should be less than the ZKFC session timeout value.",
        "Hadoop comes configured with a single mandatory queue, called 'default'.",
        "Hadoop set this to 1 by default, whereas Hive uses -1 as its default value.",
        "Hadoop assumes that filesystems are consistent; that creation, updates and deletions are immediately visible, and that the results of listing a directory are current with respect to the files within that directory.",
        "Hadoop Auth enforces authentication on protected resources, once authentiation has been established it sets a signed HTTP Cookie that contains an authentication token with the user name, user principal, authentication type and expiration time.",
        "Hadooop should include support in the form of headers, libraries, packages for supported target languages that enable easy inclusion and use of generated code in applications.",
        "If set, Hadoop will attempt to resolve group names from this attribute, instead of making the second LDAP query to get group objects.",
        "Typically, Hadoop resolves a user's group names by making two LDAP queries: the first query gets the user object, and the second query uses the user's Distinguished Name to find the groups."
    ],
    "<select files, selected files>": [],
    "<memory systems>": [],
    "<high code>": [],
    "<Xuefu>": [],
    "<CRLA>": [],
    "<step number>": [
        "Each step is labeled with its step number."
    ],
    "<entire order>": [],
    "<META table, meta table>": [
        "Checks if the table is a hbase:meta table.",
        "The hbase:meta table (previously called .META.) keeps a list of all regions in the system.",
        "Here are others that you may have to take into account: The hbase:meta table is forced into the block cache and have the in-memory priority which means that they are harder to evict.",
        "The hbase:meta table structure is as follows: Region key of the format ([table],[region start key],[region id])."
    ],
    "<input policies, input policy>": [],
    "<Block Cache, Block cache>": [
        "When true, default settings of the table and family are used (this will never override caching blocks if the block cache is disabled for that family or entirely)."
    ],
    "<input Paths, input paths>": [
        "If any input path points to an empty table or partition a dummy file in the scratch dir is instead created and added to the list."
    ],
    "<passed Table, passed table>": [],
    "<ADMIN, Admin, Admins, admin, admins>": [
        "Global - permissions granted at global scope allow the admin to operate on all tables of the cluster.",
        "So admin added GPU label to h5.",
        "And datanode admin states include the followings: NORMAL The node is in service.",
        "Admin states are part of the namenode's webUI and JMX.",
        "This allows admins to specify which blockpools the balancer will run on.",
        "Cluster admins are allowed by default to read timeline data.",
        "The admin scans the list to see which backup was created at a date and time closest to the recovery objective.",
        "Admin (A) - can perform cluster operations such as balancing the cluster or assigning regions at the given scope.",
        "In a typical production environment, an admin should not have Read or Write permissions to data within tables.",
        "In this case, the admin decides to overwrite the data because it is corrupted."
    ],
    "<REST requests>": [],
    "<HDFS, Hadoop Distributed File System, hdfs>": [
        "It is a synonym for hdfs dfs when HDFS is in use.",
        "HDFS is on our CLASSPATH.",
        "In a typical cluster HDFS and YARN services will be launched as the system hdfs and yarn users respectively.",
        "If HDFS is being used, hdfs dfs is a synonym.",
        "Projects that access HDFS can depend on the hadoop-hdfs-client module instead of the hadoop-hdfs module to avoid pulling in unnecessary dependency.",
        "HDFS Federation addresses this limitation by adding support for multiple Namenodes/namespaces to HDFS.",
        "HDFS access is authorized through the use of HDFS permissions.",
        "If required, HDFS could be placed in Safemode explicitly using bin/hdfs dfsadmin -safemode command.",
        "HDFS supports the fsck command to check for various inconsistencies.",
        "HDFS datanodes simply see a stream of encrypted bytes.",
        "The default HDFS location is /hbase/coprocessor.",
        "HDFS now supports the option to configure AES encryption for block data transfer.",
        "HDFS is the primary distributed storage used by Hadoop applications.",
        "HDFS supports a traditional hierarchical file organization.",
        "HDFS has to be treated as correct in its behavior.",
        "HDFS encryption is able to provide good performance and existing Hadoop applications are able to run transparently on encrypted data.",
        "HDFS does not support this.",
        "HDFS is a distributed file system that is well suited for the storage of large files.",
        "In this example, HDFS is running on the localhost at port 8020.",
        "In Hadoop v2, HDFS supports highly-available (HA) namenode services and wire compatibility.",
        "HDFS supports user quotas and access permissions.",
        "Added HDFS file access times.",
        "However, the normative specification of the behavior of this class is actually HDFS: if HDFS does not behave the way these Javadocs or the specification in the Hadoop documentations define, assume that the documentation is incorrect.",
        "In either case HDFS daemons will bind to a single IP address making the daemons unreachable from other networks.",
        "Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines.",
        "HDFS supports writing to off-heap memory managed by the Data Nodes.",
        "Previously hdfs client was using commons-logging as the logging framework.",
        "hdfs fsck <path> // only show decommission state hdfs fsck <path> -maintenance // include maintenance state \u00a9 2018 Apache Software Foundation - Privacy Policy.",
        "HDFS also has more context than traditional filesystems when it comes to making policy decisions.",
        "HDFS is designed to reliably store very large files across machines in a large cluster.",
        "By default HDFS endpoints are specified as either hostnames or IP addresses.",
        "For instance, HDFS serializes using protobuf.",
        "HDFS is highly configurable with a default configuration well suited for many installations.",
        "HDFS provides best-effort persistence guarantees for Lazy Persist Writes.",
        "HDFS stores the data on the local hard disks, avoiding network traffic if the code can be executed on that host.",
        "HDFS allows user data to be organized in the form of files and directories.",
        "HDFS does not support hard links or soft links.",
        "HDFS takes a while to mark a node as dead.",
        "Compute HDFS blocks distribution of a given file, or a portion of the file.",
        "HDFS will create a \".Trash\" subdirectory when creating a new encryption zone to support soft delete for files deleted within the encryption zone.",
        "HDFS replicates data for faster query performance.",
        "HDFS will deduct quotas from both target storage type based on storage policy and the overall space quota.",
        "HDFS append internally guarantees that only a single writer may append to a path at a given time.",
        "HDFS will no longer support upgrades from versions without CRCs for block data.",
        "HDFS supports extended attributes out of the box, without additional configuration.",
        "Instead, HDFS moves it to a trash directory (each user has its own trash directory under /user/<username>/.Trash).",
        "HDFS persists metadata (the image and edit logs) in a particular format.",
        "To support this strong guarantee without losing the flexibility of using different encryption zone keys in different parts of the filesystem, HDFS allows nested encryption zones.",
        "and HDFS will still contain references to the parent region.",
        "By default, HDFS does not mark a node as dead until it is unreachable for 630 seconds.",
        "You might consider this profile when you are intent on a simple deploy profile, the loading is light, but the HDFS where data is replicated ensures the latter.",
        "HDFS (as of this writing) writes checksums to a separate file than the data file necessitating extra seeks.",
        "HDFS now supports ACLs (Access Control Lists).",
        "However, HDFS will continue to run without any impact.",
        "Most recent deleted files are moved to the current trash directory (/user/<username>/.Trash/Current), and in a configurable interval, HDFS creates checkpoints (under /user/<username>/.Trash/<date>) for files in current trash directory and deletes old checkpoints when they are expired.",
        "HDFS will make a best effort to lazily write these files to persistent storage, however file contents may be lost at any time due to process/ node restarts, hence there is no guarantee of data durability.",
        "While HDFS meets all these requirements directly, eventually consistent object stores may not -hence these tests.",
        "While MR and HDFS are always released in sync today, they may change down the road.",
        "HDFS Snapshots are read-only point-in-time copies of the file system.",
        "That is, if it has an out of date HDFS token \u2014that token is not renewed.",
        "HDFS applications need a write-once-read-many access model for files.",
        "HDFS now can choose to append data to a new block instead of end of the last partial block."
    ],
    "<directory number>": [],
    "<MVN, mvn>": [
        "Clean the checkout dir $ mvn clean $ git clean -f -x -d."
    ],
    "<Metrics implementation, metrics implementation>": [],
    "<memory intensive>": [],
    "<given credentials>": [],
    "<Memory store, memory store>": [],
    "<pretty format>": [],
    "<connected cluster>": [],
    "<Javadocs, javadocs>": [
        "Javadoc warnings are checked during precommit."
    ],
    "<counts row>": [],
    "<partitioned table>": [
        "If the table is not a partitioned table then an error is thrown.",
        "If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed.",
        "When the table is being read, a S lock is acquired, whereas an X lock is acquired for all other operations (insert into the table, alter table of any kind etc.).For a partitioned table, the idea is as follows:A 'S' lock on table and relevant partition is acquired when a read is being performed."
    ],
    "<SERDE, Serde, serde>": [],
    "<destination table>": [],
    "<Input query, input query>": [],
    "<Timeseries, timeseries>": [],
    "<column order>": [],
    "<path property>": [],
    "<SATA>": [],
    "<queue state>": [
        "If the new queue state is not \"running,\" jobs in progress will continue, but no other jobs from that queue will be started."
    ],
    "<master level>": [],
    "<integer counter>": [],
    "<global directory>": [],
    "<table access>": [
        "Once created, table access is via an instance of Table."
    ],
    "<YARN>": [
        "In order to scale YARN beyond few thousands nodes, YARN supports the notion of Federation via the YARN Federation feature.",
        "YARN supports a rudimentary registry which allows YARN Application Masters to register a web URL and an IPC address.",
        "YARN Nodes could be decommissioned NORMAL or GRACEFUL.",
        "YARN Applications and containers publishing their management and metrics bindings.",
        "OSS YARN has been known to scale up to about few thousand nodes.",
        "YARN is known to scale to thousands of nodes.",
        "YARN remains responsible for the management and allocation of resources.",
        "Hadoop YARN allows applications to run on the Hadoop cluster.",
        "In a typical cluster HDFS and YARN services will be launched as the system hdfs and yarn users respectively.",
        "YARN Containers registering their public service endpoints.",
        "YARN will create directories with appropriate permissions for users where YARN deployed services can be registered by a user.",
        "YARN containers in a secure cluster use the operating system facilities to offer execution isolation for containers.",
        "YARN deployed services instances must be able register their bindings and be discovered by clients.",
        "YARN will deploy them across the cluster depending on the individual each component requirements and server availability.",
        "YARN has an option parsing framework that employs parsing generic options as well as running classes.",
        "For a completely functional NM restart, YARN relies on any auxiliary service configured to also support recovery.",
        "YARN services SHOULD be registered using the following convention: /users/{username}/{serviceclass}/{instancename}.",
        "YARN applications should propagate the user name of the user launching an application by setting this environment variable.",
        "YARN commands are invoked by the bin/yarn script.",
        "YARN distributed shell: in hadoop-yarn-applications-distributedshell project after you set up your development environment.",
        "Hence other than the five defined enums, YARN can consider other integers also.",
        "YARN applications that attempt to use new APIs (including new fields in data structures) that have not yet been deployed to the cluster can expect link exceptions.",
        "Use case: In a secure cluster, YARN requires the user hdfs delegation-tokens to do localization and log-aggregation on behalf of the user.",
        "If the resource is a tgz, zip, or jar - you can have YARN unzip it.",
        "DEFAULT : While submitting a job, if the user is not specifying priority, YARN has the capability to pick the default priority as per its config."
    ],
    "<ustring>": [],
    "<RHEL>": [],
    "<double comparison>": [],
    "<ORDER>": [
        "Update 30-Dec-2009: Based on feedback in JIRA, ORDER BY is important as forward-looking to materialized views."
    ],
    "<capacity numbers>": [
        "</description> Attempting to perform more IO than the capacity requested simply throttles the IO; small capacity numbers are recommended when initially experimenting with S3Guard."
    ],
    "<HQL>": [],
    "<specified master>": [
        "Wait for the specified master to stop."
    ],
    "<location number>": [],
    "<store scanner>": [],
    "<ephemeral node>": [],
    "<Cache files, cache files, cached files>": [
        "Clearly the cache files should not be modified by the application or externally while the job is executing.",
        "The cache file into which container token is written.",
        "Adjust this property can make cache file be available for the time as you want.",
        "The cache files are checked at the client side for public/private access on the file system, and that information is passed in the configuration.",
        "Cached file metadata (or overrides as the case may be)."
    ],
    "<Tiered Compaction>": [],
    "<service failure>": [],
    "<Bucket size, bucket size>": [
        "Bucket size is 1 / error rate."
    ],
    "<data availability>": [],
    "<view names>": [],
    "<RTP>": [],
    "<NULL>": [
        "The NULL in the union should be silently removed.",
        "NOT NULL is not inferred on UNIQUE and needs to be explicitly declared."
    ],
    "<values array>": [],
    "<history directory>": [],
    "<table lock>": [],
    "<data leakage>": [],
    "<data paths>": [],
    "<double literal>": [],
    "<DATA blocks, Data Block, Data Blocks, Data block, data block, data blocks>": [
        "Data block encoder used for data blocks.",
        "Data block index reader keeping the root data index in memory.",
        "Default data block encoding algorithm.",
        "This size is approximate, because Bloom blocks can only be inserted at data block boundaries, and the number of keys per data block varies.",
        "Encapsulates a data block compressed using a particular encoding algorithm.",
        "Bloom filter blocks and index blocks (we call these \"inline blocks\") become interspersed with data blocks, and as a side effect we can no longer rely on the difference between block offsets to determine data block length, as it was done in version 1.",
        "Smaller blocks are good for random access, but require more memory to hold the block index, and may be slower to create (because we must flush the compressor stream at the conclusion of each data block, which leads to an FS I/O flush).",
        "If the DATA blocks fit inside fscache, this alternative may make sense when access is completely random across a very large dataset."
    ],
    "<read entity>": [],
    "<Remote directory>": [
        "The remote directory for the jobs is remote/a and the local directory for storing output is local/output."
    ],
    "<output schema>": [],
    "<array stores>": [],
    "<DN, database name, database names>": [
        "This setting controls how much DN volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced."
    ],
    "<intermediate format>": [],
    "<Registry Service>": [
        "Registry service properties: The registry must be highly available."
    ],
    "<memory parameters>": [
        "Modifies shuffle related memory parameters to use 'long' from 'int' so that sizes greater than maximum integer size are handled correctly."
    ],
    "<Data Access, data access>": [],
    "<complete key>": [],
    "<optimal number>": [],
    "<SSD, SSDs, ssd>": [],
    "<netgroup>": [],
    "<filter incompatibility>": [],
    "<Common Table>": [],
    "<output recovery>": [
        "If task output recovery is supported, job restart can be done more efficiently."
    ],
    "<columns values>": [],
    "<kill request>": [
        "If it's already running, a kill request will be sent to it."
    ],
    "<Spark cluster>": [],
    "<replication service>": [],
    "<provided bytes>": [],
    "<instance name>": [],
    "<table called>": [
        "For this example, assume that a table called normal exists with a single double column called val, containing a large number of random number drawn from the standard normal distribution.",
        "hive> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING); creates a table called invites with two columns and a partition column called ds."
    ],
    "<completion time>": [
        "Remove the sentinels that are marked as finished and the completion time has exceeded the removal timeout."
    ],
    "<heap bytes>": [],
    "<index block>": [],
    "<limit number>": [],
    "<context array>": [],
    "<Index table, index table>": [],
    "<side memory>": [],
    "<DATA LOCAL, Data Locality, Data locality, data local, data locality>": [],
    "<memory table>": [],
    "<initialize time>": [],
    "<Data consumer>": [],
    "<performance code>": [],
    "<virtual memory>": [
        "Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.",
        "The virtual memory limit is set as a configurable multiple of the physical limit."
    ],
    "<sequence files>": [
        "Existing sequence files can be appended.",
        "Text files require an extension, whereas others, like sequence files, do not.",
        "When two sequence files, which have same Key type but different Value types, are mapped out to reduce, multiple Value types is not allowed."
    ],
    "<table link, table links>": [
        "I agree that table links are a degenerate case of views.",
        "I don't think dynamic table links satisfy the use case covered by Team 2's access requirements for table T1.",
        "If we assume that it's possible to make this change then from a quota management standpoint I don't think the table links makes quota management easier.",
        "I don't think table links make this process atomic, and as I mentioned above the process of maintaining this linked set of partitions actually seems easier if you use views instead."
    ],
    "<given Cell, given cell>": [
        "Deep clones the given cell if the cell supports deep cloning."
    ],
    "<reduction keys>": [],
    "<data transferred>": [],
    "<Chukwa>": [
        "Chukwa supports pipelined writers for improved extensibility."
    ],
    "<directory quota>": [],
    "<least number>": [],
    "<DDL, DDLs, ddl>": [],
    "<row group>": [
        "In a row group (rows having the same key), rows are also sorted by their tags."
    ],
    "<binary secret>": [],
    "<Quantiles, quantile, quantiles>": [],
    "<flush cache>": [],
    "<data compaction>": [],
    "<lock node>": [],
    "<pending memory>": [],
    "<export time>": [],
    "<max sequence>": [],
    "<SARG>": [],
    "<side filter>": [],
    "<Bucketized Table>": [],
    "<CNAMEs>": [],
    "<High Availability, High availability, high availability>": [],
    "<key sent>": [],
    "<foreign keys>": [
        "All foreign keys for a particular table can be fetched by passing null for the last two arguments."
    ],
    "<small files>": [],
    "<secret directory>": [],
    "<right level>": [
        "The right level of parallelism for maps seems to be around 10-100 maps per-node, although it has been set up to 300 maps for very cpu-light map tasks."
    ],
    "<data diverges>": [],
    "<iSCSI>": [],
    "<secure token>": [],
    "<ultimate control>": [],
    "<Output pairs, output pairs>": [
        "Output pairs need not be of the same types as input pairs."
    ],
    "<checkpointed>": [],
    "<Retry times, retry times>": [
        "This places a limit even if the retry times and interval limit, combined with the backoff policy, result in a long retry period."
    ],
    "<data security>": [],
    "<carry table>": [],
    "<encryption scheme>": [],
    "<longer time>": [],
    "<table group>": [],
    "<cluster space>": [],
    "<skewed keys>": [
        "The skewed keys in A are only read and processed by the Mapper, and not sent to the reducer."
    ],
    "<actual time>": [],
    "<scan info>": [],
    "<Full table, full table>": [],
    "<shm>": [],
    "<key portion>": [],
    "<separate table>": [
        "If you leave it unset/empty, a separate table will be created for each S3 bucket you access, and that bucket's name will be used for the name of the DynamoDB table."
    ],
    "<signal value>": [],
    "<interval value>": [],
    "<cluster communication>": [],
    "<whole row>": [],
    "<old format>": [
        "If the value is not specified, old format fsimage will not be saved in checkpoint."
    ],
    "<serializers>": [
        "Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce)."
    ],
    "<table coprocessor>": [
        "In this version, the coprocessor is loaded dynamically (table coprocessor for the flowrun table).",
        "If 'false' (disabled), any table coprocessor attributes in table descriptors will be ignored."
    ],
    "<large requests>": [],
    "<Truncate Table, truncate table>": [],
    "<System table, system table>": [],
    "<column stored>": [],
    "<HCD>": [],
    "<digest bytes>": [],
    "<nohup>": [],
    "<output folder>": [],
    "<Trie, trie>": [],
    "<reinitialization>": [
        "Once the reinitialization has been committed, It cannot be rolled back."
    ],
    "<startcode>": [
        "The startcode distinguishes restarted servers on same hostname and port (startcode is usually timestamp of server startup).",
        "What to use if no startcode supplied."
    ],
    "<Time ranges, time range, time ranges>": [
        "The WAL can be replayed for a set of tables or all tables, and a time range can be provided (in milliseconds)."
    ],
    "<Copy files, copy files>": [],
    "<tfile>": [],
    "<aggregated number>": [],
    "<mlock>": [],
    "<disk checks>": [],
    "<CQ, Column Qualifier, Column qualifiers, column qualifier, column qualifiers, cq>": [
        "The column qualifier and the timestamp are optional.",
        "This class is used for the tracking and enforcement of columns and numbers of versions during the course of a Get or Scan operation, when explicit column qualifiers have been asked for in the query."
    ],
    "<classnames>": [],
    "<resource usages>": [
        "If multiple queues are unsatisfied in this situation, resources go to the queue with the smallest ratio between relevant resource usage and minimum."
    ],
    "<Row Keys, Row keys, row key, row keys>": [
        "Rows are sorted alphabetically by the row key as they are stored.",
        "This effectively randomizes row keys, but sacrifices row ordering properties.",
        "Row keys are uninterpreted bytes.",
        "If the row key is made of arbitrary bytes, the charset ISO-8859-1 is recommended.",
        "However, poorly designed row keys are a common source of hotspotting.",
        "Your first schema is \"tall\": each row represents one value for one user, and so there are many rows in the table for each user; the row key is user + valueid, and there would be (presumably) a single column qualifier that means \"the value\"."
    ],
    "<metrics records>": [
        "Each metrics record contains Hostname tag."
    ],
    "<bytes apiece>": [],
    "<aggregate length>": [],
    "<Storage Directory, storage directories, storage directory>": [],
    "<input references>": [],
    "<backoff policy>": [
        "Again, some backoff policy with some jitter helps stop a newly-restarted AM being overloaded."
    ],
    "<dbcp>": [],
    "<contrib>": [],
    "<long comparison>": [],
    "<RCP>": [],
    "<point key>": [],
    "<OLTP>": [],
    "<change request>": [],
    "<confdir>": [],
    "<Maximum capability, maximum capability>": [],
    "<Storage requires>": [],
    "<cache capacity>": [],
    "<specified lock>": [
        "Called when the specified lock has failed."
    ],
    "<flow level>": [],
    "<zero status>": [],
    "<sequence ID>": [
        "If the sequence of the last edit is greater than or equal to the sequence ID included in the file name, it is clear that all writes from the edit file have been completed."
    ],
    "<write patterns>": [],
    "<Metastore filter>": [],
    "<index information>": [
        "The metadata and index information can be cached even for data that is not currently cached."
    ],
    "<jstack>": [
        "jstack is one of the most important tools when trying to figure out what a java process is doing apart from looking at the logs."
    ],
    "<running state>": [
        "Beyond all the groundwork that has been done in Phase 1 to ensure the persistency of application state and reload that state on recovery, Phase 2 primarily focuses on re-constructing the entire running state of YARN cluster, the majority of which is the state of the central scheduler inside RM which keeps track of all containers' life-cycle, applications' headroom and resource requests, queues' resource usage etc."
    ],
    "<client name>": [],
    "<flush period>": [],
    "<vector header>": [],
    "<Partition keys, partition key, partition keys>": [],
    "<encoding code>": [],
    "<SQM>": [],
    "<LIST code>": [],
    "<fail message>": [],
    "<master instance>": [
        "Called immediately after an active master instance has completed initialization."
    ],
    "<root privileges>": [
        "No root privileges are required for this command."
    ],
    "<single queue>": [],
    "<JSONP>": [],
    "<xattr, xattrs>": [
        "This xattr can be set and accessed by any user, assuming normal filesystem permissions.",
        "raw xattrs are preserved based solely on whether /.reserved/raw prefixes are supplied.",
        "Administrators could potentially be interested in the options limiting the number of xattrs per inode and the size of xattrs, since xattrs increase the on-disk and in-memory space consumption of an inode.",
        "This xattr does not allow a value to be set.",
        "If the xattr exists already, exception will be thrown.",
        "This xattr can only be set on files, and it will prevent the superuser from reading the file's contents.",
        "Determination of whether raw.* namespace xattrs are preserved is independent of the -p (preserve) flag.",
        "This xattr is also write-once, and cannot be removed once set."
    ],
    "<table parameter>": [],
    "<fieldID>": [],
    "<submit time>": [],
    "<metrics collected, metrics collector>": [],
    "<serverside>": [],
    "<disk capacity>": [],
    "<resource availability>": [],
    "<Cluster Information, cluster information>": [],
    "<startdate>": [],
    "<read time>": [],
    "<specified nodes>": [
        "Creates the specified node, iff the node does not exist."
    ],
    "<side information>": [],
    "<table identification>": [],
    "<vertex name>": [],
    "<fastpath>": [],
    "<quota state>": [],
    "<ltt>": [],
    "<S3, s3>": [
        "AWS S3 endpoint to connect to.",
        "SSE-S3 is where S3 will manage the encryption keys for each object.",
        "S3 Frankfurt and Seoul only support the V4 authentication API.",
        "Amazon S3 is an eventually consistent object store.",
        "Appendix S3 for n00bs One of the things useful to understand is how S3 is used as a file system normally.",
        "The minimum multipart size which S3 supports.",
        "Amazon S3 uses a set of front-end servers to provide access to the underlying data.",
        "S3 writes each inbound and outbound file to disk, which adds overhead to each operation.",
        "These operations can be significantly slower when S3 is the destination compared to HDFS or other \"real\" filesystem.",
        "Amazon S3 is an example of \"an object store\".",
        "S3 is an excellent place to store data for the long term.",
        "S3 is an obvious choice - but the user must restore and backup Hive metadata at the launch and termination of the Hadoop cluster.",
        "In order to achieve scalability and especially high availability, S3 has \u2014as many other cloud object stores have done\u2014 relaxed some of the constraints which classic \"POSIX\" filesystems promise.",
        "S3 of course has no \"native\" support for them.",
        "S3 Buckets are hosted in different \"regions\", the default being \"US-East\".",
        "NOTE: s3: is being phased out."
    ],
    "<internal name>": [],
    "<uncompressed format>": [],
    "<Write records, write records>": [],
    "<heap size, heap sizes>": [
        "As the heap size is increased, the operation rate linearly increases."
    ],
    "<cache instance>": [],
    "<column schema>": [],
    "<live node>": [],
    "<security entity>": [],
    "<table filter>": [],
    "<resource type>": [
        "</description> If a resource type is listed, it will check the shared cache to see if the resource is already in the cache.",
        "The distributed cache api: If a resource is specified via the distributed cache the resource will not use the shared cache regardless of if the resource type is enabled for the shared cache.",
        "These resources will be placed in the distributed cache and, if their resource type is enabled the client will use the shared cache as well."
    ],
    "<Cache pool, Cache pools, cache pool>": [
        "A cache pool is an administrative entity used to manage groups of cache directives.",
        "Cache pools are also used for resource management.",
        "Cache pools also track a number of statistics to help cluster users determine what is and should be cached.",
        "Cache pools have UNIX-like permissions, which restrict which users and groups have access to the pool."
    ],
    "<rows list>": [],
    "<Failure detection, failure detection>": [],
    "<delegation token identifier>": [],
    "<DDOS>": [],
    "<Data Distribution, data distribution>": [],
    "<Uploader, uploader>": [],
    "<Schema Creation, schema creation>": [],
    "<empty cells>": [],
    "<CPU count>": [],
    "<memory estimation>": [
        "However, memory estimation could be tricky especially considering interned string."
    ],
    "<equal number>": [],
    "<cache metrics>": [],
    "<table storage>": [],
    "<Existing Table, existing Table, existing table>": [],
    "<disk device>": [
        "Similar to posix fsync, flush out the data in client's user buffer all the way to the disk device (but the disk may have it in its cache)."
    ],
    "<single metric>": [],
    "<work directory>": [],
    "<SLAs>": [],
    "<output collector>": [],
    "<temporary table>": [],
    "<store counts>": [],
    "<Script time, script time>": [],
    "<put files>": [],
    "<fixed row>": [],
    "<memory index>": [],
    "<role names>": [
        "This is because, unlike role names, user names are not managed within Hive."
    ],
    "<Libaries>": [],
    "<artifact names>": [],
    "<Disk usage, disk usage>": [
        "Once the disk usage reaches below the threshold, name node will be put out of the safe mode."
    ],
    "<Cluster Metrics, cluster metrics>": [],
    "<split request>": [
        "A region split request is enqueued if the policy recommends it."
    ],
    "<storage capacity>": [
        "Archival Storage is a solution to decouple growing storage capacity from compute capacity."
    ],
    "<data tens>": [
        "Rewriting the same data tens of times is the last thing you want."
    ],
    "<Parameters table>": [],
    "<end status>": [],
    "<impls>": [],
    "<Namit>": [
        "Permalink Delete comments Namit Jain Depends on what the use case is.",
        "Namit offered to host the next contrib meeting at Facebook.",
        "I think Namit meant this in reference to the design option of using a single database and using scripts for quota management.",
        "Carl raised concerns about potential dependency creep preventing future Hive refactoring, and Namit Jain proposed reworking the approach to restrict it to pre/post-analysis hooks (limiting the dependencies exposed) rather than full-blown analyzer pluggability+subclassing."
    ],
    "<data recovery>": [],
    "<privilege name>": [],
    "<service application>": [],
    "<Commit Message, commit message>": [
        "The commit message should contain the JIRA ID and a description of what the patch does.",
        "These typically take the form of a veto (-1) in reply to the commit message sent when the commit is made.",
        "If the commit message is not appropriate, you can still use the commit, then run git commit --amend and reword as appropriate."
    ],
    "<rdiff>": [],
    "<Automated code>": [],
    "<sorted array>": [],
    "<maximal number>": [],
    "<data loads>": [],
    "<system directory>": [
        "The mapred system directory generated by HOD is cleaned up at cluster deallocation time."
    ],
    "<snapshottable directory>": [
        "If there are snapshots in a snapshottable directory, the directory can be neither deleted nor renamed before all the snapshots are deleted.",
        "A snapshottable directory is able to accommodate 65,536 simultaneous snapshots."
    ],
    "<percentile values>": [],
    "<noatime>": [],
    "<access priority>": [],
    "<merged directory>": [],
    "<Check table>": [],
    "<Bloom information>": [
        "Bloom information from the cell is retrieved."
    ],
    "<compaction state>": [],
    "<LCE>": [
        "The LCE requires that container-executor binary be owned by root:hadoop and have 6050 permissions.",
        "The LCE also provides enhanced security and is required when deploying a secure cluster.",
        "If so, the command will be overridden when LCE launches the image with YARN's container launch script.",
        "When the LCE launches a YARN container to execute in a Docker container, the application can specify the Docker image to be used.",
        "If running in non-secure mode, by default, the LCE runs all jobs as user \"nobody\"."
    ],
    "<binary key>": [],
    "<WebUI, webUI, webui>": [
        "The class is synchronized, as WebUI may access information about a running query."
    ],
    "<Output table, output table, output tables>": [
        "Checks if the output table exists and is enabled."
    ],
    "<schema version>": [],
    "<SMB, smb>": [
        "SMB joins are used wherever the tables are sorted and bucketed."
    ],
    "<metrics context>": [],
    "<col4>": [],
    "<Index code>": [],
    "<column titles>": [],
    "<Repos, repo, repos>": [],
    "<storage ratio>": [],
    "<checksum name>": [],
    "<DROP TABLE, Drop Table, Drops table, drop table>": [
        "DROP TABLE removes metadata and data for this table.",
        "Insert, select, create table, drop table, create database, add partition, drop partition have been tested."
    ],
    "<hfile, hfiles>": [],
    "<dead values>": [],
    "<index size>": [
        "This is optimized for point-lookups in the case where a value typically occurs more than once in nearby rows; the index size is kept small since there are many fewer blocks than rows."
    ],
    "<requisite number>": [],
    "<capacity cluster>": [],
    "<Added time>": [
        "Added time, permission and user attribute support to libhdfs."
    ],
    "<operation type>": [],
    "<counter limit>": [],
    "<specified metric>": [],
    "<error bounds>": [],
    "<partition name, partition names>": [
        "Basically this partition will contain all \"bad\" rows whose value are not valid partition names."
    ],
    "<local size>": [],
    "<AES>": [
        "AES offers improved cryptographic strength and performance over the prior options of 3DES and RC4.",
        "By default, this is unspecified, so AES is not used."
    ],
    "<Partition size, partition size>": [],
    "<storage solution>": [],
    "<QL, ql, query level>": [],
    "<read requests>": [
        "This could indicate that read requests are often slow, or that hedged reads are triggered too quickly."
    ],
    "<table restore>": [
        "When a table restore operation starts, a two-step process is initiated."
    ],
    "<metrics tag>": [],
    "<family path>": [],
    "<Trace values>": [
        "Trace values retrived from the database."
    ],
    "<web request>": [],
    "<Cluster size, cluster size>": [
        "Cluster size tends to top out at a couple of hundred thousand regions; beyond this, cluster start/stop takes hours and is prone to corruption."
    ],
    "<IRB, irbrc>": [],
    "<xattr name>": [],
    "<input directories, input directory>": [],
    "<Tarball, tarball, tarballs>": [],
    "<catalog table>": [],
    "<Output path, output path, output paths>": [
        "Thus the output of the job is: < Bye, 1> < Goodbye, 1> < Hadoop, 2> < Hello, 2> The main method specifies various facets of the job, such as the input/output paths (passed via the command line), key/value types, input/output formats etc., in the Job."
    ],
    "<host3>": [],
    "<bucket number>": [
        "This is similar to hash bucketing currently, where the bucket number determines the file number."
    ],
    "<Forrest, forrest>": [
        "To commit documentation changes you must have Forrest installed and the forrest executable on your $PATH."
    ],
    "<cache service>": [
        "First, your YARN cluster must have the shared cache service running."
    ],
    "<given Store, given store, given stores>": [],
    "<TLP>": [
        "The TLP and bylaws votes passed, so Hive is now officially an Apache top level project!"
    ],
    "<Plugin, Plugins, plugin, plugins>": [
        "This plugin is run when you specify the site goal as in when you run mvn site.",
        "In our build we use a maven plugin for convenience; however, the plugin may not be able to retrieve appropriate binaries for all platforms.",
        "Plugins for checkstyle, shellcheck, and whitespace now execute as necessary.",
        "The plugin can decide how two keys are joined.",
        "If not, click Available Plugins, select it, and click Install."
    ],
    "<column vectors>": [
        "Bytes column vectors allow a by reference entry to bytes."
    ],
    "<master cluster>": [
        "The master cluster relies on randomization to attempt to balance the stream of replication on the slave clusters.",
        "So if the master cluster crashes, the slave cluster may not have the newest data."
    ],
    "<trichotomous value>": [],
    "<configured size>": [],
    "<history version>": [],
    "<Token Service, token service>": [],
    "<store value>": [],
    "<comparer>": [],
    "<Hortonworks>": [
        "Alan said that Hortonworks would like to contribute to this, and is interested in collaborating with others on it."
    ],
    "<storage timeline>": [],
    "<vtable, vtables>": [],
    "<side properties>": [],
    "<host2>": [
        "In the following example, host1 and host2 need to in service."
    ],
    "<YARN cluster, yarn cluster>": [
        "First, your YARN cluster must have the shared cache service running."
    ],
    "<primitive keys>": [],
    "<count threshold>": [],
    "<bit key>": [],
    "<data reliability>": [],
    "<cluster resources>": [],
    "<input statistics>": [
        "The work also details how additional input statistics can be exploited to improve efficiency."
    ],
    "<Restore table>": [],
    "<snapshot state>": [],
    "<refresh requests>": [
        "Pending refresh requests beyond this value are queued and processed when a thread is free."
    ],
    "<predicate information>": [],
    "<Thrift types>": [],
    "<Pull Request, pull request>": [],
    "<corrupt blocks>": [],
    "<PUT>": [
        "PUT and POST are equivalent for scans."
    ],
    "<pattern filter>": [],
    "<Palo>": [],
    "<string column>": [
        "The standard output of the user script will be treated as TAB-separated STRING columns, any cell containing only \\N will be re-interpreted as a NULL, and then the resulting STRING column will be cast to the data type specified in the table declaration in the usual way.",
        "If your STRING column contains tabs, an identity transformer will not give you back what you started with!"
    ],
    "<root location>": [
        "By default, this means the root location is stored at /hbase/root-region-server."
    ],
    "<Accumulo, accumulo>": [
        "Accumulo doc on how to contribute and develop is also good read to understand development workflow."
    ],
    "<Column Names, Column names, column names, columns names>": [
        "Skewed column name should be a valid column defined.",
        "Vectorization: Partition column names are not picked up.",
        "Column names in this string are unqualified references to the columns of the table over which the filter operates, as they are known in the Hive metastore.",
        "Only column names appear in PARTITIONED ON; no types etc.",
        "Note that skewed column name matches skewed value in order.",
        "Skewed column name and value should match.",
        "As part of this step, the column names are verified and expansions like * are performed.",
        "Any column name that is specified within backticks (`) is treated literally.",
        "(Initially, all column names are allowed.).",
        "Value can be \"none\" or \"column\".column: Column names can contain any Unicode character."
    ],
    "<part files>": [
        "All output part files are created regardless of whether the corresponding task has output."
    ],
    "<source table, source tables>": [],
    "<failure condition>": [],
    "<global level>": [],
    "<cache memory>": [],
    "<compute cost>": [],
    "<kinit>": [
        "The default setting assumes that kinit is in the PATH of users running the Hadoop client."
    ],
    "<null order>": [],
    "<Update state>": [],
    "<backoff>": [],
    "<export directory>": [],
    "<scratch directory>": [
        "If the lock is available, the scratch directory will be cleared.",
        "hive --service cleardanglingscratchdir [-r] [-v] [-s scratchdir] -r dry-run mode, which produces a list on console -v verbose mode, which prints extra debugging information The tool tests if a scratch directory is in use, and if not, will remove it."
    ],
    "<single split>": [],
    "<YARN prefix>": [],
    "<stopword>": [],
    "<merge table>": [],
    "<metric range>": [],
    "<index entries>": [
        "(A bit field or bloom filter could also be included.) Row index entries provide offsets that enable seeking to the right compression block and byte within a decompressed block."
    ],
    "<ramdisk>": [],
    "<ACID table>": [
        "External tables cannot be made ACID tables since the changes on external tables are beyond the control of the compactor (HIVE-13175).Reading/writing to an ACID table from a non-ACID session is not allowed."
    ],
    "<failed requests>": [],
    "<column node>": [],
    "<next key>": [],
    "<LIST>": [],
    "<cache entry, cached entries>": [
        "If successful the cache entry will be set to valid status and be usable for cached queries.",
        "Wait for the cache entry to go from PENDING to VALID status."
    ],
    "<Input records, input records>": [],
    "<input files>": [
        "'LOCAL' signifies that the input file is on the local file system."
    ],
    "<readahead>": [],
    "<short requests>": [
        "The short requests will be newer, so the result is not terrible, but still suboptimal compared to a mechanism which allows large requests to be split into multiple smaller ones."
    ],
    "<protection code>": [],
    "<Replication System, replication system>": [
        "The replication system has a fairly 'light touch', exhibiting a low degree of coupling and using the Hive-metastore Thrift service as an integration point.",
        "Could indicate dataloss since the replication system is unable to determine if the end of readable entries lines up with the expected end of the file."
    ],
    "<online table>": [],
    "<META, metainfo>": [
        "However, every time your cluster starts, META is scanned to ensure that it does not need to be converted."
    ],
    "<combined size>": [],
    "<Binary Type, Binary type, binary type>": [
        "Binary type will not be coerced into any other type implicitly.",
        "One important thing to note is this binary type is not modeled after blob type as it exists in other systems.",
        "Binary type will just piggyback on it and will reuse the same code."
    ],
    "<memory Compaction, memory compaction>": [
        "Called before in memory compaction started.",
        "No memory compaction, when size threshold is exceeded data is flushed to disk.",
        "In-memory compaction works best when high data churn; overwrites or over-versions can be eliminated while the data is still in memory."
    ],
    "<snapshot size>": [],
    "<given dimension>": [],
    "<stripe size>": [],
    "<PKI>": [],
    "<summing size>": [],
    "<client service>": [
        "intra-cluster traffic between two services may be routed over a private interface but the client service looked up its public hostname."
    ],
    "<Column Pruning, column pruning>": [],
    "<queue order>": [],
    "<decryptor>": [],
    "<aggregate size>": [],
    "<encrypted bytes>": [],
    "<group mapping>": [
        "The groups of a user is determined by a group mapping service provider."
    ],
    "<secure service>": [],
    "<Thrift metastore>": [],
    "<keylist>": [],
    "<key series>": [
        "The key series will be positioned to the beginning.",
        "The key series is logically indexed."
    ],
    "<proccesses, procs>": [],
    "<Max length, max length>": [],
    "<retry accounting>": [],
    "<single machine>": [],
    "<batch row>": [],
    "<output vectors>": [],
    "<integer identifier>": [],
    "<typed values>": [],
    "<reading length>": [],
    "<disk location>": [],
    "<pread>": [],
    "<Validation code>": [],
    "<dequoted name>": [],
    "<credential store>": [],
    "<cmds>": [],
    "<Slave nodes, slave node, slave nodes>": [
        "One node will be used as the master and as such the cluster will have 9 slave nodes.",
        "Slave nodes then use this reader context to read data."
    ],
    "<error due>": [],
    "<Protobuf, Protobufs, protobuf, protobufs>": [
        "Convert a protobuf token into a rpc token and set its service."
    ],
    "<Request service>": [],
    "<total Resource>": [],
    "<index search>": [],
    "<Quota Type>": [],
    "<Table integrity, table integrity>": [
        "Table integrity problems can require repairs that deal with overlaps."
    ],
    "<data applies>": [],
    "<HLL>": [],
    "<duplicate name>": [],
    "<access call>": [],
    "<maximum percentage>": [],
    "<reserved number>": [],
    "<storage endpoint>": [],
    "<Input format, input format>": [
        "A storage handler consists of a bundle of the following: input format output format serde metadata hooks for keeping an external catalog in sync with Hive's metastore rules for setting up the configuration properties on map/reduce jobs which access tables stored by this handler Storage handler classes are plugged in using the STORED BY 'classname' clause in CREATE TABLE."
    ],
    "<system location>": [],
    "<History Files, history files>": [
        "Every job history file consists of events.",
        "Post HADOOP-4372, empty job history files caused NPE.",
        "Parses history file and calls call back functions.",
        "The job history file at this path may or may not be existing depending on the job completion state."
    ],
    "<per table>": [],
    "<State Store>": [
        "The State Store combines a remote Mount Table (in the flavor of ViewFs, but shared between clients) and utilization (load/capacity) information about the subclusters."
    ],
    "<Writables, writables>": [
        "WRITABLE means converting all primitive objects to writable objects."
    ],
    "<time granularity>": [],
    "<Store statistics>": [],
    "<Store file, Store files, store file, store files, stores files>": [
        "Statistics about the snapshot How many store files and logs are in the archive How many store files and logs are shared with the table Total store files and logs size and shared amount.",
        "Store files which contains only expired rows are deleted on minor compaction.",
        "Store file refresher is a thread per region server, which runs periodically, and does a refresh operation for the store files of the primary region for the secondary region replicas."
    ],
    "<long keys>": [],
    "<single word>": [],
    "<destination cluster>": [
        "Create tables with the same names and column families on both the source and destination clusters, so that the destination cluster knows where to store data it will receive."
    ],
    "<create node>": [],
    "<UAM>": [],
    "<regionserver, regionservers>": [
        "It is called when the regionserver is stopped.",
        "The regionserver will throw some errors in its logs as it recalibrates where to get its data from \u2014 it will likely roll its WAL log too \u2014 but in general but for some latency spikes, it should keep on chugging.",
        "By default, all tables and regionservers belong to the APIs."
    ],
    "<nproc>": [],
    "<Thrift directory>": [],
    "<Architecture level>": [],
    "<forall>": [],
    "<edit files, edits files>": [
        "In case there is some problem with hadoop cluster and the edits file is corrupted it is possible to save at least part of the edits file that is correct.",
        "One edit file may contain several transactions (edits)."
    ],
    "<specfied>": [],
    "<Metrics instance, metrics instance>": [],
    "<max versions>": [],
    "<operation handle>": [],
    "<proxied>": [],
    "<passed list>": [],
    "<table referenced, table refers>": [],
    "<Meta Store, meta store>": [
        "NOTE: \u2022 Meta Store would need to answer queries with in clauses."
    ],
    "<block files>": [
        "If a block file is specified, we will verify that the checksums in the metadata file match the block file.",
        "If a block file is specified, we will compute the checksums from the block file, and save it to the specified output metadata file.",
        "Only use as a last measure, and when you are 100% certain the block file is good."
    ],
    "<table scope>": [],
    "<edits directory>": [],
    "<given permissions>": [],
    "<every row>": [
        "every row gets its own set of rows to process the UDAF on."
    ],
    "<Encrypted Key, encrypted Key, encrypted key>": [],
    "<status filter>": [],
    "<boot time>": [],
    "<DML>": [],
    "<reduced table>": [
        "The reduced table is then joined on c."
    ],
    "<Jobtracker, jobtracker>": [
        "Jobtracker was modified to cleanup reservations created on tasktracker nodes to support high RAM jobs, when the nodes are blacklisted."
    ],
    "<data statistics>": [],
    "<given split>": [],
    "<lookup time>": [],
    "<Storage documentation>": [],
    "<load files, loads files>": [],
    "<local store>": [],
    "<autonumber>": [],
    "<specified procedure>": [
        "The specified procedure was executed, and the new state should be written to the store."
    ],
    "<service calls>": [],
    "<statistics key>": [],
    "<read pattern>": [],
    "<Short Circuit, short circuit>": [
        "By default none of the above are enabled and short circuit read will not kick in."
    ],
    "<Vectorized>": [],
    "<short size>": [],
    "<fetch status>": [],
    "<export command>": [],
    "<specified counter>": [],
    "<release time>": [],
    "<access levels>": [],
    "<access special>": [],
    "<Type entry>": [],
    "<attempt ID>": [],
    "<time series>": [],
    "<Fsck, fsck>": [
        "Fsck now checks permissions as directories are traversed.",
        "fsck can be run on the whole file system or on a subset of files.",
        "By default fsck ignores open files but provides an option to select all files during reporting.",
        "The output of hdfs fsck now also contains information about decommissioning replicas.",
        "hdfs fsck <path> // only show decommission state hdfs fsck <path> -maintenance // include maintenance state \u00a9 2018 Apache Software Foundation - Privacy Policy."
    ],
    "<mountd>": [],
    "<single compaction>": [],
    "<result size>": [],
    "<sub directory>": [],
    "<batch key>": [
        "Generate optimized results when entire batch key is repeated and it matched the hash map."
    ],
    "<output duration>": [
        "Folding essentially means that the output duration of the resulting trace is fixed and job timelines are adjusted to respect the final output duration."
    ],
    "<Committer, Committers, committer, committers>": [
        "The intention is that submodule committers will work towards becoming committers.",
        "Submodule committers must be voted on by the PMC in the same way as other Hive contributors to become committers.",
        "Submodule committers are committers who are responsible for maintenance of a particular submodule of Hive.",
        "Submodule committers are not directly created by the PMC.",
        "Branch committers are responsible for shepherding their feature into an active release and do not cast binding votes or vetoes in the project.",
        "Committers on submodules may cast binding votes on any technical discussion regarding that submodule.",
        "New committers are encouraged to first read Apache's generic committer documentation: Apache New Committer Guide.",
        "Committer access is by invitation only and must be approved by lazy consensus of the active PMC members.",
        "A committer who makes a sustained contribution to the project may be invited to become a member of the PMC.",
        "Committers are busy people too.",
        "A Committer is considered emeritus by their own declaration or by not contributing in any form to the project for over six months.",
        "Committers should always be polite to contributors and try to instruct and encourage them to contribute better patches.",
        "Committers have access to a specified set of subproject's subversion repositories.",
        "If a committer wishes to improve an unacceptable patch, then it should first be rejected, and a new patch should be attached by the committer for review.",
        "Committers have access to and responsibility for all of Hive's source code repository.",
        "When Hive adopts new code bases, for example by merging in an existing project, committers on that newly adopted code base become committers on the submodules that correspond to the new code base.",
        "If a committer commits a patch, it is their responsibility to make sure it passes the test suite.",
        "For non-trivial changes, it is required that another committer review your patches before commit.",
        "Committers should scan the list from top to bottom, looking for patches that they feel qualified to review and possibly commit.",
        "It is expected that committers demonstrate a sustained history of high-quality contributions to the project and community involvement.",
        "If the committer asks where the image should be committed, it should go into the above directory.",
        "Committers are responsible for reviewing and integrating code changes, testing and voting on release candidates, weighing in on design discussions, as well as other types of project contributions.",
        "I'm surprised to see this ticket mentioned here since three committers have already gone on record saying that this is the wrong approach, and one committer even -1'd it."
    ],
    "<iSQL, isql>": [],
    "<key duration>": [],
    "<service entry>": [
        "If the service entry is found, the client should attempt to communicate with the AM on its channel.",
        "If the registered service entry cannot be found, the container MAY do one of: exit."
    ],
    "<cluster Federation>": [],
    "<WARN level, warn level>": [],
    "<directory count>": [],
    "<context name>": [],
    "<Replaces files>": [],
    "<Data Nucleus, data nucleus>": [],
    "<seek key>": [],
    "<Inodes>": [
        "Inodes record the file type (regular file or directory) and the list of blocks.",
        "For a single file /dir1/file1 which takes two blocks of storage, the file structure in S3 would be something like this: / /dir1 /dir1/file1 block-6415776850131549260 block-3026438247347758425 Inodes start with a leading /, while blocks are prefixed with block-."
    ],
    "<maximum resource, maximum resources>": [],
    "<Rowkey, rowkey>": [
        "The rowkey also has to be defined in details as a column (col0), which has a specific cf (rowkey).",
        "The rowkey [hostname][log-event][timestamp] is a candidate if there is a large-ish number of hosts to spread the writes and reads across the keyspace.",
        "In the Hostname In The Rowkey Lead Position example, it might look like this: Composite Rowkey With Hashes: [MD5 hash of hostname] = 16 bytes."
    ],
    "<variable substitution>": [
        "All paths must be absolute and no environment variable substitution will be performed."
    ],
    "<Split size, split size>": [
        "Split size is the number of regions that are on this server that all are of the same table, cubed, times 2x the region flush size OR the maximum region split size, whichever is smaller."
    ],
    "<Thrift server, thrift server>": [],
    "<byte follows>": [],
    "<Jodd>": [],
    "<byte size>": [],
    "<disk size>": [],
    "<Hftp>": [
        "Hftp changed to store the tokens as HFTP and renew them over http.",
        "Note that WebHDFS and HFTP use the HTTP port of the namenode but not the RPC port.",
        "Note that HFTP is read-only so the destination must be an HDFS filesystem.",
        "HFTP is wire-compatible even between different versions of HDFS.",
        "HFTP is a read-only filesystem, and will throw exceptions if you try to use it to write data or modify the filesystem state.",
        "HFTP can now serve a specific byte range from a file.",
        "HFTP is primarily useful if you have multiple HDFS clusters with different versions and you need to move data from one to another."
    ],
    "<Error Response, Error response, error response>": [],
    "<batch mutation>": [
        "This will be called for every batch mutation operation happening at the server."
    ],
    "<secure access>": [
        "This is because secure access control is not possible for the Hive command line using an access control policy in Hive, because users have direct access to HDFS and so they can easily bypass the SQL standards based authorization checks or even disable it altogether."
    ],
    "<CHOWN, chown>": [],
    "<read element>": [],
    "<data upon>": [],
    "<Record reader, record reader>": [
        "The record reader breaks the data into key/value pairs for input to the Mapper."
    ],
    "<long requests>": [],
    "<PTF, PTFs>": [
        "PTFs are black boxes, we assume all columns are needed."
    ],
    "<RS3, rs3>": [],
    "<Block Storage>": [],
    "<equi>": [
        "Rule that merges a join with multijoin/join children if the equi compared the same set of input columns."
    ],
    "<start key, start keys>": [],
    "<cluster free>": [],
    "<elasped time>": [],
    "<ZK, Zk, zk>": [],
    "<IRC, irc>": [],
    "<Column Specification>": [],
    "<Metadata Store, Metadata store, metadata store>": [
        "Metadata store URIs include a scheme that designates the backing store.",
        "The DynamoDB metadata store takes advantage of the fact that the DynamoDB service uses the same authentication mechanisms as S3.",
        "The Metadata Store to use in production is bonded to Amazon's DynamoDB database service.",
        "Object stores with cached metadata databases (for example: AWS S3 with an in-memory or a DynamoDB metadata store) may have timestamps generated from the local system clock, rather than that of the service.",
        "A DynamoDB metadata store can be initialized with additional parameters pertaining to Provisioned Throughput: Example 1."
    ],
    "<extract entries>": [],
    "<serialized size>": [],
    "<accessed files>": [],
    "<Encryption Zone, encryption zone, encryption zones>": [
        "Each encryption zone is associated with a single encryption zone key which is specified when the zone is created.",
        "An encryption zone is a special directory whose contents will be transparently encrypted upon write and transparently decrypted upon read.",
        "Therefore, if the encryption zone key is compromised, it is important to identify all vulnerable files and re-encrypt them.",
        "To comply with the above rule, each encryption zone has its own .Trash directory under the \"zone directory\".",
        "When the entire encryption zone is deleted, the \"zone directory\" will be moved to the .Trash directory under the user's home directory."
    ],
    "<regular number>": [],
    "<load information>": [],
    "<table write>": [
        "Get the table write Ids that are valid for the current transaction."
    ],
    "<POM files, pom files>": [
        "Fixed pom files to refer to the correct MR app-jar needed by the integration tests."
    ],
    "<CTAS>": [],
    "<metric value>": [
        "Metric id must be a string and and metric value must be an integral value."
    ],
    "<success status>": [],
    "<Split directory>": [],
    "<insecure cluster>": [],
    "<JDO, JDOQL, jdo>": [
        "Java Data Objects (JDO) is a standard way to access persistent data in databases, using plain old Java objects (POJO) to represent persistent data."
    ],
    "<column list>": [
        "The PARTITIONED BY clause may be used to specify a subset of the table's partitioning columns (this column list may be empty to indicate that the index spans all partitions of the table)."
    ],
    "<wire format>": [],
    "<MB value>": [
        "NB: the 2MB value is configurable."
    ],
    "<byte exists>": [],
    "<cluster performance>": [],
    "<store directory>": [],
    "<input filter>": [],
    "<given states>": [],
    "<single count>": [],
    "<Znode, znode, znodes>": [
        "[root znode] is the path of the zookeeper znode, under which the edit log information will be stored.",
        "The znodes that were recovered are renamed with the ID of the slave cluster appended with the name of the dead server.",
        "Index at which the RM Delegation Token ids will be split so that the delegation token znodes stored in the zookeeper RM state store will be stored as two different znodes (parent-child).",
        "It contains one znode per peer cluster (if 5 slave clusters, 5 znodes are created), and each of these contain a queue of WALs to process."
    ],
    "<source control>": [],
    "<given procedure>": [],
    "<Replication Factor, Replication factor, replication factor>": [
        "The replication factor specifies the number of block replicas to cache.",
        "-r indicates the desired replication factor; if this optional argument is not specified, a replication factor of 3 will be used.",
        "The block size and replication factor are configurable per file.",
        "The replication factor can be specified at file creation time and can be changed later.",
        "If the replication factor is greater than 3, the placement of the 4th and following replicas are determined randomly while keeping the number of replicas per rack below the upper limit (which is basically (replicas - 1) / racks + 2)."
    ],
    "<Mockito, mockito>": [
        "Mockito is a mocking framework."
    ],
    "<Cluster Web>": [],
    "<write checksum>": [],
    "<KFS>": [
        "KFS has been replaced by QFS (HADOOP-8885).",
        "Kosmos FS (KFS) is no longer maintained and Hadoop support has been removed."
    ],
    "<pipelined>": [],
    "<time length>": [],
    "<empty key>": [
        "If the first character on a line is the separator, empty key is assumed, and the whole line is the value (due to a bug this was not the case)."
    ],
    "<Group entry, group entry>": [],
    "<task number>": [],
    "<Backticked names>": [
        "Backticked names are interpreted as regular expressions."
    ],
    "<quite time>": [],
    "<HWI>": [],
    "<selected directory>": [
        "If the selected directory does not exist, an attempt is made to create it."
    ],
    "<output resides>": [],
    "<Database Service, database service>": [],
    "<import command>": [],
    "<read performance>": [],
    "<Fetch block, fetch block>": [],
    "<level filters>": [],
    "<arena size>": [],
    "<small table>": [
        "No small table values are needed for left semi join since they would be empty.",
        "Outer join uses a hash map since small table columns can be included in the join result.",
        "Small table columns for non-matches will be NULL."
    ],
    "<HDFS Storage, HDFS storage>": [],
    "<response size>": [],
    "<socket level>": [],
    "<earliest time>": [],
    "<changing name>": [],
    "<execution error>": [],
    "<host files>": [],
    "<AVRO, Avro, avro>": [
        "Avro supports nullable types via a union of type T and null."
    ],
    "<Storage Handlers>": [
        "Gives the storage handler a chance to decompose a predicate.",
        "A storage handler consists of a bundle of the following: input format output format serde metadata hooks for keeping an external catalog in sync with Hive's metastore rules for setting up the configuration properties on map/reduce jobs which access tables stored by this handler Storage handler classes are plugged in using the STORED BY 'classname' clause in CREATE TABLE.",
        "Copies the storage handler proeprites configured for a table descriptor to a runtime job configuration.",
        "Storage Handlers like ORC, which can handle predicate push down, could take advantage of this.",
        "This means that Hive gives the storage handler the entire filter, and the storage handler passes back a \"residual\": the portion that needs to be evaluated by Hive.",
        "The breakdown need not be non-overlapping; for example, given the predicate x LIKE 'a%b', the storage handler might be able to evaluate the prefix search x LIKE 'a%', leaving x LIKE '%b' as the residual."
    ],
    "<ASCII>": [
        "ASCII code for 'A'."
    ],
    "<Relative paths, relative paths>": [
        "All relative paths will be resolved relative to it.",
        "Please keep in mind that all paths should be fully specified (no relative paths).",
        "Hence Hadoop path names can be one of: fully qualified URI: scheme://authority/path slash relative names: /path relative to the default file system wd-relative names: path relative to the working dir Relative paths with scheme (scheme:foo/bar) are illegal.",
        "Relative paths can be used.",
        "Relative paths path Resolves to [Y'][path]."
    ],
    "<memory sorting>": [],
    "<splittable>": [],
    "<Cluster Application>": [],
    "<writes block>": [],
    "<Replication source, replication source, replication sources>": [],
    "<Zookeeper, zookeeper>": [
        "Zookeeper has a default limit of 1MB/node.",
        "If using a setup with multiple KMS instances, 'zookeeper' should be used."
    ],
    "<memory cluster>": [],
    "<precommit>": [],
    "<CTR>": [],
    "<owner name>": [
        "If an owner is specified, that owner name overrides that of the caller."
    ],
    "<encoded entries>": [],
    "<ASM>": [],
    "<customer table>": [],
    "<root column>": [],
    "<jsvc>": [],
    "<retry attempt, retry attempts>": [],
    "<actual row>": [],
    "<secret manager>": [
        "The secret manager is responsible for generating and accepting the password for each token.",
        "Setting to true always allows the DT secret manager to be used, even if security is disabled."
    ],
    "<Scala, scala>": [],
    "<LOCAL>": [],
    "<accumulator value>": [],
    "<row schema>": [
        "The new Row Schema can only be a subset of this TS schema."
    ],
    "<artifact directory>": [
        "On certain Jenkins machines, the artifact directory sometimes gets deleted from outside the test-patch script."
    ],
    "<Cluster Node, Cluster Nodes, cluster node, cluster nodes>": [
        "If your cluster nodes use OS X, see the section, SSH: Setting up Remote Desktop and Enabling Self-Login on the Hadoop wiki."
    ],
    "<decent number>": [],
    "<CPU requests>": [],
    "<reduce directory>": [],
    "<composable>": [],
    "<maximum periodicity>": [],
    "<cache blocks, cached block, cached blocks>": [
        "The cached blocks map is a hash map which uses chained hashing."
    ],
    "<key vector>": [],
    "<Cache Key>": [],
    "<Try store>": [],
    "<MB size>": [],
    "<caching blocks>": [],
    "<Time Protocol>": [],
    "<DAO>": [],
    "<replication queues>": [],
    "<skewed table>": [
        "The table will be a skewed table."
    ],
    "<snapshot name>": [],
    "<table specified>": [],
    "<BYTES code>": [],
    "<key definition>": [],
    "<read path>": [],
    "<Cluster topology, cluster topology>": [
        "Cluster topology is used as follows : To reconstruct the splits and make sure that the distances/latencies seen in the actual run are modeled correctly."
    ],
    "<generated format>": [
        "The generated format has been changed from HTML to markdown."
    ],
    "<SFTP, sftp>": [],
    "<given columns>": [
        "This expression selects a row if the given column is null."
    ],
    "<incremental amount>": [],
    "<Github, github>": [],
    "<data unavailability>": [],
    "<Total input, total input>": [],
    "<column paths>": [],
    "<Average Time, average time>": [],
    "<project files>": [],
    "<Scan filter, scan filter>": [
        "This makes this filter unsuitable as a Scan filter."
    ],
    "<metadata Table, metadata table>": [],
    "<Metrics Sources, metrics source, metrics sources>": [],
    "<underlying table>": [],
    "<lock info>": [],
    "<proper name>": [],
    "<Lipcon>": [],
    "<permissible size>": [],
    "<passed name>": [],
    "<checks files>": [],
    "<task names>": [],
    "<database store>": [],
    "<datafiles>": [
        "All files in the filesystem are migrated by re-writing the block metadata - no datafiles are touched."
    ],
    "<lockless>": [],
    "<JNs, Job names>": [],
    "<checkout code>": [],
    "<long amount>": [],
    "<input strategy>": [],
    "<large code>": [],
    "<Eclipse files>": [],
    "<UDF, UDFs, udf>": [
        "This should be done before the UDF is initialized.",
        "Certain optimizations should not be applied if UDF is not deterministic.",
        "Generic UDF params utility class.",
        "String UDFs can be created instead, and the varchar values will be converted to strings and passed to the UDF.",
        "Generic UDF to generate Bloom Filter.",
        "By extending from this class these UDFs will automatically support decimals as well.",
        "Only Hive code and blessed UDFs are accepted in LLAP.",
        "The UDFs are schema agnostic - no XML validation is performed.",
        "If the UDF is not deterministic, or if it is stateful, it is necessary to annotate it as such for correctness.",
        "This UDF has no way of detecting failures or rolling back a transaction.",
        "UDF to determine whether to enforce restriction of information schema.",
        "The example provide was using the RLIKE operator but LIKE or a generic boolean UDF could be used as well.",
        "UDF to determine the current authorizer (class name of the authorizer) This is intended for internal usage only.",
        "This UDF is useful for exporting small to medium summaries that have a unique key.",
        "An exception may be thrown if the UDF doesn't know what to do with this data.",
        "If set to empty, then treated as wildcard character \u2013 all UDFs will be allowed.",
        "This UDF accepts arbitrary number of String arguments, so we use String[]."
    ],
    "<given rows>": [],
    "<input column, input columns>": [
        "If a column is dynamic partition column, its value will be coming from the input column.",
        "In the dynamic partition insert, the input column values are evaluated to determine which partition this row should be inserted into.",
        "This expression evaluates to true if the given input columns is null."
    ],
    "<given Token, given token>": [],
    "<indicated size>": [],
    "<JIRA, JIRAS, JIRAs, Jira, Jiras, jira, jiras>": [
        "This jira introduces backward incompatibility.",
        "JIRA comments now use much more markup to improve readability.",
        "JIRA sorts attached files by the time they were attached, and has no problem with multiple attachments with the same name.",
        "Even if it doesn't get fixed, the JIRA is a public record of it, and will help others out if they run into a similar issue in the future.",
        "The jira made the following changes: 1.",
        "This jira only allows providing paths using back slash as separator on Windows.",
        "This JIRA makes following change: Change Router metrics context from 'router' to 'dfs'.",
        "This jira changes the default block size to 128MB."
    ],
    "<given principal>": [
        "Get the grant information of roles the given principal belongs to."
    ],
    "<error token>": [],
    "<xzvf>": [],
    "<JDA>": [],
    "<side table>": [],
    "<auth method>": [
        "Note: This auth method is suitable for running interactive tools, but will not work for jobs submitted to a cluster."
    ],
    "<access cells>": [],
    "<paramters>": [
        "Two paramters can be specified."
    ],
    "<HAR, hardlink>": [
        "Note that only the file count will be reduced; HAR does not provide any compression."
    ],
    "<source row>": [
        "In this case the source row would never appear in the results."
    ],
    "<Storage Based>": [],
    "<compressed format>": [],
    "<gids>": [],
    "<counter name, counter names>": [],
    "<DROP>": [
        "DROP TABLE removes metadata and data for this table."
    ],
    "<start code>": [],
    "<service invocation>": [],
    "<S3A, S3a, s3a>": [
        "S3A now supports use of AWS Security Token Service temporary credentials for authentication to S3.",
        "S3A now supports configuration of multiple credential provider classes for authenticating to S3.",
        "S3A does not really enforce any authorization checks on these stub permissions.",
        "S3A then performs optimizations tailored to that access pattern.",
        "S3A is now the recommended client for working with S3 objects.",
        "S3A supports configuration via the standard AWS environment variables.",
        "Currently, S3A only supports S3's Server Side Encryption for at rest data encryption.",
        "S3A now supports read access to a public S3 bucket even if the client does not configure any AWS credentials.",
        "S3A has added support for configurable input policies.",
        "S3A can connect to different regions \u2014the tests support this.",
        "S3A now includes the current Hadoop version in the User-Agent string passed through the AWS SDK to the S3 service.",
        "If a region is not configured, S3A will assume that it is in the same region as the S3 bucket.",
        "This is the default: S3A does not need to be configured."
    ],
    "<arbitrary name, arbitrary names>": [
        "There is no notion of a directory; arbitrary names can be assigned to objects \u2014 within the limitations of the naming scheme imposed by the service's provider."
    ],
    "<completed order>": [],
    "<good number>": [],
    "<murmurhash values>": [],
    "<VPN>": [],
    "<SNN>": [],
    "<Retired table>": [],
    "<mytable>": [],
    "<container name>": [],
    "<info key>": [
        "The info key is a string but value can be any object."
    ],
    "<Entry instance>": [],
    "<Process row>": [],
    "<dev>": [],
    "<key element>": [],
    "<TEZ, Tez, tez>": [
        "We let Tez handle the situation.",
        "Tez allows for small datasets to be handled entirely in memory, while no such optimization is available in map-reduce.",
        "Also Tez does not restrict the job to be only Map followed by Reduce; this implies all of the query execution can be done in a single job without having to cross job boundaries."
    ],
    "<enddate>": [],
    "<data world>": [],
    "<basedir>": [],
    "<input signature>": [],
    "<quota usage>": [],
    "<monitor table>": [],
    "<deterministic order>": [],
    "<input topology>": [],
    "<BFS>": [],
    "<given database>": [],
    "<storage unit>": [],
    "<Hadoop Archive>": [],
    "<RPC server>": [
        "An RPC server that hosts protobuf described Services.",
        "Indicate that the rpc server tells client to fallback to simple auth but client is disabled to do so.",
        "When the feature is enabled, RPC server will no longer block on the processing of RPC requests when RPC call queue is full.",
        "Improve how RPC server reads and writes large buffers."
    ],
    "<Stripe Compaction>": [],
    "<array slice>": [],
    "<RCX>": [],
    "<bytes count>": [],
    "<binary prefix>": [],
    "<hex format>": [],
    "<ADL, adl>": [],
    "<repeated key>": [],
    "<text values>": [
        "Text Encoding Each text value begins with a single byte of 0x33 and ends with a single byte of 0x00."
    ],
    "<ctor>": [],
    "<rootdir>": [],
    "<read type>": [],
    "<Data Size, Data size, data size, data sizes>": [
        "In the absence of basic statistics like number of rows and data size, file size is used to estimate the number of rows and data size.",
        "Previously, both the data size and overhead for storage were used to calculate utilization against the flush threashold.",
        "Now, only data size is used to make these per-region decisions."
    ],
    "<perf>": [],
    "<PMC>": [
        "The PMC votes to make a contributor a committer based on an assessment of their contributions to the project.",
        "When the current chair of the PMC resigns, the PMC votes to recommend a new chair using lazy consensus, but the decision must be ratified by the Apache board.",
        "While the initiative is active the PMC may grant commit rights on the branch to its consistent contributors.",
        "When the chair is rotated or if the current chair of the PMC resigns, the PMC votes to recommend a new chair using Single Transferable Vote (STV) voting.",
        "Where necessary, PMC voting may take place on the private Hive PMC mailing list.",
        "PMC members, please read this WIP doc on policy voting for a release candidate, Release Policy.",
        "Contributors often ask Hive PMC members the question, \"What do I need to do in order to become a committer?\" The simple (though frustrating) answer to this question is, \"If you want to become a committer, behave like a committer.\" If you follow this advice, then rest assured that the PMC will notice, and committership will seek you out rather than the other way around."
    ],
    "<given Table, given table>": [
        "Check whether the given table is contained in a sync replication peer which can pass the state checker."
    ],
    "<JAAS>": [
        "JAAS files are not needed by Hadoop itself, but some services (such as Zookeeper) do require them for secure operation."
    ],
    "<metrics registry>": [],
    "<attr2>": [],
    "<RM, RMs, Rm, reserved memory, rm>": [
        "If true, then the RM will not allocate a container for the AM and start it.",
        "When a container finishes its execution at a node, the RM gets notified that there are available resources through the next NM-RM heartbeat, then the RM schedules a new container at that node, the AM gets notified through the next AM-RM heartbeat, and finally the AM launches the new container at the node.",
        "In the meantime, AM needs to re-send the outstanding resource requests to RM because RM may lose the unfulfilled requests when it shuts down.",
        "If the application stops before the flag is true then the RM may retry the application.",
        "RM set next Heartbeat interval for NM.",
        "RM drains the ATS events dispatcher when stopping.",
        "Whether the RM should enable Reservation System.",
        "The RM retains state on all running applications.",
        "While demand persists, the RM will repeat its request; applications should not interpret each message as a request for additional resources on top of previous messages.",
        "Allow the RM to create per-user regions of the registration space 1.",
        "The RM gathers this information from all nodes and determines the least loaded ones.",
        "RM will use these configurations for renewing tokens.",
        "Conversely, an RM may request a different profile of containers in subsequent requests.",
        "Get if the RM should manage the execution of the AM.",
        "If the option is disabled, the RM does not provide any registry support at all.",
        "Note that the RM may have an inconsistent view of the resources owned by the AM.",
        "RM will reload this information from state-store upon restart and re-kick the previously running applications.",
        "In this way, RM doesn't need to kill the AM and re-run the application from scratch as it is done in Phase 1.",
        "Operability Node labels and node labels mapping can be recovered across RM restart Update node labels - admin can update labels on nodes and labels on queues when RM is running.",
        "Which would mean the RM could end up giving it a lot of new allocated containers.",
        "This configuration is added for below scenario: User needs to run distcp jobs across two clusters, but the RM does not have necessary hdfs configurations to connect to the remote hdfs cluster.",
        "To determine the number of least loaded nodes that will be used when scheduling opportunistic containers and how often this list will be refreshed, we use the following parameters: As discussed in the node load rebalancing section above, at regular intervals, the RM gathers all NM queue lengths and computes their mean value (avg) and standard deviation (stdev), as well as the value avg + k*stdev (where k a float).",
        "Besides, RM also saves the credentials like security keys, tokens to work in a secure environment.",
        "The RM shall publish a Release Plan on the dev@hive list stating the branch from which they intend to make a Release Candidate, at least one week before they do so.",
        "The RM runs as a trusted user, and people visiting that web address will treat it, and links it provides to them as trusted, when in reality the AM is running as a non-trusted user, and the links it gives to the RM could point to anything malicious or otherwise."
    ],
    "<LZOP, Lzop, lzop>": [],
    "<pre order>": [],
    "<distributed cluster>": [],
    "<peer cluster>": [
        "Clusters with replication enabled also must replicate the visibility expressions to the peer cluster.",
        "The peer cluster may not have the same labels table with the same ordinal mapping for the visibility labels."
    ],
    "<http location>": [],
    "<shared directory>": [],
    "<aggregate source>": [],
    "<heap Block>": [],
    "<generic filter>": [],
    "<Write number>": [
        "Write number and whether write has completed given out at start of a write transaction."
    ],
    "<output locations>": [],
    "<trash directory>": [
        "Deleted delete/test2 We can see now that the Trash directory contains only file test1."
    ],
    "<larger files>": [],
    "<maximum download>": [
        "Get the current maximum download bandwidth."
    ],
    "<Duplicate Filters>": [],
    "<actual cluster>": [],
    "<complete size>": [],
    "<storage layer>": [],
    "<Webapp, webapp, webapps>": [
        "If only a host is provided as the value, the webapp will be served on a random port."
    ],
    "<replication level>": [],
    "<Decimal type, decimal type>": [],
    "<table exists>": [
        "Thrown when a table exists but should not.",
        "Check if table exists or not.",
        "If target table exists and is not partitioned, it must be empty.",
        "Check whether a table exists in the default catalog.",
        "Throw an exception if the table exists on peer cluster but descriptors are not same.",
        "If the region is not set, verify that the table exists in the same region as the bucket being used."
    ],
    "<length Key, length key>": [],
    "<Timeframes>": [
        "Timeframes can be expressed in the following units: sec, min, hour, day."
    ],
    "<table normalization>": [],
    "<parse code>": [],
    "<HTTP methods, http methods>": [
        "Possible values are: false: All HTTP methods are permitted - GET/PUT/POST/DELETE."
    ],
    "<ORC, Orc, orc>": [
        "Fix array out of bounds when ORC is used with ACID and predicate pushdown.",
        "ORC cache uses this to store compressed length; buffer is cached uncompressed, but the lookup is on compressed ranges, so we need to know this."
    ],
    "<associative array>": [],
    "<HEAD>": [
        "GET,POST,HEAD Comma separated list of headers that are allowed for web services needing cross-origin (CORS) support."
    ],
    "<Control List, control list>": [],
    "<binary service>": [],
    "<quota table>": [],
    "<data discovery>": [
        "The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse."
    ],
    "<client cache>": [],
    "<TABLE creates, table created, table creates>": [
        "If a table created using the PARTITIONED BY clause, a query can do partition pruning and scan only a fraction of the table relevant to the partitions specified by the query.",
        "The table created by CTAS is atomic, meaning that the table is not seen by other users until all the query results are populated."
    ],
    "<Scan requests, scan request>": [],
    "<column date>": [],
    "<local cache>": [],
    "<space usage>": [],
    "<transformed row>": [],
    "<output batch>": [],
    "<client ID>": [],
    "<Authentication filter>": [
        "The authentication filter must be configured with the following init parameters: [PREFIX.]type: The keyword ldap."
    ],
    "<Pradeep>": [
        "Pradeep Kamath gave an update on progress with Howl."
    ],
    "<key part>": [
        "The key part of the cell is taken for comparison which includes row, family, qualifier, timestamp and type."
    ],
    "<Table names, table named, table names>": [
        "The table name of the table we are putting data into .",
        "Notice that table names, rows, columns all must be enclosed in quote characters.",
        "If this option is not provided, the original table name is used.",
        "* * rowID : a string identification the statistic to be gathered, possibly the table name + the partition specs.",
        "No regular-expression or wildcard support is present; all table names must be explicitly listed."
    ],
    "<storage fault>": [],
    "<NULL entries, null entries>": [],
    "<NPE>": [
        "NPE occurred in the following cases - a blacklisted tracker is either decommissioned or expires."
    ],
    "<core request>": [],
    "<Hive directory>": [],
    "<memory dedicated>": [],
    "<secure registries>": [],
    "<PUT Request, PUT request, Put request, Put requests, put request, puts request>": [
        "A PUT request uploads an object/\"Blob\"; a GET request retrieves it; ranged GET operations permit portions of a blob to retrieved.",
        "The put request will be buffered by its corresponding buffer queue.",
        "Put requests are used to modify the scheduler configuration."
    ],
    "<ASCII Table>": [],
    "<bytes token>": [],
    "<Table table, table TABLE>": [
        "Error \"DynamoDB table TABLE does not exist in region REGION; auto-creation is turned off\"."
    ],
    "<given quota>": [],
    "<memory restrictions>": [],
    "<JVM metrics>": [
        "JVM metrics published to Ganglia now include the process name as part of the gmetric name."
    ],
    "<RSC>": [],
    "<system variable>": [],
    "<Aggr>": [
        "Map Aggr & No Skew: Mapper: *Hash-based group by operator to perform partial aggregations *Reduce sink operator, performs some partial aggregations."
    ],
    "<disk table>": [],
    "<disk statistics>": [],
    "<error result>": [],
    "<mmapped>": [],
    "<merge request>": [],
    "<Table read, table read>": [],
    "<bulk directory>": [],
    "<deallocate>": [],
    "<metrics cache>": [],
    "<replication rate>": [],
    "<auths>": [
        "Hadoop Auth is a Java library consisting of a client and a server components to enable Kerberos SPNEGO authentication for HTTP.",
        "here so the connection header is encoded if auth enabled.",
        "Hadoop Auth enforces authentication on protected resources, once authentiation has been established it sets a signed HTTP Cookie that contains an authentication token with the user name, user principal, authentication type and expiration time.",
        "Then the auth will pass for the notification related calls from those hosts.",
        "Hadoop Auth also supports additional authentication mechanisms on the client and the server side via 2 simple interfaces.",
        "If the auths contain secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within."
    ],
    "<time Sample>": [],
    "<Create Table, Create table, create table, creating Table, creating table>": [
        "from transactions The CREATE TABLE creates a dummy table which controls how the output of the sort is written.",
        "Insert, select, create table, drop table, create database, add partition, drop partition have been tested.",
        "hive> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING); creates a table called invites with two columns and a partition column called ds."
    ],
    "<single replication>": [],
    "<Defalt value>": [],
    "<Table Properties, table Properties, table properties, table property>": [
        "All the table properties/parameters will be that of table that was used in EXPORT to generate the archive.",
        "The advantage of this over external tables is that all the table properties (bucketing/sorting/list bucketing) for the underlying tables are used."
    ],
    "<master address>": [],
    "<remote block, remote blocks>": [],
    "<ORM>": [],
    "<Copy bytes>": [],
    "<disk reads>": [],
    "<IPC, ipc>": [
        "Address where the localizer IPC is.",
        "Address where the collector service IPC is."
    ],
    "<long quantity>": [],
    "<Values info>": [],
    "<slow cluster>": [],
    "<reference cell>": [],
    "<JOBID, JobID, jobID, jobid>": [
        "JobID represents the immutable and unique identifier for the job.",
        "First part represents the jobtracker identifier, so that jobID to jobtracker map is defined.",
        "JobID consists of two parts.",
        "JOBID is set by the framework."
    ],
    "<nfsd>": [],
    "<local disk>": [],
    "<group blocks>": [],
    "<memory exception>": [],
    "<time dimension>": [],
    "<active cluster>": [],
    "<Storage Volumes, storage volumes>": [],
    "<input request>": [],
    "<implemenation>": [],
    "<Reporter type, reporter type>": [],
    "<Aliyun>": [
        "Aliyun OSS is an example of \"an object store\".",
        "In order to achieve scalability and especially high availability, Aliyun OSS has relaxed some of the constraints which classic \"POSIX\" filesystems promise."
    ],
    "<home node>": [],
    "<Cluster Replication, Cluster replication, cluster replication>": [
        "Replication to secondary region replicas works over standard inter-cluster replication."
    ],
    "<filter name>": [
        "The filter name must be a single word."
    ],
    "<maximum versions>": [],
    "<Retry policy, retry policy>": [
        "If \"false\", retry policy is turned off."
    ],
    "<Side Encryption, side encryption>": [],
    "<Ulimit, ulimit>": [],
    "<Master Node, master node>": [],
    "<Big Table, big table>": [
        "\u2022 Even if the Big Table columns are not partitioned, the set of values generated from small tables could be pushed down as a predicate on the big table.",
        "For the Map Join, the query processor should know which input table the big table is.",
        "Columns that are identified from small table has following characteristics: \u2022 Column is the other side of predicate in the join condition and Big Table column is identified as a target for partition pruning.",
        "During the probe phase, the big table S is scanned sequentially and for each row of S, the hash table is probed for matching rows."
    ],
    "<Rollup>": [],
    "<logins>": [
        "Did the login happen via keytab.",
        "If login details were provided in the filesystem URI, a warning is printed and then the username and password extracted for the AWS key and secret respectively.",
        "Did the login happen via ticket cache.",
        "This method assumes that login had happened already."
    ],
    "<optimal value>": [
        "The optimal value depends on the jobs characteristics, the cluster configuration and the target utilization."
    ],
    "<compat>": [],
    "<table key>": [],
    "<past time>": [],
    "<cluster directory>": [],
    "<Group Membership, group membership>": [],
    "<read queue, read queues>": [],
    "<Column Level, Column level, column level>": [
        "In order to accurately compute the average row size, column level statistics is required."
    ],
    "<encryption type>": [],
    "<table quota, table quotas>": [
        "When a table with a quota exists in a namespace with a quota, the table quota takes priority over the namespace quota.",
        "Because there is no table quota on 'ns1:t1', this table can grow up to 100TB, but only if 'ns1:t2' and 'ns1:t3' have a usage of zero bytes."
    ],
    "<client token>": [],
    "<nolock>": [],
    "<whole table>": [
        "Each compaction task handles 1 partition (or whole table if the table is unpartitioned).",
        "for a query of type: select expr from T where limit 100; Most probably, the whole table T need not be scanned."
    ],
    "<Entry size, entry size>": [],
    "<single column>": [],
    "<size table>": [],
    "<Retrieve block>": [],
    "<Max number, max number>": [
        "Defines the max number of applications could be fetched using REST API or application history protocol and shown in timeline This property should never be set to false.",
        "Defines the max number of applications could be fetched using REST API or application history protocol and shown in timeline server web ui."
    ],
    "<Partition values, partition value, partition values>": [
        "Builds a key for the partition cache which is concatenation of partition values, each value separated by a delimiter.",
        "Partition values should be quoted only if they are strings."
    ],
    "<SATA disks>": [],
    "<defalt>": [],
    "<live cluster>": [],
    "<row prefix>": [],
    "<vector result>": [],
    "<Structs, structs>": [
        "Structs are not nullable but their component fields may be.",
        "Struct treats the right-most nullable field members as special.",
        "Rather than writing null values to the output buffer, Struct omits those records all together.",
        "A struct is a sequence of <member> elements.",
        "A Struct may be used as a member of another Struct.",
        "The set of fields this struct contains, along with convenience methods for finding and manipulating them."
    ],
    "<input Scan, input scan>": [
        "Configure input scan with proper ranges, iterators, and columns based on serde properties for Hive table."
    ],
    "<Customer number, customer number>": [],
    "<serialized vector>": [],
    "<recovery time>": [],
    "<table types>": [],
    "<millisec>": [],
    "<column descriptor>": [],
    "<Record writer, record writer>": [
        "NOTE: This record writer is NOT thread-safe."
    ],
    "<Table level, table level>": [
        "Since for replication we have the need for hierarchy of tasks we need to make sure that db level are processed first before table, table level are processed first before partitions etc.",
        "Table level co-processors can be also checked by -table option."
    ],
    "<security system>": [],
    "<Data capture>": [
        "Data capture boundaries should coincide with activity boundaries."
    ],
    "<Cache size, cache size>": [
        "This is very useful specially when the Cache size requirement is high."
    ],
    "<table source>": [],
    "<supplied cluster>": [],
    "<key matches>": [],
    "<slave cluster>": [
        "So if the master cluster crashes, the slave cluster may not have the newest data.",
        "When the slave cluster is finally available, the buffer is applied in the same way as during normal processing.",
        "If a slave cluster does run out of room, or is inaccessible for other reasons, it throws an error and the master retains the WAL and retries the replication at intervals.",
        "It is expected that the slave cluster has storage capacity to hold the replicated data, as well as any data it is responsible for ingesting."
    ],
    "<Column Metadata, column metadata>": [],
    "<call queue>": [],
    "<side call>": [],
    "<awesomesauce>": [],
    "<byte range>": [
        "Limits the byte range upto a specified value."
    ],
    "<side error>": [],
    "<local branch>": [],
    "<key request>": [
        "If the key is not specified, the default expiry duration specified in the sas-key request takes effect."
    ],
    "<shortcircuit>": [],
    "<delta files>": [
        "Once a delta file has been closed it cannot be reopened.",
        "If the delta file already exists, delete it.",
        "When the compactor kicks in, these delta files get rewritten into read- and storage-optimized ORC format (enable dictionary encoding, indexes and compression).",
        "An acid delta file is created for each combination partition, and bucket id (a single write id is implied)."
    ],
    "<AVG, avg>": [
        "AVG uses a STRUCT with count and sum for partial aggregation data.",
        "On the other hand avg would have an evaluator only for the double type.",
        "The avg, min, or max can also be used."
    ],
    "<violation policy>": [],
    "<time checks>": [],
    "<network topology>": [
        "The network topology of the cluster would determine the number of components in the network path."
    ],
    "<Metrics2, metrics2>": [],
    "<libs, libtool>": [],
    "<Arpit>": [],
    "<single computation>": [
        "A single computation was divided into 14,000 jobs and totally 7,000,000 tasks."
    ],
    "<memory requested>": [],
    "<snapshotting>": [],
    "<object pool>": [
        "A thread-safe shared object pool in which object creation is expected to be lightweight, and the objects may be excessively created and discarded."
    ],
    "<worker node>": [],
    "<Expert level, expert level>": [],
    "<partition directory>": [],
    "<data backup>": [],
    "<Task Counters, Task counters, task counters>": [
        "Counters holds per job/task counters, defined either by the Map-Reduce framework or applications."
    ],
    "<smaller level>": [],
    "<format table>": [],
    "<row range>": [],
    "<checksum files>": [],
    "<tbl>": [],
    "<disruptor>": [],
    "<key provider>": [],
    "<column mapping>": [],
    "<average cluster>": [],
    "<dependent table>": [
        "The dependent table does not have a location."
    ],
    "<capacity limits>": [],
    "<delegation token secret manager>": [],
    "<Data Encryption, Data encryption, data encryption>": [
        "Get the current working directory of the Trash Policy This API does not work with files deleted from encryption zone when HDFS data encryption at rest feature is enabled as rename file between encryption zones or encryption zone and non-encryption zone is not allowed.",
        "Data encryption is required by a number of different government, financial, and regulatory entities."
    ],
    "<Earlier code>": [],
    "<Input Split, Input splits, input split, input splits>": [
        "Get the input split for this map.",
        "Get the size of the split, so that the input splits can be sorted by size."
    ],
    "<PAM>": [],
    "<explicit files>": [],
    "<desired size>": [],
    "<Reusable code>": [],
    "<Policy Generator>": [
        "Global Policy Generator overlooks the entire federation and ensures that the system is configured and tuned properly all the time."
    ],
    "<local blocks>": [],
    "<dedicated directory>": [],
    "<data replication>": [],
    "<quota rules>": [],
    "<error tracker>": [],
    "<filter list>": [],
    "<configurable number>": [],
    "<database directory>": [],
    "<Prasad>": [
        "Prasad pointed out that in the future, for materialized views, we may need the view definition to be tracked at the partition level as well, so that when we change the view definition, we don't have to discard existing materialized partitions if the new view result can be derived from the old one.",
        "Update 30-Dec-2009: Prasad pointed out that even without supporting materialized views, it may be necessary to provide users with metadata about data dependencies between views and underlying table partitions so that users can avoid seeing inconsistent results during the window when not all partitions have been refreshed with the latest data."
    ],
    "<Replicate Table>": [],
    "<large store>": [],
    "<ACID>": [
        "Utility method for ACID to normalize logging info."
    ],
    "<remote cluster>": [],
    "<SLF4J, slf4j>": [],
    "<memory availability>": [],
    "<mutation request>": [
        "Controls whether a mutation request is allowed."
    ],
    "<commandline>": [],
    "<Skips bytes>": [
        "Skips bytes in a loop."
    ],
    "<YARN code>": [],
    "<CSRF>": [
        "In this case, CSRF is not a potential attack vector, so the prevention is not enforced.",
        "If the incoming User-Agent matches any of these regular expressions, then the request is considered to be sent by a browser, and therefore CSRF prevention is enforced."
    ],
    "<common code>": [],
    "<Acceptance time, acceptance time>": [],
    "<snapshot level>": [],
    "<monitor status>": [],
    "<item number>": [],
    "<AMRM>": [
        "Get the AMRM token of the application."
    ],
    "<source length>": [],
    "<RPC, RPCs, rpcs>": [
        "This is because RPC commands are stateless.",
        "Logs if a RPC is really slow compared to rest of RPCs.",
        "Check if RPC is in asynchronous mode or not.",
        "When the feature is enabled, RPC server will no longer block on the processing of RPC requests when RPC call queue is full.",
        "The master retries sending the close request to the server until the RPC goes through or the master runs out of retries.",
        "RPC can use Avro serialization.",
        "Improve how RPC server reads and writes large buffers.",
        "Does RPC against a cluster.",
        "The time after which a RPC will timeout.",
        "Called after getting if is rpc throttle enabled.",
        "rpc version is introduced which should change when the format of rpc messages is changed.",
        "If the RPC threw an exception, the source will retry 10 times before trying to find a different sink.",
        "If the RPC was successful, the source determines whether the current file has been emptied or it contains more data which needs to be read."
    ],
    "<yz>": [],
    "<daterange>": [],
    "<RR, RRs, Random row, random row, rr>": [
        "When the read pattern is a random row read load and each of the rows are smaller in size compared to this 64 KB, try reducing this."
    ],
    "<LQI>": [
        "This takes the LQI's grouped by likely regions and attempts to bulk load them."
    ],
    "<data lives>": [],
    "<row level>": [],
    "<retry cycle>": [],
    "<checkpoint files>": [],
    "<double constant>": [],
    "<failed directory>": [],
    "<potential cluster>": [],
    "<single patch>": [],
    "<vector length>": [],
    "<time errors>": [
        "If a compile time or run time error occurs that appears related to vectorization, please file a Hive JIRA."
    ]
}