{
    "<inbound layer, inbound layers>": [
        "Indicates whether this layer a valid inbound layer."
    ],
    "<gemm>": [],
    "<KMP>": [],
    "<statistic value>": [],
    "<Required image>": [],
    "<leading dimensions>": [
        "This operation assumes that the leading dimensions of indices are dense, and the gathers on the axis corresponding to the last dimension of indices."
    ],
    "<cpp>": [],
    "<max element>": [],
    "<multiple critical>": [],
    "<HDFS, hdfs>": [],
    "<residual sum>": [
        "If \\(m \\geq n\\), the residual sum of squares for the solution in each column is given by the sum of squares of elements in the remaining \\(m - n\\) rows of that column."
    ],
    "<Convolution2D, convolution2d>": [],
    "<Cross Entropy, cross entropy>": [],
    "<Backend, Backends, backend, backends>": [
        "When multiple backends are present on the classpath, by default the CUDA backend will be tried first.",
        "As I understand the backend of pytorch is written to a large degree in C++ anyway and then Peter built upon that and provided a C++ interface.",
        "By default, both NCCL and Gloo backends will try to find the network interface to use for communication.",
        "A backend also has 2 variables to be aware of.",
        "Applicability: nd4j-cuda-xx, when multiple backends are on classpath Usage: A fallback for determining the local IP the parameter server, if other approaches fail to determine the Description: If set, only a single GPU will be used by ND4J, even if multiple GPUs are available in the system.",
        "The backend can perform further HLO-level optimizations, this time with target specific information and needs in mind.",
        "At this stage, backends may also pattern-match certain operations or combinations thereof to optimized library calls.",
        "nccl backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed training.",
        "Backends are loaded in priority order (highest first)."
    ],
    "<full grid>": [],
    "<Predictive models, prediction model>": [
        "Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model)."
    ],
    "<gram Model, gram model>": [],
    "<fnr>": [],
    "<random perturbations>": [],
    "<extra dimension>": [],
    "<margin constant>": [],
    "<sparse representation>": [],
    "<rotating matrices>": [],
    "<Training iterations, training iterations>": [],
    "<big number>": [],
    "<classification one>": [],
    "<normalization strategies, normalization strategy>": [],
    "<data region>": [],
    "<average rate>": [],
    "<embedding data>": [],
    "<data yielded>": [],
    "<alexnet>": [],
    "<candidate generator>": [],
    "<Full examples, full example>": [],
    "<AIS data>": [
        "AIS data, coordinates can be reported at irregular intervals over time.",
        "Furthermore, AIS data for 1 year is over 100GB compressed."
    ],
    "<Resample, resample, resampler>": [
        "The resampler currently only supports bilinear interpolation of 2D data."
    ],
    "<true positions>": [],
    "<mental model>": [],
    "<axis argument>": [
        "The axis argument specifies the axis of the input tensor along which to find the greatest value."
    ],
    "<vector space, vector spaces>": [
        "A 300-dimensional vector space of words and phrases, for instance, is often called low-dimensional (and dense) when compared to the millions of words and phrases it can contain.",
        "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other')."
    ],
    "<Computes dropout>": [],
    "<Image tensor, image Tensor, image tensor, images tensor>": [],
    "<Constraint function, constraint function, constraint functions>": [],
    "<raw features>": [
        "For Iris, the 4 raw features are numeric values, so we'll build a list of feature columns to tell the Estimator model to represent each of the four features as 32-bit floating-point values."
    ],
    "<one chart>": [],
    "<recommended number>": [
        "Since our vocabulary size in this example is 81, the recommended number of dimensions is 3: Note that this is just a general guideline; you can set the number of embedding dimensions as you please."
    ],
    "<type dimension>": [],
    "<float beta>": [],
    "<matrix inverse>": [],
    "<standard random>": [],
    "<Oord>": [],
    "<regression metric>": [
        "A common regression metric is Mean Absolute Error (MAE)."
    ],
    "<output result>": [],
    "<numerical precision>": [],
    "<Real data, real data>": [
        "# 100 is much bigger than on a real data set, but real datasets have more than # two instances.",
        "This style of comparison is useful for image translation problems, where the generator input is a corrupted image, the generator output is the reconstruction, and the real data is the target."
    ],
    "<scatter phase>": [],
    "<error trying>": [],
    "<Perceptron, Perceptrons, perceptron, perceptrons>": [],
    "<original parameters>": [
        "Note that the weights are copied before being modified - the original parameters are not changed."
    ],
    "<auxiliary nodes>": [
        "Separating out the auxiliary nodes typically doesn't remove critical information since these nodes are usually related to bookkeeping functions."
    ],
    "<fixed window>": [],
    "<complex relationships>": [],
    "<CPU device, CPU devices>": [],
    "<large text>": [],
    "<Model Trained, Model Training, Model training, model trained, model training, model trains, models trained, models training>": [
        "If True, performs intermediate model checkpoints and exports during the training process, rather than only once model training is complete.",
        "A model trained on more data will naturally generalize better.",
        "With the model trained, we can use it to make predictions about some images.",
        "Machine learning models, however, need to have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!)."
    ],
    "<dense results>": [],
    "<GPU activity>": [],
    "<tracking list>": [],
    "<larger array>": [],
    "<pybind11>": [],
    "<depth values>": [],
    "<weighted correlation>": [],
    "<MSE, Mean Squared Error, Mean squared error, mean square error, mean squared error>": [
        "Mean Squared Error loss function where each the output is (optionally) weighted/scaled by a flags scalar value.",
        "Mean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems)."
    ],
    "<Visualizing Learning>": [],
    "<second distribution>": [],
    "<Input arrays, input arrays, inputs array>": [
        "The array is of the same rank as each of the input array operands (which must be of the same rank as each other) and contains the arguments in the order that they were specified.",
        "* All input arrays must have the same dimensions.",
        "The input arrays must have the same number of columns.",
        "In many cases, this is totally safe - in others, the input array will be shared by multiple layers, and hence it's not safe to modify the input array.",
        "Note that the input array is modified in-place.",
        "Not performed in-place: the input arrays are not modified."
    ],
    "<broadcasting matrix>": [],
    "<Small model, small model>": [
        "The small model should be able to reach perplexity below 120 on the test set and the large one below 80, though it might take several hours to train."
    ],
    "<partitioning information>": [],
    "<elastic difference>": [
        "The elastic difference then be used to update both local variables and global variables."
    ],
    "<passing dimensions>": [],
    "<Gaussian distribution, gaussian distribution>": [],
    "<regression labels>": [],
    "<inference operation>": [],
    "<AOT>": [],
    "<vector element, vector elements>": [
        "Each vector element becomes a row and the vector is duplicated for each column in the matrix."
    ],
    "<Parallel Training, parallel training>": [],
    "<Output Layer, Output Layers, Output layer, Output layers, output layer, output layers>": [
        "The layers in between our input and output layers are called hidden layers.",
        "Network outputs are for output layers only.",
        "Hidden layers can learn features from the input layer and it can send those features to be analyzed by our output layer to get the corresponding outputs.",
        "These layers are not scoring/output layers: that is, they should be used as the intermediate layer in a network only.",
        "I thought my output layer is already float type.",
        "And the output layer will simply be the number of labels.",
        "Output layer used for training via backpropagation based on labels and a specified loss function."
    ],
    "<Model output, model output, model outputs>": [
        "When we print the model architecture, we see the model output comes from the 6th layer of the classifier.",
        "The model outputs an array of numbers, one for each label, and each number is the predicted likelihood of the input being that class."
    ],
    "<parameter tensor>": [],
    "<OpenNLP>": [],
    "<sequence time>": [],
    "<monotonic function>": [],
    "<Hasim>": [],
    "<individual GPU>": [],
    "<output stride>": [],
    "<Euclidean distance, euclidean distance>": [],
    "<predicted classes>": [
        "The probabilities for each possible target class for each example: the For a given example, our predicted class is the element in the corresponding row of the logits tensor with the highest raw value."
    ],
    "<deep NLP>": [],
    "<Feature weights>": [],
    "<received tensor, receiver tensor>": [],
    "<graph mode, graph model>": [],
    "<column2>": [],
    "<renorm>": [],
    "<spectral norm, spectral normalization>": [],
    "<Independent distribution, independent distributions>": [],
    "<HLO>": [],
    "<graph data>": [],
    "<product operation>": [],
    "<Gradient Aggregation>": [],
    "<standard parameter>": [],
    "<regression score>": [],
    "<pixel images>": [],
    "<Signal guide>": [],
    "<report device>": [],
    "<logdet>": [],
    "<model created>": [],
    "<model development>": [],
    "<evaluation classes>": [
        "The Evaluation class is used for evaluation.",
        "In DL4J the Evaluation Class and variants of the Evaluation Class are available to evaluate your model's performance.",
        "An Evaluation class has more built-in methods if you need to extract a Under Curve (AUC)."
    ],
    "<closest integer>": [],
    "<batch dimension, batch dimensions>": [
        "The channels and batch dimensions are the same as that of the input tensor.",
        "If the batch dimension is omitted, it is assumed to be 1.",
        "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.",
        "Example with contracting dimension numbers: Associated batch dimension numbers from the 'lhs' and 'rhs' must have the same dimension number, must be listed in the same order in both arrays, must have the same dimension sizes, and must be ordered before contracting and non-contracting/non-batch dimension numbers."
    ],
    "<dense embedding>": [],
    "<dimensional mismatch>": [],
    "<embedding values>": [],
    "<statistical distributions>": [],
    "<Reverse subtraction, reverse subtraction>": [],
    "<counter parallel>": [],
    "<map structure>": [],
    "<left node>": [],
    "<logits Tensor, logits tensor>": [],
    "<Matrix Factorization, matrix factorization>": [],
    "<vocabulary list>": [
        "Namely, there's way too much typing when the vocabulary list is long."
    ],
    "<weight initialisations>": [],
    "<predicted outputs>": [],
    "<half type>": [],
    "<True labels, true label, true labels>": [],
    "<Master node, master node>": [
        "Leave it null, and Spark Master node will be used as Master for parameter server as well.",
        "This method is viable only at Spark Workers, Master node will always have empty buffer here by design."
    ],
    "<loss element>": [
        "By default, the losses are averaged over each loss element in the batch."
    ],
    "<Training Training>": [],
    "<node goes>": [],
    "<freq>": [],
    "<forward operation>": [],
    "<subnode>": [],
    "<layer weights>": [],
    "<True positives, true positives>": [
        "True positives: correctly rejected.",
        "Get the true positives count for the specified output."
    ],
    "<double matrices>": [],
    "<moving window>": [
        "Moving window, capture a row x column moving window of a given matrix."
    ],
    "<color saturation>": [],
    "<slave status>": [],
    "<one epoch>": [],
    "<temporal data>": [],
    "<general operation>": [],
    "<Cauchy, cauchy>": [],
    "<single cycle>": [],
    "<error propagation>": [],
    "<Advanced Activations>": [],
    "<device side>": [],
    "<building input>": [],
    "<iris directory>": [],
    "<squares solution>": [],
    "<rejection probabilities>": [],
    "<Compute gradients, compute gradient, compute gradients, computed gradient, computed gradients, computes gradients, computing gradients>": [
        "That is, as long as the parameters are used after the underlying source of stochasticity, the computed gradient is accurate.",
        "A minibatch refers to the number of examples used at a time, when computing gradients and parameter updates."
    ],
    "<input Variables, input variable, input variables>": [],
    "<input embedding>": [
        "Operator adding input embedding to the given cell."
    ],
    "<final loss>": [],
    "<CoreML models>": [],
    "<Spark function>": [],
    "<abs value>": [
        "Printing will switch to scientific notation on a per element basis - when abs value is greater than or equal to 10000 If the number of elements in the array is greater than 1000 (by default) only the first and last three elements in a dimension are included."
    ],
    "<sampling function>": [],
    "<success probability>": [],
    "<multiplicative noise>": [
        "The multiplicative noise will have Input shape: Arbitrary."
    ],
    "<desired seed>": [],
    "<linear transformation, linear transformations>": [],
    "<tfrecord format>": [
        "I think tfrecord format is good way to save data chunks, and avoids reading lots of small files, which is especially slow on hdfs."
    ],
    "<label problems>": [],
    "<named function, named functions>": [],
    "<Coordinate function>": [],
    "<stable gradient>": [],
    "<one gradient>": [],
    "<outermost axis>": [],
    "<five layers>": [],
    "<TPU training>": [],
    "<correct dropout>": [],
    "<particular layer>": [],
    "<per device>": [],
    "<btriunpack>": [],
    "<SimpleCNN, SimpleRNN>": [],
    "<Model Score, model score, model scores>": [],
    "<triangular factorization>": [],
    "<Shard function>": [],
    "<dimensional matrices>": [],
    "<Application Step>": [],
    "<depth operation>": [],
    "<batch entries, batch entry>": [],
    "<discrete numerical>": [],
    "<partial derivative, partial derivatives>": [
        "Computing partial derivatives can require aggregating gradient contributions."
    ],
    "<sufficient statistics>": [
        "These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted."
    ],
    "<time reversal>": [],
    "<Input function, input function, input functions>": [
        "Namely: This input function builds an input pipeline that yields batches of (features, labels) pairs, where features is a dictionary features.",
        "The input function must contain values for all state components or none of them.",
        "Just as input functions can leverage the Dataset API, model functions can leverage the Layers API and the Metrics API."
    ],
    "<tag sequence, tag sequences>": [],
    "<regression performance>": [],
    "<trailing dimension, trailing dimensions>": [
        "The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened."
    ],
    "<edge number>": [],
    "<CuDNNLSTM>": [],
    "<SRU>": [],
    "<GPU nodes, gpu nodes>": [],
    "<Constant Tensor, constant Tensor, constant tensor>": [],
    "<Jcuda>": [],
    "<Nested structure, nested structure, nested structures>": [
        "The nested structure of these properties map to the structure of an element, which may be a single tensor, a tuple of tensors, or a nested tuple of tensors."
    ],
    "<correct shape>": [],
    "<next calculation>": [],
    "<Adam optimizer>": [],
    "<convert Tensor, convert tensor>": [],
    "<input flag>": [],
    "<column reduction>": [
        "A column reduction defines how a single column should be reduced."
    ],
    "<arbitrary matrix>": [],
    "<Processing gradients>": [],
    "<normalized inputs>": [],
    "<kernel tensor>": [],
    "<described tensor>": [],
    "<feeding inputs>": [],
    "<associated name>": [],
    "<variable shape>": [],
    "<gaussian functions>": [],
    "<customizable number>": [],
    "<minimum bounds>": [],
    "<weighted graph>": [],
    "<libsvm>": [],
    "<distributed storage>": [],
    "<placeholder tensor, placeholder tensors>": [],
    "<Time Series, Time series, time series>": [
        "Time series passed to this model have a batch dimension, and each series in a batch can be operated on in parallel.",
        "Consider, however, that instead the input time series was 10,000 or more time steps.",
        "Also note: that variable length time series always start at time zero in the data arrays: padding, if required, will be added after the time series has ended.",
        "Time series evaluation is very similar to the above evaluation approaches.",
        "Not all time series for a single ship have an equal length - there's high dimensionality in the data.",
        "Evaluation in DL4J is performed on all (non-masked) time steps separately - for example, a time series of length 10 will contribute 10 predictions/labels to an Evaluation object.",
        "However, each time series is still in a separate file."
    ],
    "<single number>": [],
    "<complex input>": [],
    "<tower function>": [],
    "<device tensor>": [],
    "<final nodes>": [
        "Nodes may depend on final nodes."
    ],
    "<Matrix property, matrix property>": [],
    "<label numbers>": [
        "The label numbers are mapped to a named representation, such as: 0: Iris setosa."
    ],
    "<spadd>": [],
    "<Linear view, Linear views, linear view>": [
        "Linear views are not always possible.",
        "Linear views are only good for elementwise operations (rather than matrix operations), since the views do not preserve the order of the buffer.",
        "Linear view allows you to do nondestructive operations (reshape and other operations can be destructive because elements are changed within the nd-array)."
    ],
    "<Tensor Processing, tensor processing>": [],
    "<pixel positions>": [],
    "<split input>": [],
    "<shape components>": [
        "The first two dimensions must match across all the inputs, but otherwise the ranks and other shape components may differ."
    ],
    "<Density Modeling>": [],
    "<loss fields>": [],
    "<single allocation>": [],
    "<shape Shape, shape shape>": [],
    "<dimth dimension>": [
        "The dimth dimension has the same size as the length Note."
    ],
    "<triple elements>": [],
    "<multiple Examples, multiple examples>": [
        "This works in the same way as training and the forward pass / output methods: multiple rows (dimension 0 in the input data) are used for multiple examples."
    ],
    "<capacity elements>": [],
    "<learning level>": [],
    "<digamma>": [],
    "<custom op>": [],
    "<addcdiv>": [],
    "<ptrblck>": [],
    "<Seq2seq, seq2seq>": [
        "A Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder.",
        "This is because our seq2seq autoencoder uses multiple inputs/outputs."
    ],
    "<Participation Counts>": [],
    "<Plotting results, plotting results>": [],
    "<decoder name>": [],
    "<load parameters>": [],
    "<Continuous Features, Continuous features, continuous feature, continuous features>": [
        "Continuous features can be directly fed into deep neural network models.",
        "You can specify a continuous feature like so: Although, as a single real number, a continuous feature can often be input directly into the model, Tensorflow offers useful transformations for this sort of column as well."
    ],
    "<minimum threshold>": [],
    "<node labels>": [],
    "<Dropout layer, Dropout layers, dropout layer, dropout layers>": [],
    "<sequence tasks>": [],
    "<Model Performance, model performance, model performed, model performs>": [
        "Overfitting happens when a model performs well on the data it is trained on, but worse on test data that the model has not seen before.",
        "This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before.",
        "We can see that the model performs pretty well using only 5 epochs!"
    ],
    "<output rate>": [],
    "<forward model>": [],
    "<Binary tensor>": [],
    "<Dropout2d, dropout2d>": [],
    "<expanded distribution>": [],
    "<TFDBG, tfdbg>": [
        "tfdbg will show you the history of commands that started with those characters."
    ],
    "<INT32, int32>": [
        "int32 version = 3 [deprecated = true];."
    ],
    "<FP16, fp16>": [],
    "<affine parameters>": [],
    "<initial encoding>": [],
    "<Variable node, Variable nodes>": [],
    "<loss data>": [],
    "<persistent state>": [],
    "<data module>": [],
    "<classification report>": [],
    "<alias input>": [
        "Note that an alias input may itself be an alias."
    ],
    "<additive factor>": [],
    "<visual example>": [],
    "<normalize data>": [],
    "<Mask shape, mask shape>": [],
    "<values left>": [],
    "<sparse multiplication>": [],
    "<loss Methods>": [],
    "<Spatial Transformer>": [
        "Spatial transformer networks are a generalization of differentiable attention to any spatial transformation.",
        "Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model."
    ],
    "<variance Tensor>": [],
    "<Highest memory, higher memory>": [],
    "<training code>": [
        "The training workflow usually proceeds as follows: Prepare training code with a few components: a."
    ],
    "<BTW, Btw>": [],
    "<transposed Tensor>": [],
    "<initial alignment>": [],
    "<dummy data>": [],
    "<orgqr, ormqr>": [],
    "<Carlo approximation>": [],
    "<multiple jobs>": [],
    "<Matlab, matlab>": [],
    "<optional cell>": [],
    "<non edge>": [],
    "<AUC>": [
        "By comparing these the AUC is generated, with some discretization error."
    ],
    "<argument diagonal>": [],
    "<one feature>": [
        "If only one feature array is present, this should always be 0."
    ],
    "<threshold step, threshold steps>": [
        "Target sparsity/dense level, when threshold step will happen."
    ],
    "<df2>": [],
    "<training stops>": [],
    "<Double column, Double columns, double column>": [],
    "<dot product>": [],
    "<Ubuntu, ubuntu>": [],
    "<network traffic>": [],
    "<mixture probabilities>": [
        "A Mixture is defined by a Categorical (cat, representing the mixture probabilities) and a list of Distribution objects all having matching dtype, batch shape, event shape, and continuity properties (the components)."
    ],
    "<parameter update, parameter updates>": [
        "Called once, after each parameter update has ocurred while training the network.",
        "A minibatch refers to the number of examples used at a time, when computing gradients and parameter updates."
    ],
    "<input desired>": [
        "This class defines input desired for any given node/operation within graph."
    ],
    "<sensitivity value>": [],
    "<Training workflows, training works>": [
        "The training workflow usually proceeds as follows: Prepare training code with a few components: a."
    ],
    "<distributed confidence>": [],
    "<output value>": [
        "if a decimal output value is required."
    ],
    "<Bazel>": [],
    "<ragged matrix>": [],
    "<unit cube>": [],
    "<coarse parameter>": [],
    "<one subtle>": [],
    "<unit simplex>": [],
    "<mean value, mean values>": [],
    "<Composable, composable>": [],
    "<bias array>": [
        "Note that bias array is optional."
    ],
    "<automatic join>": [],
    "<weight tensor, weight tensors, weights Tensors, weights tensor>": [],
    "<central goal>": [],
    "<L2 Loss, L2 loss>": [
        "L2 loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
    ],
    "<padding number>": [],
    "<projected value>": [],
    "<effecive>": [],
    "<calculated shape>": [],
    "<dask>": [],
    "<statistical strength>": [],
    "<loss comparisons>": [],
    "<MNIST, mnist>": [
        "MNIST dataset, please resize the images from the dataset to 32x32.",
        "The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here: Fashion MNIST is intended as a drop-in replacement for the classic MNIST dataset\u2014often used as the \"Hello, World\" of machine learning programs for computer vision.",
        "MNIST is the \"Hello World\" of deep learning.",
        "The MNIST digits are transformed into a flat 1D array of length 784 (MNIST images are 28x28 pixels, which equals 784 when you lay them end to end)."
    ],
    "<distributed devices>": [],
    "<input signal>": [],
    "<float model>": [],
    "<STL>": [],
    "<big feature>": [],
    "<random vector>": [],
    "<tensor left>": [],
    "<optimizable>": [],
    "<original element>": [],
    "<best model, best models, good models>": [
        "Terminate training if best model score does not improve for N epochs."
    ],
    "<Root folder, root folder>": [],
    "<edge list>": [],
    "<Partitions data>": [],
    "<Triangular matrix, triangular matrices, triangular matrix>": [
        "Triangular matrix elements are filled in a clockwise spiral.",
        "The documentation says that if a covariance matrix is passed to create the distribution, the corresponding triangular matrices are computed with a Cholesky decomposition."
    ],
    "<iterative training>": [],
    "<execution guide>": [],
    "<training Step, training step, training steps>": [
        "The training step time is thus the sum of both CPU pre-processing time and the accelerator training time.",
        "Each training step runs a single iteration of K-Means and must process the full input at once.",
        "Each training step accumulates the contribution from one mini-batch into temporary storage."
    ],
    "<label regression>": [],
    "<summary sequence>": [],
    "<task index, task indices>": [],
    "<negative words>": [],
    "<maximum index, maximum indices>": [],
    "<inconsistent shapes>": [],
    "<Hurwitz>": [],
    "<running standard>": [],
    "<circular mode>": [],
    "<dropout value>": [],
    "<Linear module>": [
        "Each Linear Module computes output from input using a # linear function, and holds internal Tensors for its weight and bias."
    ],
    "<ideal amount>": [],
    "<data manageable>": [],
    "<Penrose inverse>": [],
    "<output batch>": [],
    "<Supervised Learning, supervised learning>": [],
    "<huge network>": [],
    "<tensor constructor>": [],
    "<Gumbel>": [],
    "<protobuf>": [],
    "<saving models>": [],
    "<distributed group, distributed groups>": [
        "This is where distributed groups come into play."
    ],
    "<Random sequence, random sequence>": [],
    "<image bounds>": [],
    "<accuracy function>": [],
    "<older checkpoint>": [],
    "<dense Matrix, dense matrix>": [],
    "<sliding window>": [],
    "<ICML>": [],
    "<Learning phase, learning phase>": [
        "The learning phase flag is a bool tensor (0 = test, 1 = train) to be passed as input to any Keras function that uses a different behavior at train time and test time."
    ],
    "<distributed parameters>": [],
    "<lucene>": [],
    "<quantized type>": [],
    "<Keras session>": [
        "If no global Keras session exists at this point: we will create a new global session."
    ],
    "<Gaussian patterns>": [],
    "<pre process, pre processor>": [],
    "<Create gradient>": [],
    "<truth data>": [],
    "<Annotation Step>": [],
    "<temporal dependency>": [],
    "<resnet18>": [],
    "<third distribution>": [],
    "<float scalar>": [],
    "<QR decomposition>": [],
    "<estimated variance>": [],
    "<general function>": [],
    "<teacher ratio>": [],
    "<unitary matrix>": [],
    "<forward network, forward networks>": [
        "Feed-forward networks are those in which there is not cyclic connection between the network layers."
    ],
    "<matrix unchanged>": [],
    "<model construction>": [],
    "<input rank, input ranks, inputs rank>": [],
    "<distributed sessions>": [],
    "<gamma weight>": [],
    "<shallow structure>": [],
    "<L1 Loss, L1 loss>": [
        "L1 loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
    ],
    "<None result>": [],
    "<rejection sampling>": [],
    "<naming inputs>": [],
    "<flattened sequence>": [],
    "<model True>": [],
    "<sessionID>": [],
    "<prior probability>": [],
    "<previous image>": [],
    "<incorrect number>": [],
    "<double vector>": [],
    "<exact parameter>": [],
    "<affine layers>": [],
    "<Data Training, data Training>": [
        "Loading the data Training the model Visualizing the STN results."
    ],
    "<Graph level>": [],
    "<white noise>": [],
    "<CPU kernel, CPU kernels>": [],
    "<move result>": [],
    "<incoming weights>": [],
    "<noise distribution>": [],
    "<Logistic Regression, Logistic regression, logistic regression>": [
        "Logistic regression is one in which the dependant variable is categorical rather than continuous - meaning that it can predict only a limited number of classes or categories, like a switch you flip on or off.",
        "We will train a logistic regression model that, given an individual's information, outputs a number between 0 and 1\u2014this can be interpreted as the probability that the individual has an annual income of over 50,000 dollars."
    ],
    "<Tensor reference, tensor reference>": [],
    "<Entropy loss, entropy loss>": [
        "There are other loss functions, but cross-entropy loss is arguably the best one for probability distributions."
    ],
    "<single feature, single features>": [],
    "<multiple numerical>": [],
    "<facial recognition>": [],
    "<predictions tensors>": [],
    "<Gradient Computation, gradient computation>": [
        "The gradient computation of this operation will only take advantage of sparsity in the input gradient when that gradient comes from a Relu.",
        "If gradients are computed in that context, then the gradient computation is recorded as well.",
        "Note: The gradient computation on GPU is faster for large matrices but not for large batch dimensions when the submatrices are small."
    ],
    "<average error>": [
        "The graph shows the average error is about \\$2,500 dollars."
    ],
    "<random crop>": [],
    "<Gradient optimization>": [],
    "<Backward computation, backward computation>": [
        "Backward computation is never performed in the subgraphs, where all Tensors didn't require gradients.",
        "- Any backward computation that generate \"nan\" value will raise an error."
    ],
    "<partitioning strategy>": [
        "You can also specify a partitioner object to partition the primal weights during training (div partitioning strategy will be used)."
    ],
    "<FP8>": [],
    "<point error>": [],
    "<unit length>": [],
    "<NVP, nvprof>": [
        "In contrast, Real NVP can compute both forward and inverse computations in parallel."
    ],
    "<data axbc>": [],
    "<w2v>": [],
    "<model let>": [],
    "<Tensor division>": [],
    "<Sinh, sinh>": [],
    "<mass function>": [],
    "<fake label>": [],
    "<basedir>": [],
    "<merged value>": [
        "If the second component is different (a, b, ...) from each tower, then the merged value will have a wrapped map from tower device to the different values."
    ],
    "<cropped tensor>": [],
    "<one detail>": [],
    "<matmul>": [],
    "<drop probability>": [],
    "<Update layer>": [],
    "<monitor data, monitoring data>": [],
    "<D1, d1>": [],
    "<labeled point>": [],
    "<backward process>": [],
    "<double comparisons>": [],
    "<RBMs>": [],
    "<sequential features>": [],
    "<small layer>": [
        "Similarly, small layer sizes may not be able to adequately utilize the power of a GPU."
    ],
    "<Maximum layer>": [],
    "<desired normalization>": [],
    "<Data Transformation, data transformations>": [],
    "<w1>": [],
    "<assigned cluster>": [],
    "<network graph>": [],
    "<unprojected>": [],
    "<pretrained network>": [],
    "<Node node, node node>": [],
    "<forward layer>": [],
    "<Controls model>": [],
    "<critical problem>": [],
    "<corresponding bound>": [],
    "<MAE, Mean Absolute Error, Mean absolute error, mean absolute error>": [
        "Mean Absolute Error loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
    ],
    "<Outputs true>": [
        "Here is a table of the (roughly) expected first order behavior: Classical CTC behavior: Outputs true repeated classes with blanks in between, and can also output repeated classes with no blanks in between that need to be collapsed by the decoder."
    ],
    "<real interval>": [],
    "<calling list>": [],
    "<divison>": [],
    "<cost function>": [
        "The essence of the idea is that the cost function attempts to model the output as if it were a mixture of gaussian probability densities."
    ],
    "<retain probability>": [],
    "<Dilated Convolutions, dilated convolution, dilated convolutions>": [],
    "<Adam algorithm>": [
        "The original Adam algorithm maintains two moving-average accumulators for each trainable variable; the accumulators are updated at every step."
    ],
    "<forward activations>": [],
    "<full features>": [],
    "<asinh>": [],
    "<SGDR>": [],
    "<step number>": [],
    "<ndv>": [],
    "<lerp>": [],
    "<one caveat>": [
        "The one caveat with using scripting is that it only supports a restricted subset of Python."
    ],
    "<Binomial distribution, Binomial distributions, binomial distribution>": [],
    "<Density Estimation>": [],
    "<Roadmap, roadmap>": [],
    "<started process>": [],
    "<mobile device, mobile devices>": [
        "TF Lite allows mobile developers to do inference efficiently on mobile devices."
    ],
    "<Minimum resolution>": [],
    "<CIFAR>": [],
    "<convolution implementation>": [],
    "<second loop>": [],
    "<Recurrent computations>": [],
    "<SSD>": [],
    "<form matrices>": [],
    "<induced vector>": [],
    "<Optional Tensor, Optional tensor, optional Tensor, optional tensor>": [
        "weights: Optional Tensor indicating the frequency with which an example is sampled."
    ],
    "<initial step>": [],
    "<minimization problem>": [],
    "<one title>": [
        "Titles strings in array create one title column."
    ],
    "<bound constraints>": [],
    "<Group tensors>": [],
    "<Frontend, frontend>": [],
    "<Optional count>": [],
    "<generator input, generator inputs>": [],
    "<metric function, metric functions>": [
        "If None, the default metric functions are used; if {}, no metrics are used."
    ],
    "<optimization routines>": [],
    "<clamping range>": [],
    "<Kaggle, kaggle>": [],
    "<covariance matrices, covariance matrix>": [],
    "<checkpoint directory>": [],
    "<sampled classes>": [
        "Determines whether all sampled classes in a batch are unique."
    ],
    "<single bin, single binary>": [],
    "<nrow>": [],
    "<unique step>": [],
    "<Model Inputs, Model inputs, model input, model inputs>": [],
    "<true rank>": [
        "The true rank of an array is the number of dimensions which have a size greater than 1."
    ],
    "<weighted random>": [],
    "<random gradients>": [],
    "<bias centering>": [],
    "<load function, load functions>": [
        "There are no units for load, and the load function is responsible for providing an internally consistent measure.",
        "The load is calculated by a user-specified load function passed in at construction.",
        "More advanced load functions could consider the difference in access patterns across ops, or trade off CPU-intensive ops with RAM-intensive ops with network bandwidth."
    ],
    "<zeta function>": [],
    "<sampling randomly>": [],
    "<timeseries>": [],
    "<scatter plot>": [],
    "<hierarchical order>": [],
    "<beta weight>": [],
    "<Feature Columns, Feature columns, feature column, feature columns, features columns>": [
        "A feature column can be either one of the raw inputs in the original features dict (a base feature column), or any new columns created using transformations defined over one or multiple base columns (a derived feature columns).",
        "Therefore, the code to create the feature column is: Feature columns can be far more sophisticated than those we're showing here.",
        "The third feature column also specifies a lambda the program will invoke to scale the raw data: # Define three numeric feature columns.",
        "As long as all feature columns are unweighted sparse columns this computes the prediction of a linear model which stores all weights in a single variable.",
        "An Estimator expects a vector of numeric inputs, and feature columns describe how the model should convert each feature.",
        "Each feature column needs a different kind of operation during this conversion.",
        "A feature column is an object describing how the model should use raw input data from the features dictionary.",
        "A feature column is an abstract concept of any raw or derived variable that can be used to predict the target label.",
        "Feature columns are very rich, Estimators can use, allowing easy experimentation.",
        "Feature Columns, handle a variety of input data types without changes to the model.",
        "Feature columns can have internal state, like layers, so they often need to be initialized."
    ],
    "<inferred vector>": [],
    "<prediction array>": [],
    "<vector regardless>": [],
    "<input specs>": [],
    "<pprof>": [],
    "<dimensional input>": [],
    "<cluster center, cluster centers>": [
        "However, the cluster centers may be retrieved by the latest checkpoint saved during training."
    ],
    "<Manual device>": [],
    "<English word>": [
        "When training started, the model did not know how to spell an English word, or that words were even a unit of text."
    ],
    "<normalized weights>": [],
    "<layer1>": [],
    "<columns tensor>": [],
    "<gradient calculation, gradient calculations>": [
        "The gradient calculation and application are delegated to an underlying optimizer."
    ],
    "<Scalar SUM, Scalar sum, scalar sum, scalar summary>": [],
    "<structure prediction>": [],
    "<Noise Layers>": [],
    "<absolute scope>": [],
    "<forward variable, forward variables>": [],
    "<count sequence>": [],
    "<restore values>": [],
    "<one mapped>": [],
    "<Conduct learning>": [],
    "<Array format, array format>": [],
    "<glu>": [],
    "<logit>": [],
    "<loaded model>": [],
    "<optimization step>": [],
    "<connected layer, connected layers>": [],
    "<generating data>": [],
    "<weighting moving>": [],
    "<preconditioner, preconditioners>": [],
    "<Nvidia, nvidia>": [
        "Check the NVIDIA guides for instructions on setting up CUDA on the NVIDIA website."
    ],
    "<Nyquist frequency>": [],
    "<model fails>": [
        "When the episode ends (our model fails), we restart the loop.",
        "Increasing the learning rate of generator does prevent the loss of discriminator decaying to 0 and the loss of generator increasing, but the model fails to learn anything useful."
    ],
    "<rank array, rank arrays>": [],
    "<second tuple>": [],
    "<assuming matrix>": [],
    "<TOCO>": [],
    "<minimize loss>": [],
    "<example sequence>": [
        "An example sequence of events, leading to an outdated workspace pointer: Workspace W is opened (iteration 1)."
    ],
    "<model parallel>": [],
    "<Perform evaluation, perform evaluation, performing evaluation>": [
        "Singleton evaluation hrunner class for performing evaluation on Spark."
    ],
    "<window length>": [
        "If only one integer is specified, the same window length will be used for both dimensions."
    ],
    "<categorical terms>": [],
    "<large corpora>": [],
    "<computed loss>": [
        "The computed loss is saved as a parameter of the module."
    ],
    "<th value>": [],
    "<CPU cores, cpu cores>": [],
    "<Local transform>": [],
    "<shape information>": [
        "Since shape inference is an optional feature, and the shapes of tensors may vary dynamically, shape functions must be robust to incomplete shape information for any of the inputs.",
        "if not enough shape information is available from inputs to build Properties.",
        "The math break down for uncompressed is: The rest is in order: shape information The math break down for compressed is: The rest is in order: shape information codec information data buffer.",
        "this constructor creates new array using given buffer (without memory allocating) and shape information stored in shape."
    ],
    "<percentile value>": [],
    "<Eager mode, eager mode>": [],
    "<third layer>": [],
    "<checkpoint number>": [],
    "<JIT, jit>": [
        "JIT compilation can be turned on at the session level or manually for select operations.",
        "JIT compilation for CPU operations must be done via the manual method documented below.",
        "JIT compilation can also be turned on manually for one or more operators.",
        "Once JIT has completed, code is likely to execute faster for all subsequent operations.",
        "Currently JIT at the session level, which is what is used for the tutorial, only supports GPU.",
        "Note: The name JIT for these components is a bit of a misnomer and comes from historical reasons."
    ],
    "<minibatch elements>": [],
    "<Argmax, argmax>": [
        "As argmax is being done here, labels and predictions type can be different."
    ],
    "<positive correlation>": [],
    "<computational semantics>": [],
    "<embedding parameters>": [],
    "<point Tensor, points tensor>": [],
    "<reduction results>": [
        "The shape has rank 2, dimension 0 of size 2 and dimension 1 of size 3: Results of reducing dimensions 0 or 1 with an \"add\" function: Note that both reduction results are 1D arrays."
    ],
    "<great idea>": [],
    "<performance computing>": [],
    "<Hermitian spectrum>": [],
    "<convolution kernels>": [
        "Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block."
    ],
    "<cifar10>": [],
    "<level abstraction>": [],
    "<GPU Acceleration, GPU acceleration>": [],
    "<Gaussian filter>": [],
    "<real type>": [],
    "<i1e>": [],
    "<participating device>": [],
    "<interval indices>": [],
    "<computational cost>": [],
    "<cart left>": [],
    "<validation step>": [],
    "<matrix operation, matrix operations, matrix ops>": [],
    "<parameters concentration>": [],
    "<space operation>": [],
    "<output slice>": [],
    "<distribution root>": [],
    "<relative probability>": [],
    "<Levenshtein distance>": [],
    "<network storage>": [],
    "<CPU tensors>": [],
    "<Binary matrix, binary matrix>": [],
    "<Alignment modes, alignment mode>": [
        "The alignment mode option is explained in the next section.",
        "Since each example has sequences of different lengths, an alignment mode of align end is needed.",
        "This alignment mode is needed due to the fact that the number of time steps differs between different patients.",
        "Since each example has exactly 50 timesteps, an alignment mode of equal length is needed.",
        "In fact, the same approach as in example 3 can do this: Alignment modes are relatively straightforward."
    ],
    "<Running Estimators, running estimates>": [
        "The running estimates are Note."
    ],
    "<data samples>": [],
    "<classnames>": [],
    "<top elements>": [
        "The top element of the column vector combines with the top elements of each column in the matrix, and so forth.",
        "The two top elements are then multiplied by each other, as are the bottom two, and the two products are added to consolidate in a single scalar."
    ],
    "<one suggested>": [
        "I'm ok to use older binaries but the one suggested by @sangeet is not compatible."
    ],
    "<dense shape>": [],
    "<training algorithm>": [],
    "<Column vector, column vector, column vectors>": [
        "Each vector must be either row or column vectors.",
        "Below, you can see why row and column vectors lead to different sums."
    ],
    "<highest results>": [],
    "<learning category>": [],
    "<random outcome>": [],
    "<collective operation>": [],
    "<Linear Algebra, Linear algebra, linear algebra>": [
        "XLA (Accelerated Linear Algebra) is an experimental compiler for linear XLA: XLA Overview, which introduces XLA.",
        "Linear algebra libraries contain hundreds of distinct operations."
    ],
    "<tuple shape>": [],
    "<Tensor denominator>": [],
    "<shrinkage function>": [],
    "<integer interval>": [],
    "<map dimensions>": [],
    "<lead dimension>": [],
    "<Virginica, virginica>": [],
    "<dedicated GPU>": [],
    "<walk sequence>": [],
    "<GPU Devices, GPU device, GPU devices, gpu device>": [
        "Unlike JIT compilation on the standard CPU and GPU devices, these devices make a copy of data when it is transferred on and off the device.",
        "True iff a gpu device of the requested kind is available."
    ],
    "<small node>": [],
    "<start training>": [],
    "<delta extraction>": [],
    "<integration better>": [],
    "<gbar>": [],
    "<Log entropy>": [],
    "<TBPTT length>": [],
    "<Variable tensors, variable tensor>": [],
    "<multiple gradients>": [],
    "<input schema>": [],
    "<circular padding>": [],
    "<repartition data>": [],
    "<final layer>": [
        "Recall, the final layer of a CNN model, which is often times an FC layer, has the same number of nodes as the number of output Imagenet, they all have output layers of size 1000, one node for each class."
    ],
    "<column Keys>": [],
    "<adjoint, adjoints>": [],
    "<Slice function>": [],
    "<little counterintuitive>": [],
    "<random port>": [],
    "<OOV>": [],
    "<embedding vector, embedding vectors>": [
        "Well, the following \"formula\" provides a general rule of thumb about the number of embedding dimensions: That is, the embedding vector dimension should be the 4th root of the number of categories."
    ],
    "<initial attentions>": [],
    "<model stands>": [],
    "<Gaussian kernel, gaussian kernel>": [],
    "<zero masking>": [],
    "<quadratic equation>": [],
    "<quickdraw>": [],
    "<multiple retries>": [],
    "<maximum count>": [],
    "<filled array>": [],
    "<flattened representation>": [],
    "<data linearly>": [],
    "<float number, float numbers, floating numbers>": [],
    "<cooccurrences list>": [
        "Parameters specifying, if cooccurrences list should be build into both directions from any current word.",
        "Parameter specifying, if cooccurrences list should be shuffled between training epochs."
    ],
    "<Manhattan distance>": [],
    "<computation device>": [],
    "<values Functions>": [],
    "<orthogonal matrix>": [],
    "<discrete input>": [],
    "<teh shape>": [],
    "<sparse copy>": [],
    "<true shuffling>": [],
    "<tarball>": [],
    "<train phase>": [],
    "<occurrence count>": [],
    "<core batch>": [],
    "<Regression Data>": [],
    "<reparameterized sample>": [
        "The reparameterized sample therefore becomes differentiable."
    ],
    "<distortion power>": [],
    "<static number>": [],
    "<evaluate call>": [],
    "<Optional padding>": [],
    "<one Graph>": [],
    "<variational autoencoder>": [],
    "<radius parameters>": [],
    "<weighted infinity>": [],
    "<singleton dimensions>": [],
    "<output node, output nodes>": [],
    "<Sequence index, sequence index>": [],
    "<max steps>": [],
    "<second copy>": [
        "After each iteration is complete, this second copy is copied back the first copy.",
        "The first copy is used to compute cluster allocations for each step, and for inference, while the second copy is the one updated each step using the mini-batch update rule."
    ],
    "<THP>": [],
    "<Jacobian matrix, jacobian matrix>": [
        "Note: Jacobian matrix is either constant for both forward and inverse or neither."
    ],
    "<update equations>": [],
    "<color channels>": [],
    "<spatial dropout>": [
        "Spatial dropout: can only be applied to 4D (convolutional) activations."
    ],
    "<output matrices, output matrix>": [
        "Note that the output matrices map output coordinates to input coordinates.",
        "The output matrices map the output coordinates (in homogeneous coordinates) of each transform to the corresponding input coordinates.",
        "If adjoint is False then each output matrix satisfies matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]."
    ],
    "<Recurrent Unit, recurrent unit>": [],
    "<logging step>": [],
    "<variable update, variable updates>": [],
    "<Normal batch>": [],
    "<lower memory>": [],
    "<FULL>": [],
    "<sentence tensor>": [],
    "<PRNGs>": [],
    "<second dimension, second dimensions>": [],
    "<corresponding evaluation>": [],
    "<sample inputs>": [],
    "<master branch>": [],
    "<pretrainable layers>": [
        "Non-pretrainable layers are ignored."
    ],
    "<gradient comparisons>": [],
    "<Tensor computation, tensor computation>": [],
    "<binary labels>": [],
    "<single device>": [
        "For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device."
    ],
    "<Maddison>": [],
    "<volumetric sampling>": [],
    "<shape stride>": [],
    "<expanded shape>": [],
    "<initial weight, initial weights>": [],
    "<upper triangular>": [
        "The upper triangular elements above the diagonal are ignored.",
        "The upper triangular part of the matrix is defined as the elements on and above the diagonal."
    ],
    "<standard layers>": [],
    "<arbitrary name>": [],
    "<control inputs>": [
        "None, both control inputs and control outputs are enabled.",
        "Regular inputs may optionally be followed by control inputs that have the format \"^node\"."
    ],
    "<automatic input>": [],
    "<V1/V2>": [],
    "<combined tensor>": [],
    "<Absolute sum>": [],
    "<network performing>": [],
    "<shape samples>": [],
    "<batch queue>": [],
    "<latent values>": [],
    "<better results, good results>": [],
    "<best path>": [],
    "<circulant>": [],
    "<GPU executions>": [],
    "<signal dimension>": [],
    "<Validate dimensions>": [],
    "<concrete shape>": [],
    "<single transform, single transformation>": [
        "Each transform is length 8 (single transform) or shape (N, 8) (batched transforms)."
    ],
    "<starting threshold>": [],
    "<fixed batch>": [],
    "<word frequencies, word frequency>": [],
    "<forward loop>": [],
    "<asynchronous GPU>": [
        "This allows asynchronous GPU copies of the data meaning transfers can be overlapped with computation."
    ],
    "<post noise>": [
        "Consequently, the parameters (post noise) should be cleared after each training iteration."
    ],
    "<attention window>": [],
    "<model outside>": [],
    "<interpretting>": [],
    "<one network>": [],
    "<input words>": [],
    "<Euclidean norm>": [],
    "<Model Persistence>": [],
    "<multiple Integer, multiple integer>": [],
    "<mtimes>": [],
    "<model argument>": [],
    "<dense level>": [],
    "<engineering function>": [],
    "<Distributed error>": [],
    "<entry index>": [],
    "<one queue>": [],
    "<Local Layers>": [],
    "<result nodes>": [],
    "<indexing operation>": [],
    "<expanding columns>": [],
    "<training argument>": [
        "The training argument takes a boolean specifying whether or not the model is currently being run in training mode; dropout will only be performed if training is True."
    ],
    "<Image Classifier, image classifier>": [],
    "<loss reduction>": [],
    "<event dimensions>": [],
    "<unfrozen layers>": [],
    "<nested sequences>": [
        "The nested sequence of the resulting slices will be applied to fn."
    ],
    "<Concentration parameter, concentration parameters>": [],
    "<negative correlation>": [],
    "<matrix addition>": [],
    "<edge effect>": [],
    "<generalization performance>": [],
    "<network connectivity>": [],
    "<sum matrix>": [
        "The sum matrix represents the march of that column vector across the matrix from left to right, adding itself along the way.",
        "The sum matrix represents that row vector falling down the matrix from top to bottom, adding itself at each level."
    ],
    "<float16>": [],
    "<reduce training>": [],
    "<TPU distribution>": [],
    "<final ordering>": [],
    "<E5>": [],
    "<central unit>": [],
    "<TriL, tril, trilinear>": [],
    "<incorrect shapes>": [],
    "<negative section>": [],
    "<Optional center>": [],
    "<regularization parameters>": [],
    "<linear equations>": [],
    "<convolution parameters>": [],
    "<area interpolation>": [],
    "<signal sequence>": [],
    "<Upsampling, upsampling>": [],
    "<custom layer>": [],
    "<validation split>": [],
    "<Content Loss>": [],
    "<Operations Normalization>": [],
    "<entire sample>": [],
    "<Autoencoder, Autoencoders, autoencoder, autoencoders>": [
        "Autoencoders are neural networks for unsupervised learning.",
        "The autoencoder here has been tuned to converge with an average reconstruction error of approximately 2% when trained for 35+ epochs.",
        "In deep learning, an autoencoder is a neural network that \"attempts\" to reconstruct its input.",
        "In practice, autoencoders are often applied to data denoising and dimensionality reduction.",
        "This is because our seq2seq autoencoder uses multiple inputs/outputs.",
        "Autoencoders are also useful for data visualization when the raw input data has high dimensionality and cannot easily be plotted.",
        "MADE: Masked Autoencoder for Distribution Estimation."
    ],
    "<monitor memory>": [],
    "<Least Squares, Least squares, least squares>": [
        "WALS (Weighted Alternating Least Squares) is an algorithm for weighted matrix factorization."
    ],
    "<HTK>": [],
    "<baseline model>": [
        "Here's the impact of our L2 regularization penalty: As you can see, the L2 regularized model has become much more resistant to overfitting than the baseline model, even though both models have the same number of parameters."
    ],
    "<reduction occurs>": [
        "Conditional reduction: apply the reduces on a specified column, where the reduction occurs *only* on those Beware, the output will be huge!"
    ],
    "<pixel convolution>": [],
    "<word existence>": [],
    "<averaging calculation>": [],
    "<lcc>": [],
    "<tensor graph>": [],
    "<one larger>": [],
    "<Processed data, processed data>": [],
    "<LMDB, lmdb>": [],
    "<tfb>": [],
    "<output depth>": [],
    "<Network Architectures, network architecture, network architectures>": [
        "In this case, our network architecture will depend completely on the input sentence."
    ],
    "<returing>": [],
    "<featurized data>": [],
    "<perform dropout>": [],
    "<map node>": [],
    "<individual matrix>": [
        "For batch of matrices, each individual matrix is raised to the power n."
    ],
    "<numeric column, numerical columns>": [],
    "<inner model>": [],
    "<global norm>": [
        "Otherwise the global norm would be underestimated because of all-zero tensors that should be ignored.",
        "The global norm with aggregated gradients can be bad as one replica's huge gradients can hurt the gradients from other replicas."
    ],
    "<fitting data>": [],
    "<Input1, input1>": [],
    "<convergence problems>": [
        "If convergence problems are observed, try increasing or decreasing this by a factor of 10 - say 1e-4 and 1e-2."
    ],
    "<selected device>": [],
    "<Loss scale, loss scale>": [
        "The loss scale is not updated for the lifetime of the class.",
        "Loss scale manager uses an exponential update strategy.",
        "Loss scale managers with a different strategy should subclass this class."
    ],
    "<Gradle, gradle>": [
        "Gradle with snapshots and Maven classifiers appears to be a problem."
    ],
    "<Jaccard, jaccard>": [],
    "<bias correction>": [],
    "<model improves>": [],
    "<Viz, viz>": [],
    "<multiple evaluation>": [],
    "<understanding images>": [],
    "<bincount>": [],
    "<link line>": [],
    "<MAF>": [
        "MAF and IAF have opposite computational tradeoffs - MAF can train all units in parallel but must sample units sequentially, while IAF must train units sequentially but can sample in parallel."
    ],
    "<performing feature>": [
        "In practice though, performing feature crosses still adds significant value to the learning capability of your models."
    ],
    "<snapshot tensor>": [],
    "<Cropping1D>": [],
    "<Run batch>": [],
    "<sparse type>": [],
    "<input filled>": [],
    "<Bernoulli distribution, Bernoulli distributions, bernoulli distribution>": [],
    "<compat>": [],
    "<Integer column, Integer columns, integer column, integer columns>": [],
    "<WALS>": [
        "WALS (Weighted Alternating Least Squares) is an algorithm for weighted matrix factorization."
    ],
    "<computation differed>": [],
    "<boundary detection>": [],
    "<saved value>": [],
    "<arange>": [],
    "<white list>": [],
    "<channel instructions>": [],
    "<rth>": [],
    "<normal noise>": [],
    "<time gap>": [],
    "<h5py>": [],
    "<previous layer, previous layers>": [],
    "<postgres>": [],
    "<vector dimension>": [],
    "<shift Tensor>": [],
    "<Training processes, training process, training processes>": [
        "The training process will then continue in this way until the model is fully trained.",
        "This way you can use a trained model without having to retrain it, or pick-up training where you left of\u2014in case the training process was interrupted.",
        "The training process itself can take several hours, so make sure you have a machine available for that long.",
        "If True, performs intermediate model checkpoints and exports during the training process, rather than only once model training is complete.",
        "If your training process uses workspaces, we recommend that you disable (or reduce the frequency of) periodic GC calls.",
        "This is very convenient as my training process uses standard disposable cloud workers that should not store anything of a value on their local drives!"
    ],
    "<corresponding examples>": [],
    "<model1>": [],
    "<indexed values>": [],
    "<Back Propagation, back propagation>": [],
    "<Input weight, input weight>": [],
    "<iostream>": [],
    "<mutiple values>": [],
    "<nested graph>": [],
    "<bin values>": [],
    "<scalar addition>": [],
    "<data transmission>": [],
    "<linear interpolation>": [],
    "<loss definition>": [
        "Default loss definition with state replicated across a batch."
    ],
    "<decreasing length>": [],
    "<Partitioner, partitioner, partitioners>": [
        "partitioner: Partitioner to be passed to the Checkpointable API."
    ],
    "<compute diagonal>": [],
    "<automatic kernel>": [],
    "<Sparse functions>": [],
    "<axis dimension, axis dimensions>": [],
    "<output classification>": [],
    "<parameters values>": [],
    "<Deterministic distribution>": [],
    "<call operator>": [],
    "<transfer model, transfer models>": [],
    "<HDF5, hdf5>": [
        "HDF5 is really sensitive about the order its resources are deallocated in."
    ],
    "<LLVM>": [],
    "<sparse segments>": [],
    "<PCIe>": [],
    "<Adjust contrast>": [],
    "<flat indices>": [],
    "<NN, Neural Net, Neural Network, Neural Networks, Neural net, Neural network, Neural networks, neural net, neural nets, neural network, neural networks, nn>": [
        "Neural networks can be difficult to tune.",
        "The neural network package contains various modules and loss functions that form the building blocks of deep neural networks.",
        "If the neural network outperforms the previous best model, then we save the neural network.",
        "A complex neural net using Convolutional and Recurrent layers is trained on a set of training videos.",
        "ONNX (Open Neural Network Exchange) is an open format to represent deep learning models.",
        "If your Neural Network is throwing nan's then the solution is to retune your network to avoid the very small gradients.",
        "Central to the idea of deep learning is that the neural network learns representations of the features, rather than requiring the programmer to design them herself.",
        "While neural networks are typically run on powerful computers using multiple GPUs, the compatibility of Deeplearning4J with the Android platform makes using DL4J neural networks in android applications a possibility.",
        "Neural network libraries contain dozens of layer types.",
        "How to save (and load) neural networks trained on Spark.",
        "Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model.",
        "Neural networks can find complex relationships between features and the label.",
        "Your neural network is ready to be used.",
        "Convolutional Neural Networks are mainly used for image recognition, although they apply to sound and text as well.",
        "If we use too many epochs, then the neural network is likely to overfit.",
        "Neural Networks, in particular, have a wide variety of hyperparameters.",
        "Neural network hyperparameters are parameters set prior to training.",
        "A Recurrent Neural Network, or RNN, is a network that operates on a sequence and uses its own output as input for subsequent steps.",
        "Most neural networks expect the images of a fixed size.",
        "In line with the Python interface, neural networks based on the C++ frontend are composed of reusable building blocks called modules.",
        "The neural network will then attempt to classify an observation using the vectorized data in the output layer.",
        "In multi-task learning, a neural network is used to make multiple independent predictions.",
        "Overfitting occurs when the neural network learns the noise in the training data and thus does not generalize well to data it has not been trained on.",
        "Convolutional neural networks (CNNs) are the current state-of-the-art model architecture for image classification tasks.",
        "Classifiers, and neural networks more generally, work on vectors of real numbers.",
        "We can interpret the softmax values for a given image as relative measurements of how likely it is that the image falls into each target Note: For a more comprehensive walkthrough of CNN architecture, see Stanford Convolutional Neural Networks for Visual Recognition course materials.",
        "These two values will determine which number the neural network has classified the drawing as and how confident the network score is.",
        "On the other hand, if we use too few epochs, the neural network might not have the chance to learn fully from the training data.",
        "Simple, straightforward, and focussed on image recognition, a task that Neural Networks do well.",
        "We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth.",
        "That is, both the data pipeline and the matrix manipulations determine how long a neural network takes to train on a dataset.",
        "However, neural networks can solve the task purely by looking at the scene, so we'll use a patch of the screen centered on the cart as an input.",
        "At the end of each epoch, the neural network is evaluated on the test set.",
        "Here's what our training data will look like: As you might have guessed, our neural network will behave like an XOR gate.",
        "Neural networks work best when the data they're fed is normalized, constrained to a range between -1 and 1.",
        "But, since neural networks are universal function approximators, we can simply create one and train it to resemble \\(Q^*\\).",
        "This method receives heavy use, because many neural network components expect their inputs to have a certain shape.",
        "An important detail to note is that neural networks from the torch library are trained with tensor values ranging from 0 to 1."
    ],
    "<ellipsis dimensions>": [
        "If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output."
    ],
    "<cell inputs>": [],
    "<TPU job>": [],
    "<local step, local steps>": [],
    "<overlaps tensor>": [],
    "<NLP models>": [],
    "<Average Pooling, Average pooling, average pooling>": [
        "Fractional average pooling is similar to Fractional max pooling in the pooling region generation step."
    ],
    "<numerical issues>": [],
    "<leaf parameter, leaf parameters>": [
        "Note that leaf parameters are parameters that do not have any nested parameter spaces."
    ],
    "<gain value>": [],
    "<Compute number>": [],
    "<real outcomes>": [],
    "<Bigtable, bigtable>": [],
    "<HW, hour window>": [],
    "<processed vector>": [],
    "<construtor>": [],
    "<vast number>": [],
    "<plot image>": [],
    "<Frobenius norm>": [],
    "<distinct models>": [],
    "<learned parameters>": [],
    "<reverse transformation>": [],
    "<dilation rate, dilation rates>": [],
    "<per step>": [],
    "<Effective padding>": [],
    "<argument tensors>": [
        "The argument tensors can be a list or a dictionary of tensors."
    ],
    "<MLP>": [],
    "<nonlinearity function>": [],
    "<RFFT, rfft>": [],
    "<data count>": [],
    "<data splits>": [],
    "<automatic procedure>": [],
    "<leaf Tensors>": [],
    "<max mode>": [],
    "<window function>": [
        "Typically the output schema is the same as the input schema, but not necessarily (for example, if the window function adds columns for the window start/end times)."
    ],
    "<reduce index, reduced indices>": [
        "All reduced indices must have non-zero size."
    ],
    "<approximation term>": [],
    "<junit>": [],
    "<forward examples>": [],
    "<proce, processable>": [],
    "<device value, device values>": [],
    "<data outputs>": [],
    "<subfolder, subfolders>": [],
    "<accurate models>": [],
    "<complex graphs>": [],
    "<distributed numbers>": [],
    "<model initialization>": [],
    "<kernel points>": [],
    "<remote case>": [],
    "<multi layer>": [],
    "<large networks, larger networks>": [
        "Notice that the larger network begins overfitting almost right away, after just one epoch, and overfits much more severely."
    ],
    "<plotting statistics>": [],
    "<masking layers>": [],
    "<score values>": [],
    "<right number>": [],
    "<standard convolutions>": [],
    "<output dimension, output dimensionality, output dimensions>": [
        "Another way to think about this is that the output dimension equals the number of weights of the linear model; the larger this dimension, the larger the \"degrees of freedom\" of the model.",
        "However, after a certain threshold, higher output dimensions increase the accuracy by very little, while making training take more time."
    ],
    "<resume training, resuming training>": [],
    "<one scalar>": [],
    "<recent model>": [],
    "<Passthrough, passthrough>": [
        "Passthrough = feed forward the input mask (if/when necessary) but don't actually apply it."
    ],
    "<List length>": [],
    "<Build System, build system>": [
        "Fortunately, our build system enables this."
    ],
    "<transformation parameters>": [],
    "<unseen data>": [],
    "<similar operation>": [
        "A similar operation is defined for columns."
    ],
    "<single Multinomial>": [],
    "<specificity value>": [],
    "<dimensional padding>": [],
    "<Upsampling2D>": [],
    "<error calculation>": [],
    "<column factor, column factors>": [],
    "<leaf variable>": [],
    "<Binomial Probabilities>": [],
    "<expected shape>": [
        "shape: Shape tuple, expected shape of the input (may include None for unchecked axes)."
    ],
    "<padding argument>": [
        "The padding argument specifies the amount of zero padding to be applied to the base area.",
        "The padding argument specifies one of two enumerated values (case-insensitive): valid (default value) or same."
    ],
    "<little frustrated>": [],
    "<failed node>": [
        "This is because a failed node may process part of a partition (sending out updates) before failing."
    ],
    "<scale scale>": [],
    "<training time, training times>": [
        "Total training time is always ETL plus computation."
    ],
    "<partial forward>": [],
    "<TPU, TPUs, tpu, tpus>": [
        "TPU prediction only works on a single host (one TPU worker).",
        "As Cloud TPUs are in alpha, you will need to specify a API definition Cloud Platform project.",
        "The majority of example TPU models can be run in this local mode, by setting the command line flags as follows: API.",
        "TPU topologies are 3-dimensional, with dimensions (x, y, core number)."
    ],
    "<upper range>": [],
    "<compute nodes>": [],
    "<reverse flag>": [],
    "<normalization constant>": [],
    "<unknown mean>": [],
    "<Sender rank>": [],
    "<Distributed Applications>": [],
    "<empty features>": [],
    "<model weights>": [],
    "<vector relative>": [],
    "<input operand>": [],
    "<Data replication>": [],
    "<Optional list, optional list>": [
        "**kwargs: Optional list or tuple or set of Variable objects to Methods."
    ],
    "<relationship column>": [],
    "<sequence alignment>": [],
    "<batch transformation, batch transformations, batching transformation>": [],
    "<connection weights>": [
        "The connection weights and biases are managed by the layer object."
    ],
    "<Optimizer, Optimizers, optim, optimiser, optimizer, optimizers>": [
        "This optimizer delegates all aspects of gradient calculation and application to an underlying optimizer.",
        "This argument has no effect if optimizer is an instance of an Optimizer.",
        "This optimizer only guarantees sparsity for linear models.",
        "optimizer: The optimizer used to train the model.",
        "Note that this optimizer can also be instantiated as.",
        "To prevent underflow, this optimizer multiplies the loss by a factor before backprop starts.",
        "As global optimization objective is strongly-convex, the optimizer optimizes the dual objective at each step.",
        "This optimizer is usually a good choice for recurrent neural networks.",
        "The optimizer adds nodes to the graph to collect gradients and pause the trainers until variables are updated.",
        "An optimizer applies the computed gradients to the model's variables to minimize the loss function.",
        "optimizer has the wrong type.",
        "Should only be called after computing the gradients (otherwise the optimizer has no weights).",
        "This optimizer records the global step for each worker before computing gradients and compares it with the global step at the time of applying the gradients.",
        "Examples are sampled uniformly, and the optimizer is learning rate free and enjoys linear convergence rate.",
        "and .grad is getting filled in but the optimizer does not change the values.",
        "If an optimizer was found as part of the saved model, the model is already compiled.",
        "This optimizer avoids stale gradients by collecting gradients from all replicas, averaging them, then applying them to the variables in one shot, after which replicas can fetch the new variables and continue.",
        "This optimizer is useful when scaling the batch size to up to 32K without significant performance degradation.",
        "The optimizer applies each update one example at a time.",
        "Optimizer s also support specifying per-parameter options.",
        "Note that Optimizer instances should not bind to a single graph, and so shouldn't keep Tensors as member variables.",
        "After applying gradients, this optimizer then clips the variable to have a maximum L2 norm along specified dimensions.",
        "Multiple optimizers that use the same or different losses are supported.",
        "This optimizer doesn't support per-parameter options and parameter groups (there can be only one).",
        "The optimizer may replace an aggregate op taking input from multiple devices with a tree of aggregate ops that aggregate locally within each device (and possibly within groups of nearby devices) before communicating.",
        "If it is set to a smaller value, the optimizer is more aggressive in reducing the global Estimator (where there is one process per worker) is the number of workers running the train steps.",
        "This optimizer takes care of regularization of unseen features in a mini batch by updating them when they are seen with a closed form update rule that is equivalent to having updated them on every mini-batch."
    ],
    "<weights list>": [],
    "<numerical stability>": [],
    "<regression head>": [],
    "<value clipping>": [],
    "<left Methods>": [],
    "<hypothesis sequences>": [],
    "<partition function>": [],
    "<Sequence Learning, Sequence learning, sequence learning>": [
        "Only sequence learning is affected."
    ],
    "<value data>": [],
    "<composing layers>": [],
    "<Training Epochs, training epochs>": [],
    "<Laplacian pyramid>": [],
    "<epoch indices>": [],
    "<sum operation, sum operator>": [
        "The sum operation operates over all the elements.",
        "The sum operation still operates over all the elements, and divides by n.",
        "This operator is similar to the unsorted segment sum operator found here."
    ],
    "<time distributed>": [],
    "<label strings>": [],
    "<input vector>": [],
    "<Count number>": [],
    "<Data Science, data science>": [],
    "<optional inputs>": [],
    "<larger step>": [
        "This is because larger epsilons mean we take a larger step in the direction that will maximize the loss."
    ],
    "<matrix matrix>": [],
    "<Model definition, model definition>": [
        "The model definition and a pre-trained model can be found here.",
        "The model definition is in the pytorch/examples repository we cloned previously, and with a few lines of python we can export it to ONNX."
    ],
    "<intermediate tensors>": [],
    "<tensors layer>": [],
    "<vin>": [],
    "<gradient op, gradient operation>": [
        "A gradient op represents a jacobian operation."
    ],
    "<Chatbot, chatbot>": [],
    "<small distributed>": [],
    "<control outputs>": [
        "None, control outputs are enabled."
    ],
    "<old matrix>": [],
    "<libjpeg>": [],
    "<academic literature>": [],
    "<error free>": [],
    "<Eigenvectors, eigenvectors>": [
        "If it is True, both eigenvalues and eigenvectors are computed."
    ],
    "<uninformed state>": [],
    "<local training>": [],
    "<label weight>": [],
    "<lower frequency>": [],
    "<indicator function>": [],
    "<Another tensor, another Tensor, another tensor>": [],
    "<erf, erfinv>": [],
    "<loading embedding>": [],
    "<prediction task>": [],
    "<independent layers>": [],
    "<standard process>": [],
    "<tail decay>": [],
    "<distributed variable>": [
        "Indicates how a distributed variable will be aggregated."
    ],
    "<onto tensors>": [],
    "<time equal>": [],
    "<next order>": [
        "Our next order of business is to create a vocabulary and load query/response sentence pairs into memory."
    ],
    "<random weights>": [],
    "<Learning Rate, Learning Rates, Learning rate, learning rate, learning rates>": [
        "Cycle schedule Starts at initial learning rate, then linearly increases learning rate until max learning rate is reached, at that point the learning rate is decreased back to initial learning rate.",
        "If learning rate shrinks too much, the net's learning is no longer efficient.",
        "There are infinite configurations of the learning rate and hidden layer size, since the learning rate space is continuous.",
        "For eg, if a learning rate is specified this learning rate will apply to all unfrozen/trainable layers in the model.",
        "If the score increases consistently, your learning rate is likely set too high.",
        "Learning configs (like updaters, learning rate etc) specified with the layer here will be honored.",
        "The learning rate is one of, if not the most important hyperparameter.",
        "Also, the default learning rate is not optimal for all of the models, so to achieve maximum accuracy it would be necessary to tune for each model separately.",
        "This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.",
        "Specify the preprocessor for the added layers Usage example: specify a learning rate will set specified learning rate on all layers The specified layer and the layers preceding it will be \"frozen\" with parameters staying constant.",
        "Add layers to the net Usage example: specify a learning rate will set specified learning rate on all layers Note this will also affect the layer that follows the layer specified, unless it is the output layer.",
        "The issues mentioned above (learning rate, normalization, data shuffling) may contribute to this.",
        "If not set, the loss, the learning rate, and the global norm of the gradients will be reported.",
        "Learning rate schedules can be specified either based on the number of iterations, or the number of epochs that have elapsed.",
        "If the score is flat or decreases very slowly (over a few hundred iterations) (a) your learning rate may be too low, or (b) you might be having difficulties with optimization.",
        "The Learning rate curve Looks something like this: +-----------------------------------------+ | XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XXX | | XXX | | XXX | | | +-----------------------------------------+."
    ],
    "<inner dimension>": [],
    "<final divide>": [],
    "<world data>": [
        "This data should have the same distribution as the real-world data you want to make predictions about with your model."
    ],
    "<single branch>": [],
    "<accuracy evaluation>": [],
    "<linear Independent>": [],
    "<Visual Studio>": [
        "Visual Studio doesn't support parallel custom task currently."
    ],
    "<data fetcher>": [],
    "<device data>": [],
    "<training hook, training hooks>": [],
    "<Kumaraswamy distribution>": [],
    "<output units>": [],
    "<adi>": [],
    "<groupname>": [],
    "<multiple tensors>": [],
    "<cron>": [],
    "<constant multiplier>": [],
    "<device strings>": [],
    "<Identification layer>": [],
    "<complex modulus>": [],
    "<optimization techniques>": [],
    "<moving variance>": [],
    "<batch axis>": [],
    "<mode value>": [],
    "<real multi>": [],
    "<OOM, oom>": [
        "if the oom happens exactly after 20 pics, you could set a breakpoint at that moment and have a closer look at the variables."
    ],
    "<gradient error>": [],
    "<weight vector, weights vector>": [
        "A weight vector of 1s should give identical results to no weight vector."
    ],
    "<Float32, float32>": [
        "Float32 truncated to 16 bits.",
        "float32 may be faster, but can be problematic for larger models and longer time series."
    ],
    "<dense one>": [],
    "<example inputs>": [
        "However, if a function with data-dependent if statements and loops is traced, only the operations called along the execution route taken by the example input will be recorded."
    ],
    "<CELU, celu>": [],
    "<label format>": [],
    "<call matrix>": [],
    "<convolution arithmetic>": [],
    "<sampling decoder>": [],
    "<MPI, mpirun>": [
        "Checks if MPI is available.",
        "The reason for these changes is that MPI needs to create its own environment before spawning the processes.",
        "Note that we used the TCP backend, but we MPI or Gloo instead.",
        "The Message Passing Interface (MPI) is a standardized tool from the field of high-performance computing."
    ],
    "<update state, updates state>": [],
    "<asd>": [],
    "<degree node, degree nodes>": [
        "A series of nodes can be ungrouped so that the nodes in the series do not Selection can also be helpful in understanding high-degree nodes."
    ],
    "<matrix manipulations>": [
        "That is, both the data pipeline and the matrix manipulations determine how long a neural network takes to train on a dataset."
    ],
    "<step time>": [],
    "<tensor range>": [],
    "<random behavior>": [],
    "<resulting variable>": [
        "The resulting variable can be used for example for evaluating functions at all locations on a grid."
    ],
    "<images total>": [],
    "<account statistics>": [],
    "<evaluation labels>": [],
    "<label smoothing>": [],
    "<word shape>": [],
    "<Iris problem>": [
        "The Iris problem is a classic classification problem."
    ],
    "<distributed category>": [],
    "<scalar angle>": [],
    "<Image grid, image grid>": [],
    "<CycleGAN training>": [],
    "<pre update>": [],
    "<Softplus, softplus>": [],
    "<positive images>": [],
    "<Singular Value, Singular values, singular value, singular values>": [
        "Otherwise, only the singular values will be computed, which can be significantly faster."
    ],
    "<analytic knowledge>": [],
    "<categorical identity>": [
        "Categorical identity columns can be seen as a special case of bucketized columns."
    ],
    "<Tensor input, Tensor inputs, tensor input, tensor inputs>": [],
    "<sorted vector>": [],
    "<step variable>": [],
    "<evaluation graph>": [],
    "<Train generator, train generator>": [],
    "<model every>": [],
    "<minimum range>": [],
    "<Rprop>": [],
    "<graph proto>": [],
    "<fixed threshold>": [],
    "<access model>": [],
    "<cmdline>": [],
    "<double Tensor, double tensor, double tensors>": [],
    "<distribution vectors>": [],
    "<model protobufs>": [],
    "<activation array, activations array>": [],
    "<coupling layer>": [],
    "<f32>": [
        "A conversion such as T=s32 to U=f32 will perform a normalizing int-to-float Note: The precise float-to-int and visa-versa conversions are currently unspecified, but may become additional arguments to the convert operation in the future."
    ],
    "<batched inference>": [],
    "<Rotated image, rotated image>": [],
    "<time grid>": [],
    "<unzipped distribution>": [],
    "<logical core>": [],
    "<Gradient tapes, gradient tape, gradient tapes>": [],
    "<kmeans>": [],
    "<scatter operation>": [
        "I is Win + Sin where + is element-wise In summary, the scatter operation can be defined as follows."
    ],
    "<acos, acosh>": [],
    "<index vector>": [],
    "<correlation average>": [],
    "<Multiscale Computers>": [],
    "<shape constraints>": [],
    "<sparse labels>": [],
    "<Pareto distribution>": [],
    "<hot feature>": [],
    "<gram matrix>": [
        "A gram matrix is the result of multiplying a given matrix by its transposed matrix.",
        "Finally, the gram matrix must be normalized by dividing each element by the total number of elements in the matrix."
    ],
    "<backward operation>": [],
    "<Squares loss, square loss, squared loss>": [],
    "<rank-2>": [],
    "<Anton, anton>": [],
    "<logcosh>": [],
    "<Tensor manipulation>": [],
    "<counter epochs>": [],
    "<manipulating tensors>": [],
    "<standard models>": [],
    "<linear system>": [],
    "<value tensors, values tensor>": [],
    "<upper length>": [],
    "<subsamplng value>": [],
    "<either device>": [],
    "<input mini>": [],
    "<real tasks>": [],
    "<channel dimension, channels dimension, channels dimensions>": [
        "The channels dimension of image must be 1.",
        "The channels dimension If format is not specified or is the empty string, a default format is picked in function of the number of channels in image: 1: Output a grayscale image.",
        "However, if you compare the shapes of the input (model output) and the target, you see that the channel dimension is missing (C in the docs)."
    ],
    "<uint32>": [],
    "<concatenated result>": [],
    "<Deep Network, deep net, deep network, deep networks>": [],
    "<output sequence>": [],
    "<classification signature>": [],
    "<improved stability>": [],
    "<meshgrid>": [],
    "<positive elements>": [],
    "<BLAS, Blas, blas>": [
        "Blas buffer util for interopping with the underlying buffers and the given ndarrays."
    ],
    "<debug tensor>": [],
    "<Discrete Cosine>": [],
    "<decoder layer, decoder layers>": [],
    "<NVCC, nvcc>": [],
    "<load status>": [],
    "<bias term>": [],
    "<Merging Tensor, merged tensor>": [],
    "<FGSM>": [],
    "<copy distribution>": [
        "Note: the copy distribution may continue to depend on the original initialization arguments."
    ],
    "<high score>": [],
    "<Vector Representations, Vector representations, vector representations>": [
        "Vector representations are learned using walks (usually random walks) on the vertices in the graph.",
        "Once learned, these vector representations can then be used for purposes such as classification, clustering, similarity search, etc on the graph."
    ],
    "<batch state>": [],
    "<precision data>": [],
    "<performance results>": [
        "Collected below are performance results running training and inference on different types of CPUs on different platforms with various compiler optimizations."
    ],
    "<mixing probabilities>": [],
    "<Sequence Models, Sequence models, sequence model, sequence models>": [
        "Sequence models are central to NLP: they are models where there is some sort of dependence through time between your Model for part-of-speech tagging.",
        "The LSTM tagger above is typically sufficient for part-of-speech tagging, but a sequence model like the CRF is really essential for strong performance on NER."
    ],
    "<Validated type>": [],
    "<logdir>": [
        "Used by chief supervisors if a logdir was specified."
    ],
    "<Copies tensor>": [],
    "<clustering algorithm>": [],
    "<SVI>": [],
    "<multiplicative coefficient>": [],
    "<open problem>": [],
    "<normal scalar>": [],
    "<Tensor output, tensor output>": [],
    "<replaced Tensors>": [],
    "<single library>": [],
    "<defs, defun>": [
        "In particular, defun is not a compiler for arbitrary Python code.",
        "By mapping each input signature to a unique graph, defun lets users transparently compile such code, as the following code snippet demonstrates: On the other hand, because defun generates graphs by tracing and not by source code analysis, it fully unrolls Python for and while loops, potentially creating large graphs."
    ],
    "<inputs tuple>": [],
    "<gram product>": [],
    "<precision format>": [],
    "<data compatibility>": [],
    "<warning mean>": [],
    "<accumulated gradients>": [],
    "<Processing inputs>": [],
    "<pooling operation>": [],
    "<compute moving>": [],
    "<global index>": [],
    "<holdout data>": [],
    "<trained parameters>": [],
    "<Hard sigmoid, hard sigmoid>": [],
    "<standard score>": [],
    "<general point>": [
        "It's an extreme example - but the general point is a valid one."
    ],
    "<either steps>": [],
    "<alarm rate>": [],
    "<original shape>": [
        "More generally, the number of tensors is given by the product of the remaining dimensions, and the shape of the tensors is given by the size of the specified dimensions in the original shape."
    ],
    "<Updates Histogram>": [],
    "<Conv, Convolutional, conv, convolutional>": [],
    "<clipping ratio>": [],
    "<Jaccard similarity>": [],
    "<values corrupted>": [],
    "<indicator column, indicator columns>": [],
    "<point division>": [],
    "<curated list>": [],
    "<caching device>": [],
    "<XLA>": [
        "XLA (Accelerated Linear Algebra) is an experimental compiler for linear XLA: XLA Overview, which introduces XLA.",
        "XLA is modular in the sense that it is easy to slot in an alternative backend to target some novel HW architecture.",
        "XLA ignores control dependencies and so this data dependency is necessary.",
        "XLA uses a similar system for determining shapes at compile time.",
        "The following diagram shows the compilation process in XLA: XLA comes with several optimizations and analysis passes that are CSE, target-independent operation fusion, and buffer analysis for allocating runtime memory for the computation.",
        "XLA requires that all tensor dimensions be statically defined at compile time.",
        "After the target-independent step, XLA sends the HLO computation to a backend.",
        "XLA takes graphs (\"computations\") defined in HLO and compiles them into machine instructions for various architectures.",
        "This is because XLA does not support \"ragged\" arrays.",
        "The XLA gather operation stitches together several slices (each slice at a potentially different runtime offset) of an input array.",
        "To perform element-wise addition, XLA needs to \"broadcast\" the vector v to the same rank as the matrix X, by replicating v a certain number of times."
    ],
    "<structured noise>": [],
    "<evaluation approaches>": [],
    "<zero tensor, zero tensors>": [],
    "<RNN, RNNs, Recurrent Neural Networks, Recurrent neural networks, Rnn, recurrent neural network, recurrent neural networks, rnn>": [
        "A recurrent neural network is a network that maintains some kind of state.",
        "Recurrent neural networks (RNN's) are used when the input is sequential in nature.",
        "A Recurrent Neural Network, or RNN, is a network that operates on a sequence and uses its own output as input for subsequent steps.",
        "RNN layers in DL4J can be combined with other layer types.",
        "Typically RNN's are much more effective than regular feed forward neural networks for sequential data because they can keep track of dependencies in the data over multiple time steps.",
        "Recurrent Neural Networks are useful for processing time series data or other sequentially fed data like video.",
        "This allows different layers to handle masks differently - for example, bidirectional RNNs and normal RNNs operate differently with masks (the former sets activations to 0 outside of the data present region (and keeps the mask active for future layers like dense layers), whereas normal RNNs don't zero out the activations/errors )instead relying on backpropagated error arrays to handle the variable length case.",
        "An RNN learns a string of characters.",
        "RNN's can also be applied to situations where the input is sequential but the output isn't.",
        "One RNN acts as an encoder, which encodes a variable length input sequence to a fixed-length context vector."
    ],
    "<metric value>": [],
    "<Keras variable>": [],
    "<Layer Parameters, layer parameters>": [],
    "<Softmin, softmin>": [],
    "<end range>": [],
    "<activation equal>": [],
    "<zeros variable>": [],
    "<padding mode, padding modes>": [],
    "<TSV>": [],
    "<TFGAN>": [
        "TFGAN is a lightweight library for training and evaluating GANs."
    ],
    "<main diagonal>": [],
    "<unit Normal>": [],
    "<stride value, strides value>": [],
    "<GPU kernel>": [
        "One thing to note, even when the GPU kernel version of pad is used, it still needs its \"paddings\" input in CPU memory."
    ],
    "<random one>": [],
    "<nested tuple>": [
        "Nested tuple shapes are not supported."
    ],
    "<labels distribution>": [],
    "<Shannon Entropy, Shannon entropy>": [],
    "<vector product>": [],
    "<learning paulsteven>": [],
    "<argument shape>": [],
    "<output interval>": [],
    "<certain statistics>": [],
    "<logits shape>": [],
    "<Output Tensor, Output tensor, output Tensor, output Tensors, output tensor, output tensors, outputs Tensors, outputs tensor>": [
        "The output tensor has shape.",
        "Other output tensor can be duplicated as well.",
        "When all the input tensors are finished, the output tensor is passed along in the graph.",
        "Note that a tensor can be both an inside tensor and an output tensor if it is consumed by operations both outside and inside of ops.",
        "The output tensor is 1-D of size steps.",
        "All this output tensor knows is its data and shape.",
        "For instance, some output tensor can be omitted.",
        "Output tensor has one more dimension than input tensor, the first dimension indicates the partition.",
        "Like the input, the resulting output tensors have a batch dimension.",
        "The output tensors can also be remapped.",
        "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1.",
        "Currently, only 4-D output tensors (batched image-like tensors) are supported.",
        "keepdim (bool): whether the output tensors have :attr:`dim` retained or not.",
        "If a directory is passed to --outdir option, the outputs will be saved as npy files named after output tensor keys under the given directory."
    ],
    "<Training statistics, training statistics>": [
        "Set whether training statistics should be collected for debugging purposes.",
        "Set whether the training statistics should be collected.",
        "Training statistics may include things like per-epoch run times, These statistics are primarily used for debugging and optimization, in order to gain some insight into what aspects of network training are taking the most time."
    ],
    "<device device>": [],
    "<treebank>": [],
    "<single forward>": [],
    "<moving column>": [],
    "<aka scalar>": [],
    "<Convolution operation, convolution operation, convolution operations, convolution operator>": [
        "Separable convolutions split a regular convolution operation into two simpler operations, which are usually computationally more efficient.",
        "Convolution is the code for applying the convolution operator."
    ],
    "<Model Deployment>": [],
    "<model relies>": [],
    "<density ratio>": [],
    "<stopping hook>": [],
    "<gradient accumulators>": [
        "Once the gradients have been computed, push them into gradient accumulators."
    ],
    "<Monte Carlo>": [],
    "<evaluation mode, evaluation model>": [],
    "<Generative Adversarial, generative adversarial>": [],
    "<Linear stack, linear stack>": [],
    "<inputs flow>": [],
    "<dtype, dtypes>": [
        "The dtype of a tensor can be access via its dtype attribute.",
        "Its dtype should be either float32 or float64.",
        "None means dtype is not enforced.",
        "y: A Tensor whose dtype is compatible with x.",
        "If dtype is also provided, they must be the same data type as specified by dtype.",
        "If dtype is real, this is equivalent to being symmetric.",
        "If dtype is None, the conversion tries its best to infer the right numpy data type.",
        "Tensor dtype and shape match the model.",
        "The dtype of the resulting tensor is inferred from the inputs unless it is provided explicitly.",
        "In that case, dtype must be specified.",
        "Allowed dtypes are float32, complex64.",
        "If a dtype is provided instead of a tensor, the column is also treated as required.",
        "If no initializer is provided, the output structure and dtypes of fn are assumed to be the same as its input; and in this case, the first argument of fn must match the structure of elems."
    ],
    "<Mathematical Society>": [],
    "<definitive list>": [],
    "<Gaussian dropout>": [],
    "<move data, moving data>": [],
    "<filled tensor>": [],
    "<Kth, kth>": [],
    "<task type>": [],
    "<Multivariate Normal, multivariate Normal, multivariate normal>": [
        "The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \\(\\mathbf{\\Sigma}\\) or a positive definite precision matrix \\(\\mathbf{\\Sigma}^{-1}\\) or a lower-triangular matrix \\(\\mathbf{L}\\) with positive-valued diagonal entries, such that \\(\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\\).",
        "The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix :math:`\\mathbf{\\Sigma}` or a positive definite precision matrix :math:`\\mathbf{\\Sigma}^{-1}` or a lower-triangular matrix :math:`\\mathbf{L}` with positive-valued diagonal entries, such that :math:`\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top`."
    ],
    "<one transform>": [],
    "<single checkpoint>": [],
    "<Mahalanobis distance>": [],
    "<histogram structure>": [],
    "<minimum rank>": [],
    "<RDD, RDDs, rdd>": [
        "This option will basically define how many partitions your RDD will be split into.",
        "However, large RDDs may not entirely fit into memory.",
        "Once your RDD is loaded, you can transform it, perform joins and use reducers to wrangle the data any way you want."
    ],
    "<normal pattern>": [
        "The normal pattern is reading the whole input stream and turning that in to a record."
    ],
    "<model graph>": [],
    "<input key, input keys>": [],
    "<data tracking>": [],
    "<U32>": [],
    "<Add matrix>": [],
    "<Distribution sample>": [],
    "<encoder layer, encoder layers>": [],
    "<Cholesky, cholesky>": [],
    "<model individually>": [],
    "<corrupted data>": [],
    "<distributed synchronization>": [],
    "<None elements>": [],
    "<single series>": [],
    "<Categorical vocabulary, categorical vocabulary>": [
        "Categorical vocabulary columns provide a good way to represent strings as a one-hot vector."
    ],
    "<max values>": [
        "Example: Max value of index is 150 and chars count is 3."
    ],
    "<sgv>": [
        "Note that sgv is modified in place."
    ],
    "<Scalar inputs, scalar inputs>": [
        "Scalars may be mixed in; these will be broadcast to the shape of non-scalar inputs.",
        "Caching: the scalar input depth D of the base distribution is not known at construction time."
    ],
    "<model replica, model replicas, model replication>": [
        "Naively employing asynchronous updates of model parameters leads to sub-optimal training performance because an individual model replica might be trained on a stale copy of the model parameters."
    ],
    "<temporal difference>": [
        "For our training update rule, we'll use a fact that every \\(Q\\) function for some policy obeys the Bellman equation: The difference between the two sides of the equality is known as the temporal difference error, \\(\\delta\\): To minimise this error, we will use the Huber loss."
    ],
    "<differential equation, differential equations>": [],
    "<Deterministic mode>": [
        "Deterministic mode can have a performance impact, depending on your model."
    ],
    "<MapDB>": [],
    "<scalar operation, scalar operations>": [
        "A matrix that ordered its elements by row would look like this: Elementwise scalar operations."
    ],
    "<proto file>": [],
    "<rotated tensor>": [],
    "<example model>": [],
    "<surrounding scope>": [],
    "<model knows>": [
        "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing style."
    ],
    "<Covariate Shift>": [],
    "<ink data>": [],
    "<training started, training starts>": [
        "When training started, the model did not know how to spell an English word, or that words were even a unit of text."
    ],
    "<DNN, Deep Neural Networks, deep neural network, deep neural networks, dnn>": [
        "The neural network package contains various modules and loss functions that form the building blocks of deep neural networks."
    ],
    "<Multiprocessing error>": [
        "Multiprocessing error \"driver shut down\"\u00b6."
    ],
    "<prediction graph>": [
        "The prediction graph is always exported."
    ],
    "<Model saved, model saved, model saver>": [],
    "<threshold array, thresholds array>": [],
    "<Depthwise, depthwise>": [],
    "<Layer Types, layer type, layer types>": [
        "RNN layers in DL4J can be combined with other layer types."
    ],
    "<batch algorithm>": [],
    "<vertex sequence>": [
        "A vertex sequence represents a sequences of vertices in a graph."
    ],
    "<load operation>": [
        "The load operation The session in which to restore the graph definition and variables."
    ],
    "<pooling regions>": [],
    "<prediction rule>": [],
    "<inv>": [],
    "<inputs values>": [],
    "<Mini batch, mini batch, mini batches>": [],
    "<Image Adjustments>": [],
    "<actual training>": [],
    "<power function>": [],
    "<allocation unit>": [],
    "<constant module>": [],
    "<entropy introduced>": [],
    "<Mathematical operation, mathematical operation>": [
        "Generally speaking, any commutative mathematical operation can be used as an operator."
    ],
    "<Divergence loss, divergence Loss, divergence loss>": [],
    "<leaning model, learned models>": [
        "Typical Deep leaning model consists of many layers between the inputs and outputs."
    ],
    "<dim2>": [],
    "<nearest clusters>": [],
    "<JavaRDD>": [
        "Converts JavaRDD labeled points to JavaRDD datasets."
    ],
    "<hidden surface>": [],
    "<residual parameters>": [],
    "<classification error>": [],
    "<Sample Adversarial>": [],
    "<multimodal distributions>": [],
    "<multiple checkpoints>": [],
    "<difference step>": [],
    "<general quantization>": [],
    "<unitary operator>": [],
    "<minimum norm>": [],
    "<one minibatch>": [],
    "<loss applied>": [
        "These two lines create a chain on operations linking your model and the loss applied to the output."
    ],
    "<cross entropy loss>": [
        "Binary cross entropy loss function Labels are assumed to take values 0 or 1."
    ],
    "<initial batch>": [],
    "<mixed Tensors>": [],
    "<Tensor Methods, Tensor methods, tensor methods>": [],
    "<srcdir>": [],
    "<StudentT>": [],
    "<DDPG>": [
        "DDPG is a bit complicated to understand regarding the gradient update of the actor.",
        "DDPG is a case of Deep Actor-Critic algorithm, so you have two gradients: one for the actor (the parameters leading to the action (mu)) and one for the critic (that estimates the value of a state-action (Q) \u2013 this is our case \u2013 , or sometimes the value of a state (V) ).",
        "But DDPG is using sample batch."
    ],
    "<mutual information>": [],
    "<tuple structure>": [],
    "<full Indexing>": [],
    "<compression statistics>": [],
    "<variance estimate>": [],
    "<relative similarity>": [],
    "<sparse matrix>": [],
    "<flattened input>": [],
    "<label vector, labels vector>": [
        "My label vector has ones at the classes which are there in the feature.",
        "That is, soft classes are not allowed, and the labels vector must provide a single specific index for the true class for each row of logits (each minibatch entry)."
    ],
    "<Hyperparameter, Hyperparameters, hyperparameter, hyperparameters>": [
        "I suspect my hyperparameters are poorly chosen.",
        "One hyperparameter that affects whether the neural network will overfit or not is the number of epochs or complete passes through the training split.",
        "Hyperparameters are variables that determine how a neural network learns.",
        "Hyperparameter optimization stops if any of the conditions are met.",
        "Hyperparameter optimization attempts to automate this process using software that applies search strategies.",
        "Intuitively, each hyperparameter is like one ingredient in a meal, a meal that can go very right, or very wrong."
    ],
    "<input mean>": [],
    "<initial variance>": [],
    "<mvlgamma>": [],
    "<processing queue>": [],
    "<Time column, Time columns, time column, time columns>": [
        "Add a Time column with the specified restrictions NOTE: Time columns are represented by LONG (epoch millisecond) values.",
        "Time for each entry in the sequence is provided by a Time column The overlapping nature of the windowing function allows for things such as a window size of 1 day, produced every hour.",
        "For time values in human-readable formats, NOTE: Time columns are represented by LONG (epoch millisecond) values.",
        "Time for each entry in the sequence is provided by a Time column Functionality here: Calculate windows of data based on a fixed window size (1 minute, 1 hour, etc), with an optional offset."
    ],
    "<performing model>": [],
    "<mean function>": [
        "The mean function creates two local variables, total and count that are used to compute the average of values."
    ],
    "<neural layers>": [],
    "<input constraints>": [],
    "<Continuous columns>": [],
    "<shape invariant, shape invariants>": [
        "An error will be raised if the shape of a loop variable after an iteration is determined to be more general than or incompatible with its shape invariant."
    ],
    "<MKV>": [],
    "<Compute score>": [
        "Compute score after labels and input have been set."
    ],
    "<Values vector>": [],
    "<link function>": [],
    "<subgraph, subgraphs>": [
        "For instance, a subgraph representing an operation with two inputs can be remapped to only take one input.",
        "A subgraph contains: a list of input tensors, accessible via the inputs property."
    ],
    "<Net model>": [],
    "<fit function>": [],
    "<inverse domain>": [],
    "<surrogate functions>": [],
    "<learning workloads>": [],
    "<Upsample, upsample, upsampled>": [],
    "<LBFGS>": [
        "Only applies for line search optimizers: Line Search SGD, Conjugate Gradient, LBFGS is NOT applicable for standard SGD."
    ],
    "<Minimum layer>": [],
    "<training condition>": [],
    "<correct rank>": [],
    "<PReLU, prelu>": [
        "PReLU layer has weights of input shape (excluding mini-batch dimension)."
    ],
    "<random integers>": [
        "In the integer case, the random integers are slightly biased unless maxval - minval is an exact power of two."
    ],
    "<mobile systems>": [],
    "<log sigmoid>": [],
    "<deterministic function>": [],
    "<Distributed Trainer, distributed trainer>": [],
    "<NLL>": [],
    "<column batches>": [],
    "<small operations>": [],
    "<Task Network, task Networks, task networks>": [],
    "<generate function>": [],
    "<arbitrary classifier>": [],
    "<sliding blocks>": [
        "The padding, stride and dilation arguments specify how the sliding blocks are retrieved."
    ],
    "<inference mode>": [],
    "<tracking shapes>": [],
    "<input Input, input input>": [],
    "<sequence condition>": [],
    "<single flow>": [],
    "<Division operation, division operation>": [],
    "<topological order, topological ordering>": [
        "The topological order defines the order in which forward pass (and hence also backward pass, which is the opposite to this) is conducted in the network."
    ],
    "<large input, large inputs>": [],
    "<Desired interpolation>": [],
    "<displaying data>": [],
    "<previous approach>": [],
    "<desired label>": [],
    "<error score>": [
        "As a well-tuned model continues to train, its error score will decrease with each iteration."
    ],
    "<Tensor type, Tensor types, tensor type, tensor types>": [],
    "<hot representation>": [],
    "<Removes name>": [],
    "<loop state>": [],
    "<unknown dimensions>": [
        "An unknown Dimension is compatible with all other Dimensions."
    ],
    "<serving graph>": [],
    "<dropout probabilities, dropout probability>": [],
    "<eps, evaluation process>": [],
    "<frequency weight, frequency weights>": [
        "NOTE: these weights are treated as \"frequency weights\", as opposed to \"reliability weights\"."
    ],
    "<declaring tensor>": [],
    "<normalized vectors>": [],
    "<non decreasing>": [],
    "<average output>": [],
    "<Sparse features, sparse feature, sparse features>": [
        "Use this when your sparse features are in string or integer format, and you want to distribute your inputs into a finite number of buckets by hashing.",
        "Sparse features can be fed directly into linear models.",
        "Use this when your sparse features are in string or integer format, and you have a vocab file that maps each value to an integer ID.",
        "Sparse features are a primary use case for the linear model tools provided by Estimators."
    ],
    "<GMM>": [],
    "<minimum length>": [],
    "<Training samples>": [],
    "<point graph>": [
        "At this point graph is finalized and you can not add ops."
    ],
    "<upper part>": [
        "The strictly upper part is assumed to be zero and not accessed."
    ],
    "<left multiplication>": [],
    "<predicted vector>": [],
    "<dimensional batch>": [],
    "<confidence value>": [],
    "<zero matrices, zero matrix>": [],
    "<CSC>": [],
    "<gradient flattening>": [],
    "<random shuffling>": [],
    "<success rate>": [],
    "<output column, output columns>": [
        "Output column name is the same as the input column name.",
        "Suppose the possible String values were {\"a\",\"b\",\"c\",\"d\"} and the String column value to be converted contained the String \"a,c\", then the 4 output columns would have values [\"true\",\"false\",\"true\",\"false\"]."
    ],
    "<ndims>": [
        "At least one of {shape, ndim} must be specified."
    ],
    "<shape one>": [],
    "<tensor rank>": [],
    "<cropping dimension>": [],
    "<input statistics>": [],
    "<Multiple GPU, multiple GPU>": [
        "In a workstation with multiple GPU cards, each GPU will have similar speed and contain enough memory to run an entire CIFAR-10 model."
    ],
    "<data moved, data movement>": [],
    "<point index, point indexes>": [],
    "<operand shape>": [],
    "<example data>": [],
    "<Weight Decay, Weight decay, weight decay>": [
        "weight decay should not be used when learning \\(a\\) for good performance.",
        "note:: weight decay should not be used when learning :math:`a` for good performance."
    ],
    "<sequence dimension>": [],
    "<dataflow graph>": [
        "The dataflow graph is a language-independent representation of the code in your model."
    ],
    "<dimension value>": [],
    "<Image Compression, image compression>": [],
    "<possible number>": [],
    "<Regression Examples>": [],
    "<Random sampling, random sample, random sampling>": [
        "This results in approximately equal partitions, though random sampling issues can Balanced: a custom repartitioning strategy that attempts to ensure that each partition ends up with the correct number of elements."
    ],
    "<mel>": [],
    "<RNN cell>": [
        "A fused RNN cell represents the entire RNN expanded over the time dimension.",
        "RNN cell composed sequentially of multiple simple cells.",
        "An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs."
    ],
    "<output activation, output activations>": [
        "Consequently, the output activations size is equal to the input size.",
        "In the CONCAT case, the output activations size (dimension 1) is 2x larger than the standard RNN's activations array.",
        "In all cases except CONCAT, the output activations size is the same size as the standard RNN that is being wrapped by this layer.",
        "- Per output masking: Where each output activation value is present or not - mask shape [n,c,h,w] (same as output)."
    ],
    "<resolution model>": [],
    "<queue element>": [
        "Each queue element is a tuple of one or more tensors, where each tuple component has a static dtype, and may have a static shape."
    ],
    "<sequence expansion>": [],
    "<outlining number>": [],
    "<Models built, model building>": [
        "Models built and ready to go!"
    ],
    "<Load multiple, load multiple>": [],
    "<lltm>": [],
    "<initial entry>": [],
    "<individual layer>": [],
    "<output neurons>": [],
    "<Theano, theano>": [],
    "<model longer>": [],
    "<Multiplies matrix, Multiply matrix, multiplied matrices>": [
        "Multiplies matrix a by matrix b, producing a * b."
    ],
    "<primal loss>": [],
    "<main graph>": [],
    "<classifier score>": [],
    "<Layer name, layer name>": [
        "Layer name assigns layer string name."
    ],
    "<result Tensor, result tensor, resulting Tensor, resulting tensor>": [
        "The resulting tensor will have values sampled from \\(\\mathcal{N}(0, \\text{std})\\) where.",
        "The resulting tensor will have values sampled from \\(\\mathcal{U}(-a, a)\\) where.",
        "If \"shape\" is None, the resulting tensor proto represents the numpy array precisely.",
        "Since inputs are dense, the resulting tensor will be a concatenated 1D buffer.",
        "In Python, this scatter operation would look like this: The resulting tensor would look like this: We can also, insert entire slices of a higher rank tensor all at once.",
        "The resulting tensor will have values sampled from \\(\\mathcal{U}(-\\text{bound}, \\text{bound})\\) where.",
        "The resulting tensor will have values sampled from :math:`\\mathcal{U}(-a, a)` where ..",
        "The resulting tensor will have values sampled from :math:`\\mathcal{N}(0, \\text{std})` where ..",
        "The resulting tensor will have values sampled from :math:`\\mathcal{U}(-\\text{bound}, \\text{bound})` where .."
    ],
    "<residual network>": [],
    "<pruned structure>": [],
    "<Input features, input feature, input features>": [
        "Input features and labels are split for consumption by each tower.",
        "The input features and labels get sharded into the chunks that correspond to the number of GPUs."
    ],
    "<ROC Curve, ROC curve, ROC curves, roc curve>": [
        "The ROC curves are produced by treating the predictions as a set of one-vs-all classifiers, and then calculating Utilizes trapezoidal integration internally."
    ],
    "<latter behavior>": [
        "This latter behavior will soon be deprecated.",
        "The latter behavior will soon be deprecated."
    ],
    "<assignment operation>": [],
    "<tower context>": [
        "Cross-tower context: tower context is when we are in some function that is being called once for each tower."
    ],
    "<large graphs>": [],
    "<training binaries, training binary>": [],
    "<zoo model>": [],
    "<end product>": [],
    "<coeffs>": [],
    "<innermost matrix>": [],
    "<name tensor>": [],
    "<output sides>": [],
    "<embedding tensors>": [],
    "<tensor reduction>": [],
    "<distribution learned>": [],
    "<optimal value>": [],
    "<Sequential Model, Sequential model, Sequential models, sequential model, sequential models>": [
        "Model Requirements: - Model must be a sequential model or functional model.",
        "Sequential models execute a list of modules/functions in order (sequentially)."
    ],
    "<output type>": [
        "The op accepts, for example, input types (float, double, float) and in that case the output type would also be (float, double, float).",
        "If the output type was qint8 ([-128, 127]), the operation will additionally subtract each value by 128 prior to casting, so that the range of values aligns with the range of qint8."
    ],
    "<destination graph>": [
        "This handler transform a tensor into itself if the source and destination graph are the same."
    ],
    "<initial bias>": [],
    "<input sentence, input sentences>": [
        "Remember that the input sentences were heavily filtered.",
        "The input sentence is evaluated Computation Graph: Forward input through encoder model."
    ],
    "<upper argument>": [],
    "<Bengio>": [],
    "<Print gradients>": [],
    "<randomized transform>": [],
    "<single words>": [],
    "<training updates>": [],
    "<classification difference>": [],
    "<frequency value, frequent values>": [],
    "<Discriminator Loss, discriminator loss>": [],
    "<generator loss>": [],
    "<parameter devices>": [
        "When using parameter servers (see above), the set of devices holding variables may be different, otherwise the parameter devices might match the worker devices."
    ],
    "<correct format>": [],
    "<variable creator>": [],
    "<single tutorials>": [
        "Yet Not a single tutorials works as expected : 1."
    ],
    "<concat>": [],
    "<full window>": [],
    "<distributed manner>": [],
    "<Dirichlet>": [],
    "<Parameter Ratio, parameter ratio>": [],
    "<spatial inputs>": [],
    "<overall variance>": [],
    "<parent graph>": [],
    "<solution matrix>": [],
    "<fpr>": [],
    "<cluster indices>": [],
    "<triangular part>": [],
    "<variable regularization>": [],
    "<transformation data>": [],
    "<regularization penalty>": [],
    "<scalar denominator>": [],
    "<wave samples>": [],
    "<NWC>": [],
    "<Tensor numerator>": [],
    "<wavenet function>": [],
    "<Vision functions>": [],
    "<optional conditioning>": [],
    "<main group>": [],
    "<eig>": [],
    "<ndf>": [],
    "<local graph>": [],
    "<Ftrl>": [],
    "<generic regression>": [],
    "<free functions>": [],
    "<learning behavior>": [
        "This would make the learning behavior erratic, slow down the learning, and may not even lead to a usable result."
    ],
    "<training specification>": [],
    "<global ratio>": [],
    "<Regressor, regressor, regressors>": [
        "This regressor ignores feature values and will learn to predict the average value of each label."
    ],
    "<probabilistic interpretation>": [],
    "<threshold decay>": [],
    "<Relative tolerance, relative tolerance>": [],
    "<labels argument, labels arguments>": [],
    "<L2, l2>": [
        "This differs from L2 above in that the L2 above is a stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.",
        "L1 and L2 regularization is applied by default on the weight parameters only.",
        "L2 regularization has a greater effect in the tails of the distribution eliminating extreme weights.",
        "L2Vertex calculates the L2 least squares error of two inputs."
    ],
    "<distributed performance>": [],
    "<logarithm function>": [],
    "<Bernoulli trial, Bernoulli trials, bernoulli trial>": [
        "A bernoulli trial is a mechanism for detecting the probability of a given event occurring k times in n independent trials."
    ],
    "<error batch>": [],
    "<Truncated Normal Distribution, truncated normal distribution>": [],
    "<analysis sample>": [],
    "<gate weights>": [],
    "<KL Divergence, KL divergence>": [
        "KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.",
        "The KL divergence, found in the variational autoencoder loss, is an example."
    ],
    "<NLP, natural language processing>": [
        "For Japanese, NLP tools like Kuromoji are useful.",
        "While words in all languages may be converted into vectors with Word2vec, and those vectors learned with Deeplearning4j, NLP preprocessing can be very language specific, and requires tools beyond our libraries.",
        "The Stanford Natural Language Processing Group has a number of Java-based tools for tokenization, part-of-speech tagging and named-entity recognition for languages such as Mandarin Chinese, Arabic, French, German and Spanish.",
        "However, natural language processing systems traditionally treat words as discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143."
    ],
    "<matplotlib>": [],
    "<channel representation>": [],
    "<dmax>": [
        "Missing rmax, rmin, dmax are set to inf, 0, inf, respectively."
    ],
    "<Beta1, beta1>": [
        "Momentum decay (beta1) is also applied to the entire momentum accumulator."
    ],
    "<equivalent location>": [],
    "<normalization statistics>": [],
    "<quad elements>": [],
    "<smallest power>": [],
    "<worker machine>": [],
    "<data history>": [],
    "<unconstrainted shape>": [],
    "<stabler function>": [],
    "<trained forever>": [],
    "<right device>": [],
    "<one forward>": [],
    "<triangular elements>": [],
    "<Source rank>": [
        "src (int): Source rank."
    ],
    "<Single training, single training>": [],
    "<matrix determinant>": [
        "If M = N, determinants and solves are done using the matrix determinant lemma and Woodbury identities, and thus require L and D to be non-singular."
    ],
    "<AFAIK>": [],
    "<Model Parameters, model parameter, model parameters>": [
        "The model parameters are updated with the gradients averaged across all model replicas.",
        "The model parameters are learned through the model training process described later."
    ],
    "<open node>": [],
    "<Xcode>": [],
    "<autodiff>": [],
    "<Cropping2D>": [],
    "<CPU host>": [],
    "<training accuracy>": [
        "If the training accuracy increases but the validation doesn't, that's a sign that overfitting is occurring, and your model is only learning things about the training clips, not broader patterns that generalize."
    ],
    "<LSTM cell>": [],
    "<NCW>": [],
    "<dense input>": [],
    "<come vector>": [
        "Creates an out come vector from the specified inputs."
    ],
    "<Binary Classifiers, binary classifier, binary classifiers>": [],
    "<total power>": [],
    "<learnable bias>": [],
    "<stride integer>": [],
    "<dense vector, dense vectors>": [],
    "<single matrix>": [],
    "<horizontal array>": [],
    "<Momentum value, momentum value>": [],
    "<dense type>": [],
    "<increasing elements>": [],
    "<output spatial>": [],
    "<trained weights>": [],
    "<bucket result>": [],
    "<model predicts>": [
        "This means that the model predicts\u2014with 95% probability\u2014that an unlabeled example flower is an Iris versicolor."
    ],
    "<unzipped data>": [],
    "<backward cell>": [],
    "<DFT, Discrete Fourier Transform, discrete Fourier transform>": [],
    "<sign operation>": [],
    "<training feature, training features>": [],
    "<batch operation>": [],
    "<CNN3D>": [],
    "<acquires shape>": [],
    "<Hadamard>": [],
    "<analytical gradients>": [],
    "<univariate distributions>": [],
    "<output predictions>": [],
    "<invocation probability>": [],
    "<LFW>": [
        "Loads LFW faces data transform.",
        "Thus, LFW images are scaled to 28 pixels x 28 pixels."
    ],
    "<word similarity>": [
        "We include an example in the NLP section since word similarity visualization is a common use."
    ],
    "<float types, floating type>": [],
    "<single way>": [],
    "<fake data>": [],
    "<weights trained>": [],
    "<Shape manipulation, shape manipulations>": [],
    "<tfe>": [],
    "<real person>": [],
    "<rate parameter, rate parameters>": [
        "If the rate parameter is greater than one, it performs convolution with holes, sampling the input values every rate pixels in the height and width dimensions.",
        "If the rate parameter is equal to one, it performs regular 2-D convolution."
    ],
    "<input scalar>": [],
    "<batched sequences>": [],
    "<CPU backend>": [
        "CPU backend supports multiple CPU ISAs.",
        "CPU backend it might be ignored, depending on Aggregate."
    ],
    "<small deviation>": [],
    "<Transfer Learning, Transfer learning, transfer learning>": [
        "The transfer learning API can be used to modify the architecture or the learning parameters of an existing multilayernetwork or computation graph.",
        "EXERCISE: Transfer learning is the idea that, if you know how to solve a task well, you should be able to transfer some of that understanding to solving related problems."
    ],
    "<Export classifier>": [],
    "<flat array>": [],
    "<standard task>": [],
    "<device path>": [],
    "<comp graph>": [],
    "<classification task, classification tasks>": [],
    "<Tensor arrays, tensor arrays>": [],
    "<UGRNN>": [],
    "<Toe state>": [
        "This method checks, if Toe state can be entered."
    ],
    "<inverse gamma>": [],
    "<static parameter>": [],
    "<Epsilon value, epsilon value, epsilon values>": [
        "Normalizes, scales and shifts: The epsilon value, usually a small number, is added to avoid divide-by-zero errors."
    ],
    "<Operation copy>": [],
    "<morphological features>": [],
    "<task number>": [],
    "<transform process>": [
        "A transform process requires a Schema to successfully transform data.",
        "Your transform process will fail to compile without it."
    ],
    "<visualization page>": [],
    "<mat1>": [
        "If mat1 is a \\((n \\times m)\\) tensor, mat2 is a Note."
    ],
    "<Feature matrix, feature matrix>": [],
    "<bid value>": [
        "The optional bid value for this cluster's spot instances Uses the on-demand market if empty."
    ],
    "<either Tensor>": [],
    "<overlapping parameter>": [],
    "<multiple topics>": [],
    "<multi word>": [],
    "<Dequeue, dequeue>": [],
    "<sole metric>": [],
    "<next cell>": [],
    "<manual call>": [],
    "<node1>": [],
    "<text models>": [],
    "<low rank, lower rank>": [],
    "<Spatial convolution, spatial convolution>": [],
    "<lower triangular>": [
        "The lower triangular part of the matrix is defined as the elements on and below the diagonal."
    ],
    "<cooccurrence>": [
        "So, you'll have to spend WAY more time building model + cooccurrence statistics will be shifted due to the absense of sentence boundaries."
    ],
    "<synchronous training>": [],
    "<innermost dimension>": [
        "The innermost dimension of indices (with length K) corresponds to indices into elements (if K = P) or slices (if K < P) along the Kth dimension of ref."
    ],
    "<left value>": [],
    "<Contrastive Training>": [],
    "<SVDF>": [],
    "<Critic Algorithm>": [],
    "<Tensor computing>": [],
    "<model2>": [],
    "<hasher name>": [],
    "<pooling type>": [],
    "<permutation matrix>": [],
    "<Merge layer>": [],
    "<unigram>": [],
    "<LARS>": [
        "Note, LARS scaling is currently only enabled for dense tensors."
    ],
    "<categorical representation>": [],
    "<attention outputs>": [],
    "<compute inputs>": [],
    "<low probabilities>": [],
    "<Quantized Distribution>": [],
    "<data access>": [],
    "<redundant shapes>": [],
    "<optional weight>": [],
    "<SIRDS>": [],
    "<MDP, mdp>": [],
    "<scale posterior>": [],
    "<Sample shape>": [
        "- Sample shape describes independent, identically distributed draws of batches from the distribution family."
    ],
    "<attention cell>": [],
    "<im2col>": [],
    "<glove model>": [],
    "<IOT, IoT>": [],
    "<complex processing>": [],
    "<paper Learning>": [],
    "<output filters>": [],
    "<kernel matrix>": [],
    "<data version, data versions>": [],
    "<good approximation>": [],
    "<Data normalization, data normalization>": [],
    "<loss shape>": [],
    "<running mean>": [],
    "<Recall rate>": [],
    "<Mixture distribution, mixture distribution>": [],
    "<initial gradient, initial gradients>": [],
    "<equivalent function>": [],
    "<Constrained Optimization, constrained optimization>": [],
    "<learning andreiliphd>": [],
    "<Remap, remap, remapped>": [],
    "<time improvement>": [],
    "<median value>": [],
    "<Input points, input points>": [],
    "<calculated mean>": [],
    "<Preprocess, Preprocessed, Preprocessing, preprocess, preprocessed, preprocessing, preprocessors>": [
        "This all occurs before the preprocessor does its magic.",
        "Allows the preprocessor to be set.",
        "However, the preprocessors can be added manually (overriding the automatic addition of preprocessors, for each layer).",
        "That is, a preprocessor is attached to a layer, and performs some operation on the input, before passing the layer to the output.",
        "The preprocessors in the next section should usually be preferred.",
        "Because our header code and source code is surrounded by macro definitions for all the types as seen above, after the preprocessor has run, the resulting code is what we would expect.",
        "This preprocessor is synchronized, thus thread-safe.",
        "Note that this transition/preprocessor only makes sense if the activations are actually CNN activations, but have been 'flattened' to a row vector.",
        "Gets rid of endings: PLEASE NOTE: This preprocessor is thread-safe by using synchronized method."
    ],
    "<parameter gradients>": [
        "Calculate parameter gradients and input activation gradients given the input and labels."
    ],
    "<steepest ascent>": [],
    "<maximum popularity>": [],
    "<Input samples, input samples>": [
        "The input samples are processed batch by batch.",
        "This method defines, how many input samples can SEQUENTIAL: Input will be sent to last-used worker unmodified.",
        "This method defines, how many input samples can be batched within given time frame."
    ],
    "<activation value, activation values>": [],
    "<advanced operations>": [],
    "<many Tensor, many tensor>": [],
    "<FID, fid>": [],
    "<density function, df>": [
        "df must contain only positive values."
    ],
    "<based checkpoint>": [],
    "<connected tensor>": [
        "The tensors (input or output) can be of two kinds: connected: a connected tensor connects to at least one operation contained in the subgraph."
    ],
    "<averaging training>": [],
    "<adjacent pooling>": [],
    "<tensor element, tensor elements>": [],
    "<Autoregressive Flow, autoregressive flow>": [],
    "<Native operation, native operation>": [],
    "<re2>": [],
    "<Weight updates, weight update, weight updates>": [],
    "<dynamic calculation>": [],
    "<scalar pad>": [],
    "<input loss>": [],
    "<Load tensor>": [],
    "<backward correlation>": [],
    "<dimensional Normal>": [],
    "<encoding vector>": [
        "Compared to the dozens of characters that might exist in a language, there are many many more words, so the encoding vector is much larger."
    ],
    "<one operating>": [],
    "<textual order>": [],
    "<repeat factor>": [],
    "<state matrix>": [],
    "<model operates>": [],
    "<Simple point>": [],
    "<pipeline optimization>": [],
    "<right node>": [],
    "<fault tolerance, fault tolerant>": [],
    "<Vectorized, vec, vectorized, vectorizer>": [
        "If mat is a \\((n \\times m)\\) tensor, vec is a 1-D tensor of size m, then tensor must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n.",
        "If mat is a \\((n \\times m)\\) tensor, vec is a 1-D tensor of Note.",
        "A Vectorizer at its essence takes an input source and converts it to a matrix for neural network consumption."
    ],
    "<Hochreiter>": [],
    "<scalar loss>": [],
    "<minimum operation>": [],
    "<max index>": [],
    "<Instance Normalization, instance normalization>": [
        "Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and This layer uses statistics computed from input data in both training and evaluation modes."
    ],
    "<apply activation>": [
        "Activation Layer Used to apply activation on input and corresponding derivative on epsilon."
    ],
    "<multi core>": [],
    "<half line>": [],
    "<Multiple outputs, multiple outputs>": [],
    "<Model Function, Model function, mode function, model function>": [
        "The model function gets invoked whenever someone calls the Estimator's train, evaluate, or predict methods.",
        "Your model function must provide code to handle all three of the mode values."
    ],
    "<gamma Tensor>": [],
    "<scalar shape>": [],
    "<Jacobian, jacobian>": [],
    "<Exponential linear, exponential linear>": [],
    "<monitor training>": [],
    "<Modern Normalizing>": [],
    "<Exponential moving, exponential moving>": [],
    "<real word, real words>": [],
    "<dumped tensor>": [],
    "<frequency spectrum>": [],
    "<Collapse operation>": [],
    "<clipped Tensor>": [],
    "<overall data>": [],
    "<weird error>": [],
    "<discrete distribution>": [],
    "<evaluation stages>": [
        "Both training and evaluation stages need to calculate the model's loss."
    ],
    "<true best>": [],
    "<sentence length, sentence lengths>": [],
    "<placeholder operation>": [],
    "<keypoint>": [],
    "<Fast Gradient>": [],
    "<feature group>": [],
    "<exponential decay>": [],
    "<conceptual level>": [],
    "<unit interval>": [],
    "<correct weights>": [],
    "<gradient equal>": [],
    "<NCCL, nccl>": [
        "Checks if NCCL is available.",
        "NCCL is an NVIDIA\u00ae library that can efficiently broadcast and aggregate data across different GPUs.",
        "By default, both NCCL and Gloo backends will try to find the network interface to use for communication.",
        "Our results show that for 8-GPUs, NCCL often leads to better performance.",
        "Note that a fourth backend, NCCL, has been added since the creation of this tutorial.",
        "NCCL has also provided a number of environment variables for fine-tuning purposes.",
        "In this tutorial, since we are using several multi-gpu nodes, NCCL is suggested.",
        "Although NCCL can transfer data faster, it takes one SM away, and adds more pressure to the underlying L2 cache.",
        "nccl backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed training."
    ],
    "<Accuracy Tensor>": [],
    "<model learning, model learns>": [
        "Training is the stage of machine learning when the model is gradually optimized, or the model learns the dataset.",
        "The model learns to associate images and labels.",
        "That is, the model learns the best way to map your input numeric categorical values to the embeddings vector value in order to solve your problem."
    ],
    "<numerical properties>": [],
    "<Transform diagonal>": [],
    "<Attention Decoder, attention decoder>": [],
    "<filter dimensions>": [],
    "<Value storage>": [],
    "<failure rate>": [],
    "<configured probability>": [],
    "<Contrib, contrib>": [
        "See the guide: Graph Editor (contrib) > Module: reroute."
    ],
    "<evaluation result, evaluation results>": [],
    "<training version>": [],
    "<computed Variable>": [],
    "<irregular intervals>": [],
    "<validation images>": [],
    "<entire path>": [],
    "<eigen value, eigen values>": [],
    "<drop control>": [],
    "<real label, real labels>": [],
    "<processing speed>": [],
    "<matrix exponential>": [],
    "<recent checkpoint>": [
        "If None (the default), the most recent checkpoint found within the model directory is chosen."
    ],
    "<uniform images>": [],
    "<diagonal term>": [
        "When None no diagonal term is added to scale."
    ],
    "<one job>": [],
    "<single person>": [],
    "<sequence one>": [],
    "<image batch>": [],
    "<Scalar float, scalar float>": [],
    "<sliding local>": [],
    "<Machine Translation, machine translation>": [],
    "<RDMA>": [],
    "<column1>": [],
    "<infinity norm>": [],
    "<computation replicas>": [],
    "<decay exponentially>": [],
    "<human brain>": [],
    "<Compute log, Computing log, compute log>": [],
    "<TPU evaluation>": [
        "TPU evaluation only works on a single host (one TPU worker) except BROADCAST mode."
    ],
    "<PDE>": [],
    "<pure function>": [],
    "<connection structure>": [],
    "<small sample>": [],
    "<Softsign, softsign>": [],
    "<opposite similarity>": [],
    "<multi nodes>": [],
    "<Contiguous inputs>": [
        "Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs."
    ],
    "<Inception graph>": [],
    "<Py3>": [],
    "<distributed one>": [],
    "<einsum>": [],
    "<correlation computation>": [],
    "<pseudo function>": [],
    "<GPU slow>": [],
    "<Spark cluster>": [
        "We're using netmasks for cases when Spark cluster is run on top of hadoop, or any other environment which doesn't assume Spark IP addresses announced.",
        "However, if the Spark cluster is configured such that one or more of the workers cannot access the internet (or specifically, the NTP server), all retries can fail."
    ],
    "<BFGS>": [],
    "<contracted dimensions>": [
        "The contracted dimensions of lhs and rhs must be of the same size."
    ],
    "<shape format>": [],
    "<Linear layer, Linear layers, linear layer, linear layers>": [],
    "<subsequent loss>": [],
    "<Jang>": [],
    "<log directory>": [],
    "<concatenated Tensor>": [],
    "<compound conditions>": [],
    "<exact process>": [
        "The exact process is beyond the scope of this guide."
    ],
    "<minibatch training>": [],
    "<State dropout>": [
        "State dropout is performed on the outgoing states of the cell."
    ],
    "<significant value>": [],
    "<VGG16, vgg16>": [],
    "<shrinking variance>": [],
    "<input steps>": [],
    "<greater length>": [],
    "<Generates values>": [],
    "<life graph>": [],
    "<model forward>": [],
    "<scalable models>": [],
    "<example tensor>": [],
    "<groups layers>": [],
    "<random selection>": [],
    "<recall array>": [],
    "<Infiniband>": [],
    "<Gloo>": [
        "By default, both NCCL and Gloo backends will try to find the network interface to use for communication.",
        "(Note that Gloo currently runs slower than NCCL for GPUs.).",
        "Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don't change this setting.",
        "gloo is good, but it cannot support gather and scatter in GPU, that 's why I chose cuda-aware MPI."
    ],
    "<Numpy, numpy>": [
        "Numpy equivalent is tensor[mask].",
        "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations.",
        "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays.",
        "# Numpy uses type 'Object' when the int overflows long, but we don't # have a similar concept.",
        "Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients."
    ],
    "<series length>": [],
    "<computed index>": [],
    "<total binary>": [],
    "<matching length>": [],
    "<multitask learning>": [],
    "<training graph>": [
        "The complete training graph contains roughly 765 operations."
    ],
    "<Load graph, loading graph, loading graphs>": [],
    "<NCE>": [],
    "<old GPU>": [],
    "<Maximum sentence, maximum sentence>": [],
    "<batch variance>": [],
    "<graph vectors>": [],
    "<Normal distribution, Normal distributions, normal distribution, normal distributions>": [],
    "<Distributions shapes>": [],
    "<discretization error>": [],
    "<total system>": [],
    "<spatial dimension, spatial dimensions>": [
        "The spatial dimensions of the output tensor depend on the padding algorithm.",
        "Prior to division into blocks, the spatial dimensions of the input are optionally zero padded according to paddings.",
        "The spatial dimensions of this intermediate result are then optionally cropped according to crops to produce the output."
    ],
    "<Pooling Layer, Pooling Layers, Pooling layer, Pooling layers, pooling layer, pooling layers>": [
        "Global pooling layer can also handle mask arrays when dealing with variable length inputs.",
        "Pooling layer #2 takes conv2 as input, producing pool2 as output."
    ],
    "<Recht>": [],
    "<Dataflow, dataflow>": [
        "Dataflow is a common programming model for parallel computing."
    ],
    "<training input, training inputs>": [],
    "<model information>": [],
    "<labels dimension>": [],
    "<cumulative distribution>": [
        "The probability mass function (pmf) and cumulative distribution function (cdf) are."
    ],
    "<middle dimension>": [],
    "<reconstruction probabilities, reconstruction probability>": [
        "Thus, reconstruction probabilities can (and should, for efficiency) be calculated in a batched manner."
    ],
    "<sigmoid function>": [],
    "<vanishing gradients>": [],
    "<Optional tuple>": [],
    "<descending list>": [],
    "<sequential input>": [],
    "<output validation>": [],
    "<top path>": [],
    "<inversion process>": [
        "Hence, the inversion process can get ambiguous."
    ],
    "<level normalization>": [],
    "<data directories>": [],
    "<GridRNN>": [],
    "<single vector>": [
        "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence."
    ],
    "<allreduce algorithm>": [],
    "<probelm>": [],
    "<model export>": [],
    "<nearest neighbors>": [
        "Run a k nearest neighbors search on a NEW data point.",
        "To do so, you can select points in multiple After clicking on a point, its nearest neighbors are also selected."
    ],
    "<positive skew>": [
        "positive skew means positive values of Y become more likely, and negative values become less likely.",
        "positive skew means, for unimodal X centered at zero, the mode of Y is \"tilted\" to the right."
    ],
    "<Input activations, input activation, input activations>": [
        "Calculate parameter gradients and input activation gradients given the input and labels.",
        "Activation layer is a simple layer that applies the specified activation function to the input activations.",
        "Dropout - (Source) - Each input activation x is independently set to (0, with probability 1-p) or (x/p with probability p)."
    ],
    "<top queue>": [],
    "<graph element, graph elements>": [],
    "<counts steps>": [],
    "<flattened tensor>": [],
    "<output model>": [],
    "<NPE>": [],
    "<Model Work, model works>": [
        "If i dont use this snippet then the model works in multi-gpu.",
        "By keeping the validation set separate, you can ensure that the model works with data it's never seen before.",
        "Let's get rid of these two assumptions, so our model works with any 2d single channel image."
    ],
    "<finite difference>": [],
    "<random variation>": [],
    "<dummy tensors>": [],
    "<single column>": [
        "A column reduction defines how a single column should be reduced."
    ],
    "<Weight matrix, weight matrices, weight matrix, weights matrix>": [
        "The weight matrix is masked.",
        "weight matrix will be a sparse tensor."
    ],
    "<multiple machines>": [],
    "<Einstein sum, Einstein summation>": [],
    "<input planes>": [],
    "<Loss computation, loss computation>": [],
    "<unconstrained problem>": [],
    "<network weights>": [],
    "<similarity function>": [],
    "<TensorRT, Tensordot, tensordot>": [
        "The sizes in these dimensions must match, but tensordot will deal with broadcasted dimensions."
    ],
    "<blocking function>": [],
    "<Fake images, fake images>": [
        "Real and fake images must have the same size."
    ],
    "<based function>": [],
    "<Vector distributions>": [],
    "<Forward features>": [
        "Forward features to predictions dictionary."
    ],
    "<Cosine similarity, cosine similarity>": [
        "Cosine similarity Note that you need to initialize a scaling constant equal to the norm2 of the vector."
    ],
    "<rough guide>": [],
    "<Cloud Detection>": [],
    "<complex vector>": [],
    "<training speed>": [
        "After I trained this model for a few hours, the average training speed for epoch 10 was slow down to 40s.",
        "However, I noticed that the training speed gets slow down slowly at each batch and memory usage on GPU also increases.",
        "Currently, the memory usage would not increase but the training speed still gets slower batch-batch."
    ],
    "<log1pexp>": [],
    "<Gradient checking, gradient checking>": [
        "Check backprop gradients for a pretrain layer NOTE: gradient checking pretrain layers can be difficult...",
        "See the guide: Testing > Gradient checking."
    ],
    "<maximal values>": [],
    "<large tensor, large tensors, larger tensor>": [],
    "<features argument>": [],
    "<Deep Belief>": [],
    "<scale example>": [],
    "<one storage>": [],
    "<previous epoch>": [],
    "<initial hidden>": [],
    "<logical analogies>": [],
    "<multiple queue>": [],
    "<inner list>": [
        "The length of the outer list is the number of shards required, and each inner list is the tuple of Tensors to use to determine the types and shapes of the corresponding shard.",
        "Each inner list indicates the types and shapes of the tuples in the corresponding shard."
    ],
    "<Parallel arrays>": [],
    "<forward step>": [],
    "<image localization>": [],
    "<None shape>": [
        "A None entry in a shape is compatible with any dimension, a None shape is compatible with any shape."
    ],
    "<fixed order>": [
        "However, the order is guaranteed to be deterministic, so that variables created in conditional branches are created in fixed order across runs."
    ],
    "<Copy model>": [],
    "<symmetric matrix>": [],
    "<RBF>": [],
    "<time domain>": [],
    "<relative error>": [],
    "<device spec>": [],
    "<Multi part>": [],
    "<spatial shape>": [],
    "<action list>": [],
    "<single labels>": [],
    "<discrete value>": [],
    "<absolute mean>": [],
    "<local task>": [],
    "<parallel learning>": [],
    "<Pairwise, pairwise>": [],
    "<continuous function>": [],
    "<continuous signal>": [],
    "<AIS>": [
        "Marine Automatic Identification System (AIS) is an open system for marine broadcasting of positions.",
        "AIS data, coordinates can be reported at irregular intervals over time.",
        "Furthermore, AIS data for 1 year is over 100GB compressed."
    ],
    "<cloud storage>": [],
    "<gradient tensor, gradient tensors, gradients tensor>": [
        "Get the gradient tensors that this object is aware of."
    ],
    "<low values>": [
        "Too low values can lead to undesired removal of words from vocab."
    ],
    "<batch normalized>": [],
    "<delimited data>": [],
    "<spawn function>": [
        "The spawn function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them."
    ],
    "<singleton list>": [],
    "<Delving Deep, Delving deep>": [],
    "<initial shape>": [],
    "<slice value>": [],
    "<update storage>": [],
    "<tflite function>": [],
    "<beta function>": [],
    "<training job>": [],
    "<prediction algorithm>": [],
    "<data layouts>": [
        "This data layout is shown below."
    ],
    "<memcpy>": [],
    "<initial module>": [],
    "<column sweep>": [],
    "<garcon>": [],
    "<Numerical gradient, numerical gradient>": [],
    "<multiple edges>": [],
    "<Step function, step function, step functions>": [],
    "<automatic dependencies>": [],
    "<real usage>": [],
    "<SVHN>": [
        "Note: The SVHN dataset assigns the label 10 to the digit 0."
    ],
    "<padding algorithm>": [],
    "<model estimator>": [],
    "<Compute Engine>": [],
    "<mode model, model modes>": [],
    "<shape changed>": [],
    "<corrupted state>": [],
    "<Objective Functions, Objective function, objective function, objective functions>": [
        "The objective function is the function that your network is being trained to minimize (in which case it is often called a loss function or cost function).",
        "You'd like to record how the learning rate varies over time, and how the objective function is changing."
    ],
    "<word embedding>": [],
    "<execution graph>": [],
    "<Baroni>": [],
    "<quantized models>": [],
    "<Gamma Distribution, Gamma distribution, gamma distribution>": [
        "The Gamma distribution is defined over positive real numbers using parameters concentration (aka \"alpha\") and rate (aka \"beta\").",
        "Here's the code we'll use: Gamma Distribution."
    ],
    "<Discrete parameter>": [],
    "<logits function>": [],
    "<negative exists>": [],
    "<stopping signal>": [],
    "<inbound nodes>": [],
    "<Geometric distribution, Geometric distributions, geometric distribution>": [
        "The Geometric distribution is parameterized by p, the probability of a positive event."
    ],
    "<incomplete batch>": [],
    "<output fields>": [],
    "<learning capability>": [],
    "<training generally>": [],
    "<model copy>": [
        "Every model copy is executed on a dedicated GPU."
    ],
    "<model debugging>": [],
    "<frequency intensities>": [],
    "<Reduce learning, reduce learning>": [],
    "<random walk, random walks>": [],
    "<Network Learning, network learn, network learned, network learning, network learnt>": [
        "Seems like the network learnt something."
    ],
    "<multiplication operation>": [],
    "<Rescaled image>": [],
    "<particular cluster>": [],
    "<event shape>": [
        "For scalar distributions, the event shape is []."
    ],
    "<Shape tuple, shape tuple>": [
        "shape: Shape tuple, expected shape of the input (may include None for unchecked axes)."
    ],
    "<dependent values>": [],
    "<adaptive step, adpative step>": [],
    "<coefficients moving>": [],
    "<input split>": [
        "An input split that already has delimited data of some kind."
    ],
    "<Pooling ratio, pooling ratio>": [],
    "<probability density function>": [
        "The probability density function (pdf) is: Where scale = sigma is the standard deviation of the underlying normal distribution.",
        "The probability density function (pdf) is, Examples."
    ],
    "<entire matrix>": [
        "If less than or equal to 0, the entire matrix will be loaded into memory."
    ],
    "<generator steps>": [],
    "<release process>": [],
    "<Bool, bool, bools>": [
        "Python bool indicating possibly expensive checks are enabled.",
        "periodic: A bool Tensor indicating whether to generate a periodic or symmetric window."
    ],
    "<mean semantic>": [],
    "<reversible layers>": [],
    "<multi input>": [],
    "<original matrix>": [],
    "<TPU cluster>": [],
    "<Adadelta>": [],
    "<isnan>": [],
    "<Logistic Distribution, Logistic distribution, Logistic distributions, logistic distribution>": [],
    "<uplo>": [],
    "<selected directory>": [
        "If the selected directory does not exist, an attempt is made to create it."
    ],
    "<average score>": [],
    "<Colab, colab>": [],
    "<scalar distributions>": [],
    "<outer dimension>": [],
    "<broadcast shape, broadcasted shape>": [],
    "<Random normal, random normal>": [],
    "<total examples>": [],
    "<migration guide>": [],
    "<Restore graph>": [],
    "<evaluation call>": [],
    "<weight ordering>": [],
    "<op type>": [],
    "<remote graph>": [],
    "<dimensional data>": [],
    "<event distribution>": [],
    "<linear classifier>": [],
    "<squared connectivity>": [],
    "<corresponding shapes>": [],
    "<ts1>": [],
    "<network Input, network input>": [
        "Used for example when network input and output data comes from different files."
    ],
    "<column axis>": [],
    "<time dimension, time dimensions>": [
        "This time dimension must be the same across those tensors of an example."
    ],
    "<dump directory>": [],
    "<exact dimensions>": [],
    "<scalar strings>": [],
    "<ARFF>": [],
    "<structural similarity>": [],
    "<occurring values>": [],
    "<hidden unit, hidden units>": [],
    "<input tags>": [
        "The input tags and values must have the same shape."
    ],
    "<vector Tensor, vector tensor, vector tensors>": [
        "The vector tensor is added to the final result."
    ],
    "<Mean Squared, mean square, mean squared>": [],
    "<state Tensor, state tensors>": [],
    "<training convergence>": [],
    "<TODO>": [
        "TODO: handle axes arguments that alter merge behavior (requires changes to DL4J?).",
        "\"\"\" # TODO allow (loc,scale) parameterization to allow independent constraints."
    ],
    "<Global step, global step, global steps>": [
        "global step this summary is associated with.",
        "If None the global step is not recorded in summaries and checkpoint files.",
        "The global step tensor must be an integer variable.",
        "Verifies that a global step tensor is valid or gets one if None is given."
    ],
    "<raw tensor>": [],
    "<polynomial order>": [],
    "<deletion operation>": [],
    "<convolution rows>": [],
    "<RealNVP>": [],
    "<maximum parallelism>": [],
    "<partial inverse>": [],
    "<scale gradients>": [],
    "<Learning example>": [],
    "<gradient penalty>": [],
    "<random postfix>": [],
    "<Markov Chains>": [],
    "<pool2d>": [],
    "<outer operation>": [],
    "<scale argument, scale arguments>": [],
    "<positive weight, positive weights>": [],
    "<normalization issues>": [],
    "<steps condition>": [],
    "<flow Tensor>": [],
    "<model spec>": [],
    "<alloc>": [],
    "<execution engine>": [],
    "<average popularity>": [],
    "<arbitrary dimension, arbitrary dimensions>": [],
    "<serializers>": [
        "Any serializer that uses Kryo to read a child object may need to This method should not be called directly, instead this serializer can be passed to Kryo read methods that accept a serialier.",
        "Serializer used for converting objects (Transforms, Conditions, etc) to JSON format."
    ],
    "<DQN, dqn>": [
        "The DQN has several actions like translation and scaling."
    ],
    "<Weight Normalization, Weight normalization, weight normalization, weight norms>": [],
    "<dominant part>": [],
    "<Kingma>": [],
    "<frozen graph>": [],
    "<online computation>": [],
    "<standard services>": [],
    "<update history>": [],
    "<verification steps>": [],
    "<Cumulative product, cumulative product>": [],
    "<auc value>": [],
    "<moment correlation>": [],
    "<convolution algorithms>": [],
    "<partial list>": [
        "Note: A partial list of columns can be used here."
    ],
    "<large machine>": [],
    "<Negative Log, Negative log, negative log>": [],
    "<compute vector>": [],
    "<Matrix transpose, matrix transpose>": [
        "Matrix transpose operation: If input has shape [a,b] output has shape [b,a]."
    ],
    "<independent sample, independent samples>": [],
    "<Interrupt training>": [],
    "<exact loss>": [],
    "<global mean>": [],
    "<processing indices>": [],
    "<stddev, stdev>": [
        "stddev is the standard deviation of all values in image.",
        "stddev is typically determined via hyperparameter tuning."
    ],
    "<bias matrices>": [],
    "<decay value>": [],
    "<padding type>": [],
    "<WRT, wrt>": [],
    "<fetch list>": [],
    "<feature list>": [],
    "<taking images>": [],
    "<true body>": [],
    "<output vertex, output vertices>": [
        "If the vertex removed is an output vertex it will also be removed from the list of outputs."
    ],
    "<black image>": [],
    "<Accurate Computation>": [],
    "<real batch>": [],
    "<validation input>": [],
    "<simulated inputs>": [],
    "<overall batch>": [],
    "<Word2vec, word2vec>": [
        "While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand.",
        "Word2vec is a method of computing vector representations of words introduced by a team of researchers at Google led by Tomas Mikolov.",
        "Given enough data, usage and contexts, Word2vec can make highly accurate guesses about a word's meaning based on past appearances.",
        "Word2vec is a two-layer neural net that processes text.",
        "Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words.",
        "As Elvis Costello said: \"Writing about music is like dancing about architecture.\" Word2vec \"vectorizes\" about words, and by doing so it makes natural language computer-readable \u2013 we can start to perform powerful mathematical operations on words to detect their similarities.",
        "Word2Vec can output text windows that comprise training examples for input into neural nets, as seen here.",
        "Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction word2vec trains words against other words that neighbor them in the input corpus.",
        "Word2vec needs to be fed words rather than whole sentences, so the next step is to tokenize the data.",
        "In that case, Word2vec would attempt a full skip-gram cycle for the whole 10,000-word \"sentence\"."
    ],
    "<upper incomplete>": [],
    "<count samples>": [],
    "<Connectionist Temporal Classification>": [],
    "<scalar division>": [],
    "<GPU case>": [],
    "<model repeatedly>": [],
    "<reduce number>": [],
    "<continue training, continuing training>": [],
    "<Training parameters, training parameters>": [],
    "<maxpool2d>": [],
    "<receiving process>": [],
    "<adjusting models>": [],
    "<subprojects>": [],
    "<actual batch>": [],
    "<leakyrelu>": [],
    "<tensor list>": [
        "Also, each tensor in the tensor list needs to reside on a different GPU.",
        "Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called.",
        "Append to lists in loops (tensor list ops are automatically created): Nested control flow."
    ],
    "<final accuracy>": [],
    "<early termination>": [],
    "<prerelease>": [],
    "<running predict>": [],
    "<input projection>": [],
    "<mathematical components>": [],
    "<powerful models>": [],
    "<online training>": [],
    "<Distribution parameter, Distribution parameters, distribution parameter, distribution parameters>": [
        "Generates n samples or n batches of samples if the distribution parameters are batched.",
        "When True distribution parameters are checked for validity despite possibly degrading runtime performance.",
        "Distribution parameters are automatically broadcast in all functions; see examples for details."
    ],
    "<Pretraining, pretraining>": [],
    "<input spatial>": [],
    "<building data>": [],
    "<variable map>": [],
    "<weight files>": [],
    "<TBPTT>": [],
    "<Tensor format>": [],
    "<CMU>": [],
    "<avoids name>": [],
    "<Means clustering>": [],
    "<Data pipeline, data pipeline, data pipelines>": [
        "The data pipeline for image classification can be constructed as follows: And that's it.",
        "That is, both the data pipeline and the matrix manipulations determine how long a neural network takes to train on a dataset."
    ],
    "<Hermite>": [],
    "<grid point>": [],
    "<labels input>": [],
    "<batch sample>": [],
    "<handling training>": [],
    "<Language Modeling, Language Models, Language modeling, language modeling, language models>": [
        "Language modeling is key to many interesting problems such as speech recognition, machine translation, or image captioning.",
        "My model is not a language model but it deals with many-to-many interaction.",
        "Our language model might do OK on this sentence, but wouldn't it be much We have seen mathematician and physicist in the same role in a sentence."
    ],
    "<Parameter Averaging, parameter averaging>": [],
    "<absolute positions>": [],
    "<Nal>": [
        "The default is based on: Nal Kalchbrenner, Ivo Danihelka and Alex Graves \"Grid Long Short-Term Memory,\" Proc."
    ],
    "<Zoph>": [],
    "<lower boundary>": [],
    "<Image Recognition, image recognition>": [],
    "<loop variable, loop variables>": [],
    "<invalid shape>": [],
    "<negative factor>": [],
    "<averaged parameters, averaging parameters>": [
        "Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.",
        "This EXERCISE: Employing averaged parameters may boost predictive performance by about 3% as measured by precision @ 1."
    ],
    "<validation loss, vl>": [
        "On the other hand, the validation loss will be identical whether we shuffle the validation set or not."
    ],
    "<filtered sequence>": [],
    "<field number>": [],
    "<norm degree>": [],
    "<natural log>": [],
    "<Evaluated values>": [],
    "<error points>": [],
    "<dense output>": [],
    "<dense representation>": [],
    "<index element>": [
        "Thus, If two elements are equal, the lower-index element appears first."
    ],
    "<linguistic data>": [],
    "<predictions dictionary>": [],
    "<full loss>": [],
    "<signature function>": [],
    "<Padded image, padded image>": [],
    "<filepath>": [],
    "<checkpointing>": [
        "By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes."
    ],
    "<maximum factor>": [],
    "<broadcast step>": [
        "While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes."
    ],
    "<pre computed>": [],
    "<linear region>": [],
    "<loss summaries>": [],
    "<FP32>": [],
    "<dimensional locations>": [],
    "<units parameter>": [
        "Here's the relevant code: The units parameter defines the number of output neurons in a given layer.",
        "Therefore, the full set of layers A logit output layer connected to the top hidden layer When defining an output layer, the units parameter specifies the number of outputs."
    ],
    "<INT16, int16>": [],
    "<overall model>": [
        "The best overall model is then taken to be the final model."
    ],
    "<Softmax2d>": [],
    "<discriminator model>": [],
    "<Chi2>": [],
    "<loss problem>": [
        "The problem I am facing right now is an exploding loss problem.",
        "However, the exploding loss problem still cannot be alleviated."
    ],
    "<hot labels>": [],
    "<one site>": [],
    "<output structure>": [
        "If no initializer is provided, the output structure and dtypes of fn are assumed to be the same as its input; and in this case, the first argument of fn must match the structure of elems."
    ],
    "<linear network>": [
        "Also they mention why a linear network won't be able to learn the representation."
    ],
    "<length sequences>": [],
    "<averaged false>": [],
    "<named job>": [],
    "<actual label>": [],
    "<cell state>": [],
    "<UINT8, uint8>": [],
    "<reduced values>": [],
    "<single box>": [],
    "<Tensorflow, tensorflow>": [
        "My guess is that tensorflow may not cache the intermediate feature maps in the graphdef mode, but pytorch may do.",
        "Once setup is completed, Tensorflow can interact with S3 in a variety of ways.",
        "sets module: Tensorflow set operations.",
        "You can specify a continuous feature like so: Although, as a single real number, a continuous feature can often be input directly into the model, Tensorflow offers useful transformations for this sort of column as well."
    ],
    "<average AUC>": [],
    "<dense elements>": [],
    "<L1, l1>": [
        "L1 and L2 regularization is applied by default on the weight parameters only.",
        "L1 regularization produces more exactly-zero values, in this case it sets ~200 to zero."
    ],
    "<oW, one word, ow>": [],
    "<training cycles>": [],
    "<hidden weights>": [],
    "<move tensors>": [],
    "<one gate>": [],
    "<complex conjugation>": [],
    "<Split data, split data>": [],
    "<chief worker>": [
        "Gradients are pushed to them and the chief worker will wait until enough gradients are collected and then average them before applying to variables."
    ],
    "<Bias tensor, bias tensor, bias tensors>": [
        "bias: Bias tensor to add."
    ],
    "<polynomial factors>": [],
    "<Optional constraint>": [],
    "<axis number>": [],
    "<Google model>": [
        "The Google model may take as much as 10G of RAM, and the JVM only launches with 256 MB of RAM, so you have to adjust your heap space."
    ],
    "<compute cluster>": [],
    "<Lambda layer, lambda layer>": [],
    "<dim value>": [],
    "<symmetric cropping>": [
        "- If int: the same symmetric cropping is applied to height and width."
    ],
    "<installation loop>": [],
    "<Conv3D, Conv3d, conv3d>": [],
    "<reconstructed data>": [],
    "<Keras version, keras version>": [],
    "<Recurrent dropout, recurrent dropout>": [],
    "<fmod>": [],
    "<Total variance, total variance>": [],
    "<CBOW>": [
        "This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic.",
        "This inversion CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation).",
        "Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model.",
        "CBOW doesn't involve any pretraining.",
        "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning."
    ],
    "<Sparse Tensor, Sparse tensors, sparse Tensors, sparse tensor, sparse tensors>": [
        "A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices.",
        "Important: This sparse tensor will produce an error if evaluated.",
        "Our sparse tensor format permits uncoalesced sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries.",
        "Sparse tensors use the default momentum optimizer."
    ],
    "<valid dimension>": [],
    "<Building model, building model>": [],
    "<point collectives>": [],
    "<evaluate model>": [],
    "<sample one>": [],
    "<URDF>": [],
    "<encoded image>": [],
    "<mixture model>": [
        "The mixture model is defined by a Categorical distribution (the mixture) and a python list of Distribution objects."
    ],
    "<output steps>": [],
    "<building conditions>": [],
    "<Training continues, training continues>": [
        "To ensure that training continues without diverging it is necessary that the restored node resumes training with a copy of the model identical to that on the other nodes at the current point."
    ],
    "<stop sign>": [],
    "<flat list>": [],
    "<Loss tensor, loss Tensor, loss tensor, loss tensors>": [],
    "<p2p>": [],
    "<opJ>": [],
    "<final parameters>": [],
    "<one contracting>": [],
    "<training session, training sessions>": [],
    "<gradient behavior>": [],
    "<linear scale>": [],
    "<visible distribution>": [],
    "<approximate distribution>": [],
    "<TPU topology>": [],
    "<semantic relation>": [],
    "<style loss>": [],
    "<differential function, differential functions, differentiated function>": [],
    "<Recurrent Layers, Recurrent layers, recurrent layer, recurrent layers>": [],
    "<layer index>": [],
    "<triggered step>": [],
    "<constructable>": [],
    "<Embeddings, embeddings>": [
        "Embeddings should be l2 normalized.",
        "Embeddings for the symbols generated from the decoder itself remain unchanged.",
        "Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model.",
        "Word embeddings are dense vectors of real numbers, one per word in your vocabulary.",
        "Embeddings are useful for a wide variety of prediction tasks in NLP.",
        "An embedding is a mapping from discrete objects, such as words, to vectors of real numbers.",
        "Embedding functions are the standard and effective way to transform such discrete input objects into useful continuous vectors.",
        "Embeddings are also valuable as outputs of machine learning.",
        "An embedding layer is a part of neural network, but an embedding is a more general concept.",
        "That is, embeddings are stored as a \\(|V| \\times D\\) matrix, where \\(D\\) is the dimensionality of the embeddings, such that the word assigned index \\(i\\) has its embedding stored in the \\(i\\)'th row of the matrix.",
        "Embeddings are always aggregated along the last dimension.",
        "In summary, word embeddings are a representation of the *semantics* of a word, efficiently encoding semantic information that might be relevant to the task at hand.",
        "Embeddings can be trained in many network types, and with various loss functions and data sets.",
        "The embedding of the input is passed into a fully connected layer which we then use as a softmax layer.",
        "Embeddings are important for input to machine learning."
    ],
    "<reduction index>": [],
    "<permutation results>": [],
    "<save model, save models, saved model>": [
        "If an optimizer was found as part of the saved model, the model is already compiled.",
        "Thus the saved model can be reinstantiated in the exact same state, without any of the code used for model definition or training.",
        "If you serialize a backward hook into a saved # model, and then you rename the function associated with the hook, # now your saved model is broken and you can't load it anymore.",
        "For our purposes, the saved model can be treated as a single binary blob."
    ],
    "<maximum feature>": [],
    "<Hidden output, hidden output>": [],
    "<deterministic order, deterministic ordering>": [],
    "<data retention>": [],
    "<transpose operation>": [],
    "<learning workflow>": [],
    "<prespecified number>": [
        "The prespecified number of workers (in this case 2) will then train its own model using its data."
    ],
    "<device systems>": [],
    "<frequency counting>": [],
    "<compute derivatives>": [],
    "<mean operation>": [
        "The only difference is that after pooling regions are generated, a mean operation is performed instead of a max operation in each pooling region."
    ],
    "<transform matrix>": [],
    "<Proximal Stochastic>": [],
    "<input models>": [],
    "<training pairs>": [],
    "<fully connected>": [],
    "<Scala, scala>": [],
    "<counting statistics>": [],
    "<TFT, TFTS>": [],
    "<square window>": [],
    "<one tick>": [],
    "<identity tensor>": [],
    "<GravesLSTM layer>": [],
    "<single float>": [],
    "<Cloud Partners>": [],
    "<art model>": [],
    "<small batch>": [],
    "<computing architecture>": [],
    "<particular dimension>": [],
    "<dense column, dense columns>": [],
    "<activation frequency>": [],
    "<PRED, pred, preds>": [],
    "<total variation>": [
        "The total variation is the sum of the absolute differences for neighboring pixel-values in the input images."
    ],
    "<data transform>": [],
    "<training performance>": [],
    "<nvtx>": [],
    "<IDCT>": [],
    "<binary tree>": [],
    "<space model, space models>": [
        "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other')."
    ],
    "<Concrete Distribution>": [],
    "<final gradient>": [],
    "<inner loop>": [],
    "<newb>": [],
    "<VCTK>": [],
    "<full cross>": [],
    "<accuracy statistics>": [],
    "<compute distribution>": [],
    "<dilation factor>": [
        "If the dilation factor in a spatial dimension is d, then d-1 holes are implicitly placed between each of the entries in that dimension, increasing the size of the array."
    ],
    "<dense behavior>": [],
    "<minibatch dimension>": [],
    "<matrix subtraction>": [],
    "<Non negative, non negative>": [],
    "<baseline value>": [],
    "<Estimator model, Estimator models>": [],
    "<Backup node>": [],
    "<ROC calculation>": [],
    "<Time step, time step, time steps>": [
        "As a result, the last time step is moved to the beginning of the time series and the first time step is moved to the end.",
        "Because the mortality label is always at the end of the sequence, we need all the sequences aligned so that the time step with the mortality label is the last time step for all patients.",
        "If this input does not have a mask array, the last time step of the input will be used for all examples; otherwise, the time step of the last non-zero entry in the mask array (for each example separately) will be used.",
        "Earlier time steps are either be trimmed or Given specified values; the last values in these columns will be truncated/removed.",
        "Each value in the Record is represented by Writable object; each time step is thus a List<Writable> and the entire sequence is represented by a List<List<Writable>>, where the outer list is over time steps, and the inner list is over values for a given time step.",
        "If passed None, then time step is set to be the t[1:] - t[:-1]."
    ],
    "<Matrix Multiplication, Matrix multiplication, matrix multiplication>": [
        "Loosely speaking, matrix multiplication is equal to the action of a Fourier multiplier: A u = IDFT3[ H DFT3[u] ]."
    ],
    "<one tower>": [],
    "<based dimensions>": [],
    "<hidden input, hidden inputs>": [],
    "<Average layer>": [],
    "<underlying distribution>": [],
    "<complex intrinsic>": [],
    "<padding value, padding values>": [
        "Same: Same mode operates differently to Strict/Truncate, in three key ways: (a) Manual padding values in convolution/subsampling layer configuration is not used; padding values are instead calculated automatically based on the input size, kernel size and strides."
    ],
    "<polygonal data>": [],
    "<decay factor>": [],
    "<model types>": [],
    "<inits>": [
        "Init has been internally called."
    ],
    "<zero mean>": [],
    "<parallel threads>": [],
    "<continuous training>": [],
    "<constructing data>": [],
    "<Linear indices, linear index, linear indexing, linear indices>": [
        "Linear indices are a single int64 value which indexes into the buffer holding the array."
    ],
    "<recurrent cell>": [],
    "<GPU context>": [],
    "<Distributed Data, distributed data>": [
        "By comparison, Distributed Data Parallel goes completely parallel among distributed processes."
    ],
    "<overridden shape>": [],
    "<train batches>": [],
    "<device capability>": [],
    "<input window>": [],
    "<linear prediction, linear predictions>": [],
    "<loss really>": [],
    "<conditional distribution>": [],
    "<Overfitting, overfit, overfitting>": [],
    "<Conv1D, Conv1d, conv1, conv1d>": [],
    "<Optimization Algorithm, Optimization algorithm, optimization algorithm, optimization algorithms>": [
        "The optimization algorithm is how updates are made, given the gradient."
    ],
    "<state map>": [],
    "<larger matrix>": [],
    "<distributed function, distributed functions>": [],
    "<round function>": [],
    "<Task Learning, task learning>": [],
    "<mirrored variable>": [],
    "<tensors condition>": [],
    "<proper data>": [],
    "<independent scaling>": [],
    "<hyper parameter, hyper parameters>": [],
    "<aggregate values>": [],
    "<device pair>": [],
    "<theoretical number>": [],
    "<Run Evaluation, run evaluation>": [],
    "<initial entropy>": [],
    "<training logs>": [
        "logdir: The directory where training logs are written to."
    ],
    "<adversarial loss>": [],
    "<Pytorch, pytorch>": [
        "Compiling pytorch from source will solve the problem for sure.",
        "Pytorch has many types of predefined layers that can greatly simplify our code, and often makes it faster too.",
        "Pytorch is a dynamic neural network kit.",
        "However, Pytorch will only use one GPU by default."
    ],
    "<Feature Mapping>": [],
    "<jth>": [],
    "<error text>": [],
    "<forward domain>": [],
    "<max scaling>": [],
    "<larger variable>": [],
    "<constant result>": [],
    "<linear term>": [],
    "<logical shape>": [],
    "<fully connected layer>": [
        "Finally, a fully connected layer is applied to the merged output, which passes the activations to the final output layer."
    ],
    "<feature information>": [],
    "<categorical value, categorical values>": [],
    "<main column>": [
        "One main column can have several sub columns."
    ],
    "<GPU training>": [
        "This utilty and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend."
    ],
    "<VDM>": [],
    "<output strategies, output strategy>": [],
    "<inference results>": [],
    "<learning rate decay>": [],
    "<moving mean>": [],
    "<loss score>": [
        "This error or loss score will eventually converge to a value close to zero."
    ],
    "<counter function>": [],
    "<validation score>": [],
    "<tad element>": [
        "Tad element wise stride: given the inner most dimension (the sorted dimension of the last) the element wise stride of the tad (disregarding order) is the last dimension's stride."
    ],
    "<replicated mode>": [],
    "<weight values>": [],
    "<one weight>": [
        "Therefore, the model now can learn four individual weights rather than just one; four weights creates a richer model than one weight."
    ],
    "<Stochastic Neighbor, stochastic neighbor>": [],
    "<releases shape>": [],
    "<generator queue>": [],
    "<learning parameters>": [],
    "<zero one>": [],
    "<Numeric Tensor, numeric Tensor, numeric tensor>": [],
    "<statistical models>": [],
    "<reconstructed images>": [],
    "<Absolute tolerance, absolute tolerance>": [],
    "<Reverse division>": [],
    "<generic data>": [],
    "<learning progress>": [],
    "<training round>": [],
    "<one rank>": [],
    "<point tree>": [],
    "<topic name>": [],
    "<insignificant amount>": [],
    "<Profile model>": [],
    "<update parameters>": [],
    "<Lagrange multipliers>": [],
    "<Grt>": [],
    "<master procedure>": [],
    "<multiple domains>": [],
    "<random matrix>": [],
    "<Model state, model state>": [],
    "<Probability coding>": [],
    "<truth sequences>": [],
    "<batch statistics>": [],
    "<Dense Layer, Dense layer, Dense layers, dense layer, dense layers>": [
        "The first Dense layer has 128 nodes (or neurons).",
        "Masks should not be applied in all cases, depending on the network configuration - for example input Dense -> RNN The first dense layer should be masked (using the input mask) whereas the second shouldn't be, as it has valid data coming from the RNN layer below.",
        "This allows different layers to handle masks differently - for example, bidirectional RNNs and normal RNNs operate differently with masks (the former sets activations to 0 outside of the data present region (and keeps the mask active for future layers like dense layers), whereas normal RNNs don't zero out the activations/errors )instead relying on backpropagated error arrays to handle the variable length case.",
        "The final dense layer in a CNN contains a single node for each target class in the model (all the possible classes the model may predict), with a softmax activation function to generate a value between 0\u20131 for each node (the sum of all these softmax values is equal to 1)."
    ],
    "<tokenizers>": [
        "This method defines tokenizer htat will be used for corpus tokenization.",
        "This tokenizer preprocessor uses given preprocessor + does english Porter stemming on tokens on top of it.",
        "tokenizer feeds it the words from the current batch.",
        "The tokenizer is created on a per-sentence basis by a tokenizer factory.",
        "A Tokenizer further segments the text at the level of single words, also alternatively as n-grams."
    ],
    "<Device Placement>": [],
    "<direct array>": [],
    "<standard decoder>": [],
    "<gitter>": [
        "Gitter is where you can request help and give feedback, but please do use this guide before asking questions we've answered below."
    ],
    "<stacked layers>": [],
    "<fixed shape>": [],
    "<pooling modes>": [],
    "<Learning Algorithm, learning algorithm, learning algorithms>": [],
    "<undirected graph>": [],
    "<front optimization>": [],
    "<Standard deviation, standard deviation, standard deviations>": [
        "stddev: The standard deviation of the Gaussian kernel to be approximated.",
        "The standard deviation is, if you want, the tradeoff between exploration and exploitation: the smaller the std, the less exploration."
    ],
    "<saveable>": [],
    "<Distributed communication, distributed communication>": [],
    "<negative trials>": [],
    "<local features>": [],
    "<tensor dimension, tensor dimensionality, tensor dimensions>": [
        "XLA requires that all tensor dimensions be statically defined at compile time."
    ],
    "<Batching tensors, batch Tensor, batch tensor>": [],
    "<Calculate activation>": [],
    "<single unsafe>": [],
    "<distributed execution>": [],
    "<Training loop, training loop, training loops>": [
        "By the way, it makes the training much faster to put MNIST on the GPU just once and manually create batches in the training loop.",
        "In that case the training loop should also stop."
    ],
    "<Jascha>": [],
    "<spline interpolation>": [],
    "<integer list>": [],
    "<Split sequences>": [],
    "<output names>": [],
    "<selected column>": [],
    "<Sobel filter>": [],
    "<training network, training networks>": [
        "You can do this whether your training network will be on a local, single node or distributed across multiple nodes via Spark."
    ],
    "<energy term>": [],
    "<device names>": [],
    "<processing batches>": [],
    "<baseline data>": [],
    "<model topology>": [],
    "<classifier distance>": [],
    "<resulting dimensions>": [
        "The resulting dimensions are: (batch, sequence, embedding)."
    ],
    "<contrastive loss>": [],
    "<Stop training, stop training, stopping training>": [],
    "<known rank>": [],
    "<distance loss>": [],
    "<reasonable name>": [],
    "<MC2>": [],
    "<Stop conditions, stop condition>": [],
    "<neural architecture>": [],
    "<Subtract layer>": [],
    "<discriminator function>": [],
    "<RELU6, relu6>": [],
    "<second trick>": [],
    "<fundamental flag>": [],
    "<sample batch>": [],
    "<output zeros>": [],
    "<running time>": [],
    "<splitting value>": [],
    "<forward transformation>": [
        "The forward transformation creates samples, the inverse is useful for computing probabilities.",
        "If X is a scalar then the forward transformation is: scale * X + shift where * denotes the scalar product."
    ],
    "<remaining dimensions>": [
        "If we reduce dimension 0, for example, we get a rank-2 array where all values across dimension 0 were folded into a scalar: If we reduce dimension 2, we also get a rank-2 array where all values across dimension 2 were folded into a scalar: Note that the relative order between the remaining dimensions in the input is preserved in the output, but some dimensions may get assigned new numbers (since the rank changes)."
    ],
    "<linear estimator>": [],
    "<Linear Models, Linear model, linear model, linear models>": [
        "Linear model with user specified head.",
        "A linear model uses a single weighted sum of features to make a prediction.",
        "Some linear models transform the weighted sum into a more convenient form.",
        "Regardless of the values of the parameters, the maximum accuracy a linear model can achieve on this dataset caps at around 93%.",
        "Because linear models assign independent weights to separate features, they can't learn the relative importance of specific combinations of feature values."
    ],
    "<Piecewise, piecewise>": [],
    "<condition tensor>": [
        "The condition tensor must be a scalar if x and y are scalar.",
        "The condition tensor acts as a mask that chooses, based on the value at each element, whether the corresponding element / row in the output should be taken from x (if true) or y (if false)."
    ],
    "<small data>": [],
    "<weight regularization>": [],
    "<Max number, max number>": [],
    "<control variable>": [
        "Variable scopes allow you to control variable reuse when calling functions which implicitly create and use variables."
    ],
    "<input slice>": [],
    "<master GPU>": [
        "When each GPU completes the computation, gradients are going to be reduced (added) onto the master GPU."
    ],
    "<Distance metric, distance metric>": [],
    "<training records>": [],
    "<conditional replacement>": [],
    "<data vectors>": [],
    "<scalar min>": [],
    "<Mesh Mode, mesh mode>": [
        "Mesh mode is a non-binary tree with Spark master at its root.",
        "Plain mode is to be used when the number of nodes in the cluster are < 32 nodes and mesh mode is to be used for larger clusters."
    ],
    "<distributed environment>": [],
    "<maximum absolute>": [
        "The maximum absolute difference allowed."
    ],
    "<local seed>": [],
    "<single minibatch>": [],
    "<NAS, Neural Architecture Search>": [],
    "<negative count>": [],
    "<Quickstart, quickstart>": [],
    "<amsgrad>": [],
    "<third image>": [],
    "<label list>": [],
    "<subtraction operation>": [],
    "<model stop, model stops>": [],
    "<mvn>": [],
    "<data quickly>": [],
    "<Iris Data, Iris data, iris data>": [],
    "<multiple encoder>": [],
    "<wave data>": [],
    "<Attention Mechanism, Attention mechanism, attention mechanism, attention mechanisms>": [
        "Overall, the Global attention mechanism can be summarized by the following figure.",
        "This flag only controls whether the attention mechanism is propagated up to the next cell in an RNN stack or to the top RNN output."
    ],
    "<training run>": [],
    "<GAN Training, GAN training>": [],
    "<aggregated loss>": [],
    "<stop flag>": [],
    "<time ordering>": [],
    "<Inception networks>": [
        "Because Inception networks are large, we will use the Deeplearning4j model zoo to help build our Zeppelin, make sure you add the deeplearning4j-zoo artifact to the Spark interpreter."
    ],
    "<False values>": [],
    "<bfloat16>": [],
    "<vertex name>": [],
    "<Gaussian Noise, Gaussian noise, gaussian noise>": [
        "Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs."
    ],
    "<Operation type, operation type>": [
        "A python line includes all graph nodes created by that line, while an operation type includes all graph nodes of that type."
    ],
    "<gate bias>": [],
    "<multivariate distributions>": [],
    "<SELU, SeLU, selu>": [],
    "<Beta distributed>": [],
    "<variable slice>": [],
    "<vraiment>": [],
    "<Probability density, probability densities, probability density>": [],
    "<hard error>": [],
    "<precision recall>": [
        "Get the precision recall curve as array."
    ],
    "<various dimensions>": [],
    "<Multiclass, multiclass>": [],
    "<StarGAN>": [],
    "<Input2, input2>": [],
    "<various inputs>": [],
    "<feature indices>": [
        "The feature indices represented as a dense tensor."
    ],
    "<input range>": [],
    "<flipped image>": [],
    "<random operations>": [
        "Many random operations internally use the two seeds to allow user to change the seed globally for a graph, or for only specific operations."
    ],
    "<depth dimension, depth dimensions>": [],
    "<gradient step>": [],
    "<RF parameters>": [],
    "<Exponential distributed>": [],
    "<training images>": [],
    "<Advanced Indexing, advanced indexing>": [],
    "<CS231n, cs231n>": [],
    "<diagflat>": [],
    "<classifier trained>": [],
    "<hidden dimension>": [],
    "<Input mask, input mask>": [
        "Note that the input mask array is not required for evaluation.",
        "Masks should not be applied in all cases, depending on the network configuration - for example input Dense -> RNN The first dense layer should be masked (using the input mask) whereas the second shouldn't be, as it has valid data coming from the RNN layer below."
    ],
    "<Hamming window>": [],
    "<contiguous tensor>": [],
    "<foldl, foldr>": [
        "foldl on the list of tensors unpacked from elems on dimension 0."
    ],
    "<Gradient normalization, gradient norm, gradient normalization>": [
        "Exploding gradients are problematic as they can 'mess up' the parameters of your network In the case of recurrent neural networks, adding some gradient normalization or gradient clipping may help."
    ],
    "<Generalized Normalization>": [],
    "<recurrent weights>": [],
    "<log elements>": [],
    "<ragged vector>": [],
    "<Sampling Algorithms>": [],
    "<tuple value>": [],
    "<final window>": [],
    "<sparse format>": [],
    "<Negative Binomial>": [],
    "<Random Search, random search>": [
        "Currently random search and grid search are supported."
    ],
    "<concentration values>": [],
    "<sub columns>": [],
    "<Total loss, total loss>": [
        "The total loss is thus accumulating Tensors and their gradient history, which may keep around large autograd graphs for much longer than necessary."
    ],
    "<checkpoint format>": [],
    "<Loading data, loading data>": [],
    "<distributed points>": [],
    "<max depth>": [],
    "<AVG, Avg, avg>": [],
    "<prediction part>": [],
    "<bit index>": [],
    "<fit operation>": [],
    "<initial cell>": [],
    "<parameter information>": [],
    "<gradient clipping, gradients clipping>": [
        "Exploding gradients are problematic as they can 'mess up' the parameters of your network In the case of recurrent neural networks, adding some gradient normalization or gradient clipping may help."
    ],
    "<Zero padding, zero padding>": [],
    "<CPU memory, cpu memory>": [],
    "<model locally>": [],
    "<topological sort>": [],
    "<behind padding>": [],
    "<communication patterns>": [],
    "<higher Tensor>": [],
    "<worker task>": [],
    "<Square matrices, square matrices, square matrix>": [
        "matrix is a tensor of shape [..., M, M] whose inner-most 2 dimensions form square matrices."
    ],
    "<single dimension>": [
        "A single dimension may be -1, in which case it's inferred from the remaining dimensions and the number of elements in input."
    ],
    "<Keras tensor, Keras tensors>": [],
    "<increment values>": [],
    "<box integration>": [],
    "<model repository>": [],
    "<sequential manner>": [],
    "<N1>": [],
    "<Trajectory Clustering>": [
        "Trajectory clustering can be a difficult problem to solve when your data isn't quite \"even\"."
    ],
    "<TDR>": [],
    "<multi label>": [],
    "<arbitrary labels>": [],
    "<Core Layers>": [],
    "<loss weights>": [],
    "<Mnist network>": [],
    "<eigen decomposition>": [],
    "<Variable vector>": [],
    "<Interpolation mode, interpolation mode>": [],
    "<layer batch>": [],
    "<Autoregressive Density>": [],
    "<optional randomisation, optional randomization>": [],
    "<reliability curve>": [],
    "<forward graph>": [],
    "<Diagonal matrix, diagonal matrices, diagonal matrix>": [],
    "<train loop>": [],
    "<one hot>": [],
    "<structured loss>": [],
    "<arrow type>": [],
    "<nested combination>": [],
    "<minimize true>": [],
    "<one probability>": [],
    "<synthetic data>": [
        "If not set, synthetic data is used."
    ],
    "<embedding weights>": [],
    "<optimization results>": [
        "An optimization result represents the results of an optimization run, including the canditate configuration, the trained model, the score for that model, and index of the model."
    ],
    "<pass shape>": [],
    "<Data Parallelism, Data parallelism, data parallelism>": [
        "Data parallelism is where we run multiple copies of the model on different slices of the input data.",
        "Data Parallelism is when we split the mini-batch of samples into multiple smaller mini-batches and run the computation for each of the smaller mini-batches in parallel."
    ],
    "<activation gradients>": [],
    "<worker job>": [
        "If empty no worker job is used."
    ],
    "<centerX>": [],
    "<img1>": [],
    "<sequential data>": [],
    "<compute capabilities>": [
        "CUDA compute capability required, or None if no requirement."
    ],
    "<Autoregressive distribution, Autoregressive distributions>": [],
    "<sequence batch>": [],
    "<capture state>": [],
    "<forward call>": [],
    "<single sample>": [
        "This single sample of negative classes is evaluated for each element in the batch."
    ],
    "<domain theorem>": [],
    "<Center Loss, center loss>": [],
    "<fixed rank>": [],
    "<frozen layer, frozen layers>": [
        "Frozen layer freezes parameters of the layer it wraps, but allows the backpropagation to continue.",
        "During the forward pass the frozen layer behaves as the layer within it would during test regardless of the training/test mode the network is in."
    ],
    "<Log probabilities, Log probability, log probabilities, log probability>": [],
    "<Copies data, copies data>": [],
    "<Nx1>": [],
    "<accidental seed>": [],
    "<next nodes>": [],
    "<input receivers>": [],
    "<analysis engine>": [],
    "<packt>": [],
    "<matrix Tensor>": [],
    "<arbitrary list>": [],
    "<best score>": [
        "Best score found so far.",
        "Time that the best score was found at, or 0 if no jobs have completed successfully."
    ],
    "<geoip>": [],
    "<Embedding Layers, Embedding layer, embedding layer>": [
        "An embedding layer is a part of neural network, but an embedding is a more general concept.",
        "Note that an embedding layer is used to encode our word indices in an arbitrarily sized feature space.",
        "Note also that embedding layer has an activation function (set to IDENTITY to disable) and optional bias (which is disabled by default)."
    ],
    "<functional transform>": [],
    "<output networks>": [],
    "<move nodes>": [],
    "<training script>": [
        "The training script automatically separates the data set into these three categories, and the logging line above shows the accuracy of model when run on the validation set."
    ],
    "<building graphs>": [],
    "<entire tensor>": [],
    "<orthogonal rows>": [],
    "<dimensional projections>": [],
    "<Decoding function>": [],
    "<VGG, VGG19, vgg19>": [],
    "<Orthogonal distribution, orthogonal distribution>": [],
    "<stratified batch>": [],
    "<kernel elements>": [],
    "<evaluation batch>": [],
    "<tag indices>": [],
    "<multiple transforms>": [],
    "<external errors>": [],
    "<post training>": [],
    "<slice dimensions>": [
        "Since all collapsed slice dimensions have to have bound 1 this reshape is always legal."
    ],
    "<adjustable dimensions>": [],
    "<cumprod>": [],
    "<Predicted values, predicted values>": [],
    "<log space>": [],
    "<shallow tree>": [],
    "<concurrent action>": [],
    "<point quantization>": [],
    "<input stride>": [],
    "<bound one>": [],
    "<regularization factor>": [],
    "<CPU architecture, CPU architectures>": [],
    "<trainer function>": [],
    "<lower latency>": [],
    "<Vision layers>": [],
    "<network issues>": [],
    "<baseline steps>": [],
    "<mean weight>": [],
    "<inverse scale>": [],
    "<taking values>": [],
    "<input section>": [
        "Notice, the how the inputs we set in the input section (nz, ngf, and nc) influence the generator architecture in code."
    ],
    "<single Gradient, single gradient>": [],
    "<GRU cell, gru cell>": [],
    "<multiple iterations>": [],
    "<isfinite>": [],
    "<Text Classification, Text classification, text classification>": [],
    "<variable value>": [],
    "<load weights, loading weights>": [],
    "<Batch shape, batch shape, batch shapes>": [
        "The batch shape is determined by broadcasting together the parameters.",
        "- Batch shape describes independent, not identically distributed draws, aka a \"collection\" or \"bunch\" of distributions."
    ],
    "<random scales>": [],
    "<Shape inference, shape inference>": [
        "Since shape inference is an optional feature, and the shapes of tensors may vary dynamically, shape functions must be robust to incomplete shape information for any of the inputs.",
        "The shape is computed using shape inference functions that are registered in the Op for each Operation.",
        "The value must be a compile-time-constant so that shape inference can determine the type of the resulting value."
    ],
    "<asynch>": [],
    "<persistable>": [],
    "<combined shape>": [
        "Even if the combined shape is well defined, the combined operator's methods may fail due to lack of broadcasting ability in the defining operators' methods."
    ],
    "<Random Tensors, random Tensors, random tensor>": [],
    "<OpenMP, OpenMPI, openMPI, openmp>": [],
    "<distributed job>": [],
    "<parallel region>": [],
    "<dialog models>": [],
    "<sampling seed>": [],
    "<tfprof>": [],
    "<NHWC>": [
        "But on CPU, NHWC is sometimes faster.",
        "NHWC is sometimes faster on CPU.",
        "'NHWC' and 'NCHW' are supported."
    ],
    "<probability predictions>": [],
    "<prediction head>": [],
    "<great learning>": [],
    "<numerical rank>": [],
    "<varying time>": [],
    "<Vector Machines>": [],
    "<diagonal vector>": [],
    "<Compute edge>": [],
    "<batch rank>": [],
    "<Redmon>": [],
    "<optimization problem>": [],
    "<reporting frequency>": [],
    "<independent input>": [],
    "<Stops learning>": [],
    "<Hadoop, hadoop>": [
        "Hadoop and Spark use this format to to make sure splits work properly in a cluster environment.",
        "Hadoop was originally developed for storing and Apache Spark was eventually developed for faster large-scale data processing, touting up to a 100x improvement over Hadoop."
    ],
    "<output projection>": [],
    "<disk location>": [],
    "<input strategy>": [],
    "<randn>": [],
    "<matching shapes>": [],
    "<extra layer>": [],
    "<compute correlation>": [],
    "<Output strings, output strings>": [],
    "<Min operation, min operation>": [
        "This operation applies Min operation to specific inputs through given indices Expected arguments: input: array to be updated indices: array containing indexes for first dimension of input updates: array containing elements to be interfered with input."
    ],
    "<classification prediction>": [],
    "<timly>": [],
    "<validate values>": [],
    "<templated>": [],
    "<model inferences>": [],
    "<rate signal>": [],
    "<gradient formula>": [],
    "<old models, older model>": [],
    "<running Properties>": [],
    "<central region>": [],
    "<positive event>": [],
    "<multiple cycles>": [],
    "<manual manipulation>": [],
    "<result shape, resulting shape>": [
        "When two compatible arrays are encountered, the result shape has the maximum among the two inputs at every dimension index."
    ],
    "<loop body>": [],
    "<batch slowly>": [],
    "<Sparse layers>": [],
    "<scalar product>": [],
    "<building layers>": [],
    "<Total sum, total sum>": [],
    "<input batch, input batches, inputs Batch, inputs batch>": [],
    "<conv layers>": [],
    "<projected factors>": [],
    "<cholesterol level>": [],
    "<regularization loss, regularization losses>": [
        "Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer."
    ],
    "<error rates>": [],
    "<Upper triangle, upper triangle>": [
        "The strictly upper triangle is ignored.",
        "The upper triangle of the last two dimensions is ignored."
    ],
    "<Spectrogram data, spectrogram data>": [],
    "<bound values>": [],
    "<linear outputs>": [],
    "<type Tensor, type tensor>": [],
    "<start state>": [],
    "<momentum rate>": [],
    "<nnnnn>": [],
    "<associated list>": [],
    "<scalar type>": [],
    "<Audio Recognition>": [
        "Because audio recognition is particularly useful on mobile devices, next we'll export it to a compact format that's easy to work with on those platforms.",
        "Most audio recognition applications need to run on a continuous stream of audio, rather than on individual clips."
    ],
    "<learning rate decrease>": [],
    "<Distributed Master, distributed master>": [
        "Where graph edges are cut by the partition, the distributed master inserts send and receive nodes to pass information between the distributed tasks Figure 6.",
        "The distributed master has grouped the model parameters in order to place them together on the Figure 5.",
        "The distributed master: prunes the graph to obtain the subgraph required to evaluate the nodes requested by the client, partitions the graph to obtain graph pieces for each participating device, and."
    ],
    "<multiple consequent>": [],
    "<evaluation function>": [],
    "<Vectorization, vectorization>": [
        "Vectorization is the first problem many data scientists will have to solve to start training their algorithms on data."
    ],
    "<serialized tensor>": [],
    "<training proceeds>": [],
    "<square image>": [],
    "<image activations>": [],
    "<single graph>": [
        "Only a single graph can be associated with a particular run.",
        "For many applications, a single graph is sufficient."
    ],
    "<tensor state>": [],
    "<prediction error, prediction errors>": [],
    "<float Tensor, float Tensors, float tensor, float tensors>": [],
    "<compute mean, computed mean, computes mean, computing mean>": [],
    "<weight variable>": [],
    "<Squared Error, squared error>": [],
    "<Exploding gradients, exploding gradient, exploding gradients>": [
        "Exploding gradients are problematic as they can 'mess up' the parameters of your network In the case of recurrent neural networks, adding some gradient normalization or gradient clipping may help."
    ],
    "<model configurations>": [],
    "<vector counts>": [],
    "<remote development>": [],
    "<generic condition>": [],
    "<parallel inputs>": [],
    "<Root value>": [
        "Root value is left -- right is unused."
    ],
    "<Deconvolution, deconvolution>": [],
    "<discrete states>": [],
    "<resnet>": [],
    "<termination condition>": [],
    "<context window>": [],
    "<Encoding function>": [],
    "<model loaded>": [],
    "<identical state>": [],
    "<Applies normalization, applies normalization, apply normalization>": [],
    "<saved checkpoint>": [],
    "<GER, ger>": [],
    "<normal development>": [],
    "<Embedding matrix, embedding matrix>": [
        "It is also easy to write: The embedding matrix will be initialized randomly and the model will learn to differentiate the meaning of words just by looking at the data."
    ],
    "<variable depends>": [],
    "<Discrete Random, discrete random>": [],
    "<attention vector, attention vectors>": [],
    "<crop window>": [
        "This significantly speeds up the process if the crop window is much smaller than the full image."
    ],
    "<loc Tensor>": [],
    "<square mean>": [
        "We precompute 'square mean' instead of 'variance', because the square mean can be easily adjusted on a per-example basis."
    ],
    "<Visual Programming>": [],
    "<weight noise>": [],
    "<calculation errors>": [],
    "<squared values>": [],
    "<SNE, sne>": [],
    "<final entry>": [],
    "<equal rank>": [],
    "<intrinsic conditioning>": [],
    "<penalize length>": [],
    "<shortcut function>": [
        "The only difference is that the shortcut function versions create and run the layer in a single call."
    ],
    "<numerical range, numerical ranges>": [],
    "<intermediate value>": [],
    "<label name, label names>": [
        "Sets the label names, will throw an exception if the passed in label names doesn't equal the number of outcomes.",
        "Sets the label names, will throw an exception if the passed Splits the data transform such that examples are sorted by their labels."
    ],
    "<rollout sampling>": [],
    "<minor dimension>": [],
    "<fundamental type>": [],
    "<output details>": [],
    "<ffmpeg>": [
        "Note that ffmpeg is free to select the \"best\" audio track from an mp4."
    ],
    "<error log>": [],
    "<usually images>": [],
    "<square patch>": [],
    "<multiple authors>": [],
    "<point scalar>": [],
    "<label column>": [],
    "<sspmm>": [],
    "<Add layer>": [],
    "<one minds>": [],
    "<objective learning>": [],
    "<good prediction>": [],
    "<Wrapped inputs>": [],
    "<conditional statement>": [],
    "<miminum values>": [],
    "<starting nodes>": [],
    "<Luong>": [],
    "<norm constraints>": [],
    "<Global Vectors>": [
        "Global Vectors for Word Representation are useful for detecting relationships between words."
    ],
    "<compute devices>": [
        "Get device (GPU, etc) maximum bytes - may be null if no compute devices are present in the system."
    ],
    "<parameter upper>": [],
    "<resulting operation>": [
        "The resulting operation takes no inputs."
    ],
    "<imagenet>": [],
    "<column separator>": [],
    "<training operation, training ops>": [
        "Examples of this include the initialization, and training ops demonstrated later.",
        "When run, the training op will update variables in the graph."
    ],
    "<equal shape, equal shapes>": [],
    "<training corpus>": [],
    "<model checkpoint, model checkpoints>": [
        "If None, model checkpoints and summaries will not be written."
    ],
    "<Reshaping distribution>": [],
    "<random factor>": [],
    "<complex sequence>": [],
    "<Online Learning>": [],
    "<beta parameter, beta parameters>": [],
    "<adjusted tensor>": [],
    "<crossed feature>": [],
    "<logits layer>": [],
    "<Latent Semantic, latent semantic>": [],
    "<fixed feature>": [],
    "<Caffe>": [
        "(Caffe forces you to use hdf5, for example.)."
    ],
    "<tan function>": [],
    "<channel values>": [],
    "<inferred shape, inferred shapes>": [
        "The inferred shape of a tensor is used to provide shape information without having to launch the graph in a session.",
        "These inferred shapes might have known or unknown rank."
    ],
    "<Load distribution>": [],
    "<Small gradients, small gradient, small gradients>": [
        "Small gradients means it is hard to learn.",
        "During backproping, small gradients might underflow in the reduced numerical range, causing a model to converge at suboptimal level."
    ],
    "<regularization term>": [],
    "<compute output>": [],
    "<supported device>": [
        "The supported device names are \"/device:CPU:0\" (or \"/cpu:0\") for the CPU device, and \"/device:GPU:i\" (or \"/gpu:i\") for the ith GPU device."
    ],
    "<optimization functions>": [],
    "<baddbmm>": [],
    "<non tensor>": [],
    "<input trying>": [],
    "<initial states>": [
        "In the first segment of an example the state is still the initial state.",
        "The initial state values must all have fixed and well defined dimensions.",
        "The simplest form of RNN network generated is: However, a few other options are available: An initial state can be provided."
    ],
    "<Temporal convolution, temporal convolution>": [],
    "<fifth column>": [],
    "<Converted image>": [],
    "<UIMA>": [
        "UIMA enables us to perform language identification, language-specific segmentation, sentence boundary detection and entity detection (proper nouns: persons, corporations, places and things)."
    ],
    "<columns value>": [],
    "<local servers>": [],
    "<variance One>": [],
    "<constant beta>": [],
    "<conditional reduction>": [
        "Conditional reduction: apply the reduces on a specified column, where the reduction occurs *only* on those Beware, the output will be huge!"
    ],
    "<single transition>": [],
    "<gather values>": [],
    "<average discriminator>": [],
    "<core part>": [],
    "<beta integral>": [],
    "<Tensor data, tensor data>": [],
    "<feature dimension, features dimension>": [
        "Each example has 7 (pool2 height) * 7 (pool2 width) * 64 (pool2 channels) features, so we want the features dimension to have a value of 7 * 7 * 64 (3136 in total)."
    ],
    "<Gradient averaging, gradient averaging>": [],
    "<Image warping>": [],
    "<updated values>": [
        "After aggregation, updated values above some configurable threshold are propagated across the network as a sparse binary array."
    ],
    "<hacky>": [],
    "<Elementwise, elementwise, elems>": [
        "Elementwise computes the bitwise XOR of x and y.",
        "Elementwise computes the bitwise OR of x and y.",
        "Suppose that elems is unpacked into values, a list of tensors.",
        "So you see, elementwise operations match the elements of one matrix with their precise counterparts in another matrix.",
        "If initializer is None, elems must contain at least one element, and its first element is used as the initializer.",
        "If elems is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension.",
        "Elementwise computes the bitwise left-shift of x and y.",
        "Elementwise operations are more intuitive than vectorwise operations, because the elements of one matrix map clearly onto the other, and to obtain the result, you have to perform just one arithmetical operation.",
        "That is, if elems is (t1, [t2, t3, [t4, t5]]), then an appropriate signature for fn is: fn = lambda (t1, [t2, t3, [t4, t5]]):."
    ],
    "<upscaling>": [],
    "<triangular band>": [],
    "<Clojure>": [
        "Clojure programmers may want to use Leiningen or Boot to work with Maven."
    ],
    "<Handling images>": [],
    "<adder function>": [],
    "<transposed matrix>": [],
    "<log likelihood>": [
        "This is useful when an observed output at a given time step is consistent with more than one tag, and thus the log likelihood of that observation must take into account all possible consistent tags."
    ],
    "<True negatives, true negatives>": [
        "Get the true negatives count for the specified output."
    ],
    "<Level1>": [],
    "<full statistics>": [],
    "<ETL>": [
        "This method defines, if ETL time per iteration should be reported together with other data.",
        "In our dl4j-examples repo, we don't make the ETL asynchronous, because the point of examples is to keep them simple."
    ],
    "<one run>": [],
    "<model faster>": [],
    "<TopK, topk>": [],
    "<angle value>": [],
    "<cat image>": [],
    "<unique identity>": [],
    "<read image>": [],
    "<numbers log>": [
        "Negative numbers log always; this is the default."
    ],
    "<Peak Signal>": [],
    "<GPU available>": [],
    "<cell type, cell types>": [],
    "<dz>": [],
    "<feedforward>": [],
    "<next checkpoint>": [],
    "<Scalar multiplication, scalar multiplication>": [
        "Elementwise scalar multiplication can be represented several ways.",
        "Elementwise scalar multiplication looks like this: And produces this: Subtraction and division follow a similar pattern: If you perform all these operations on your initial 2 x 2 matrix, you should end up with this matrix: Elementwise vector operations."
    ],
    "<label one>": [],
    "<squared distance>": [],
    "<parametric constraints>": [],
    "<stable order>": [],
    "<dense option>": [],
    "<Input columns, input columns>": [
        "Note that the input columns must also be numerical."
    ],
    "<desired dimensions>": [],
    "<img2>": [],
    "<single array>": [],
    "<Simple function, simple function>": [],
    "<Increments number, increment number, increments number>": [],
    "<Sigmoid activation, sigmoid activation>": [
        "Consequently, the sigmoid activation function should be used to bound activations to the range of 0 to 1."
    ],
    "<candidate cell>": [],
    "<constant memory>": [
        "PLEASE NOTE: CUDA constant memory is limited to 48KB per device."
    ],
    "<Batch elements, batch element, batch elements>": [
        "Batch elements will be ordered decreasingly by their length."
    ],
    "<Model prediction, model predictions>": [
        "Compute model predictions given input data."
    ],
    "<sequence examples>": [],
    "<Classification statistics, classification statistics>": [],
    "<inverted index>": [],
    "<determine number>": [],
    "<full epoch>": [],
    "<line content>": [],
    "<particular worker>": [],
    "<network convergence>": [],
    "<simplest network>": [],
    "<score calculation, score calculator>": [],
    "<precision function>": [],
    "<standard graph>": [],
    "<actual device>": [],
    "<next index>": [],
    "<complex pipeline>": [],
    "<matrix columns>": [
        "The matrix columns represent the prediction labels and the rows represent the real labels."
    ],
    "<Random Field, random field>": [],
    "<network state>": [],
    "<correlation coefficient>": [],
    "<high dimension, higher dimension>": [
        "The dimension indices must be an in-order (low to high dimension numbers), consecutive subset of T's dimensions."
    ],
    "<previous batch>": [],
    "<sample covariance>": [],
    "<shape arguments>": [],
    "<Machine Learning, Machine learning, machine learning>": [
        "While handwriting recognition has been attempted by different machine learning algorithms over the years, deep learning performs MNIST dataset.",
        "If you are reading this, hopefully you can appreciate how effective some machine learning models are.",
        "Many machine learning models are represented by composing layers.",
        "Machine learning provides many algorithms to classify flowers statistically.",
        "Machine learning techniques have a set of parameters that have to be chosen before any training can begin.",
        "Some simple models can be described with a few lines of algebra, but complex machine learning models have a large number of parameters that are difficult to summarize.",
        "When publishing research models and techniques, most machine learning practitioners share: code to create the model, and.",
        "The reason you can't simply use your training data for evaluation is because machine learning methods are prone to overfitting (getting good at making predictions about the training set, but not performing well on larger datasets).",
        "Overfitting is when a machine learning model performs worse on new data than on their training data.",
        "Typical machine learning, of course, has one hidden layer, and those shallow nets are called Perceptrons.",
        "Machine learning models, however, need to have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!)."
    ],
    "<rastered>": [],
    "<standard element>": [],
    "<scale shear>": [],
    "<Sequence Classification, Sequence classification, sequence classification>": [
        "Sequence classification is one common use of masking."
    ],
    "<associated step>": [
        "When writing to an event log file, the associated step will be zero."
    ],
    "<poor cluster>": [],
    "<data indefinitely>": [],
    "<STN>": [],
    "<Stochastic dual>": [],
    "<temperature variable>": [],
    "<update messages>": [],
    "<dense approach>": [],
    "<memory length>": [],
    "<random batch, random batches>": [],
    "<loss layer>": [],
    "<one kernel>": [],
    "<second tensors>": [],
    "<sparse indices>": [],
    "<activation matrix>": [],
    "<constant scalar>": [],
    "<optimality>": [],
    "<feature vector, feature vectors, features vector>": [
        "When the feature vector assigned to a word cannot be used to accurately predict that word's context, the components of the vector are adjusted."
    ],
    "<MTGP>": [],
    "<Bernoulli Bernoulli>": [],
    "<processing path>": [],
    "<concrete terms>": [],
    "<categorical input>": [],
    "<maximum shape>": [],
    "<Hann window>": [],
    "<Parameter Server, Parameter servers, parameter server, parameter servers>": [
        "Server state for the parameter server stopped or started.",
        "In a distributed system, each worker process runs the same model, and parameter server processes own the master copies of the variables.",
        "The parameter server method can also be used for local training, In this case, instead of spreading the master copies of variables across parameters servers, they are either on the CPU or spread across the available GPUs.",
        "If you are running on multiple machines, you might have a single master host that drives computation across all of them, or you might have multiple clients driving the computation To distribute an algorithm, we might use some of these ingredients: Parameter servers: These are hosts that hold a single copy of parameters/variables."
    ],
    "<Load model, load model>": [],
    "<Good values>": [
        "Good values to start mostly depends on your workstation."
    ],
    "<Fixed coefficient>": [],
    "<Activation parameter>": [],
    "<attention module>": [],
    "<dummy dimension>": [],
    "<Parametrized, param, parametrized>": [
        "param writable the writable to transform.",
        "param is the inputstream to load from.",
        "param repartition Setting for repartitioning.",
        "param conf a configuration for initialization.",
        "param writable the element to test.",
        "param lookback Look back period for windowing.",
        "param is Input stream to read from.",
        "param is the inputstream to get the computation graph from.",
        "param files the statistics to save.",
        "param features The normalized array of inputs.",
        "param allowNaN If false: don't allow NaN values.",
        "param is the input stream to restore from.",
        "param guess the system guess.",
        "param array the data to normalize.",
        "param split the split that defines the range of records to read.",
        "param array the data to denormalize.",
        "param features the given feature matrix.",
        "param gradient the gradient to get the update for.",
        "param processor to be used on the data.",
        "param gradient the gradient to get the updated gradient for.",
        "param input multidataset to feed into the computation graph with frozen layer vertices.",
        "param gradient the gradient to get learning rates for.",
        "param ratio - this value will be used as splitter.",
        "param network the network to use.",
        "param guesses the guesses/prediction (usually a probability vector)."
    ],
    "<unevaluated Tensor>": [],
    "<Optional scope, optional scope>": [],
    "<Optional inverse>": [],
    "<column vocab>": [],
    "<compute weighted>": [],
    "<randint>": [],
    "<Exponential distribution, Exponential distributions, exponential distribution>": [
        "The Exponential distribution is parameterized by an event rate parameter.",
        "Construct Vector Exponential distribution supported on a subset of R^k."
    ],
    "<general matrix>": [],
    "<old graph>": [],
    "<tensor label>": [],
    "<zero dropout>": [],
    "<Binary Encoding>": [],
    "<cumulative computation>": [],
    "<normal convolution>": [],
    "<average statistics>": [],
    "<parameter vector>": [],
    "<positive error>": [],
    "<scalar outputs>": [],
    "<logical sum>": [],
    "<reliability weights>": [
        "NOTE: these weights are treated as \"frequency weights\", as opposed to \"reliability weights\"."
    ],
    "<erfc>": [],
    "<Float weight, float weights>": [],
    "<neighbor points>": [
        "A convolutional network is appropriate for this type of gridded data, since each point in the 2-dimensional grid is related to its neighbor points."
    ],
    "<shape depends>": [
        "The shape depends on reduction."
    ],
    "<running average, running averages>": [],
    "<MFCC, MFCCs>": [],
    "<component tensor>": [],
    "<labels array, labels arrays>": [
        "param labels Each triple in the list specifies the shape, array order and type of values for the labels arrays."
    ],
    "<gradient square, gradient squared>": [],
    "<Image Transformation, image transformations, image transforms>": [
        "Transforms are common image transformations."
    ],
    "<normalized text>": [],
    "<normalizing labels>": [],
    "<random data>": [],
    "<low padding>": [
        "The low padding is applied in the direction of lower indices while the high padding is applied in the direction of higher indices."
    ],
    "<randomized algorithm>": [],
    "<update mean>": [],
    "<distance measure>": [],
    "<bilinear interpolation>": [
        "Since only bilinear interpolation is currently supported, the last dimension of the warp tensor must be 2."
    ],
    "<Finetuning, finetuning>": [],
    "<dense flow>": [],
    "<real valued, real values>": [],
    "<idx>": [],
    "<sent tensor>": [],
    "<prediction labels>": [
        "Correct prediction labels are blue and incorrect prediction labels are red."
    ],
    "<equal dimensions>": [],
    "<parallelism model>": [],
    "<JDKs>": [
        "Java JDKs can be downloaded from Oracle's website."
    ],
    "<empty values>": [
        "Duplicate and empty values are removed."
    ],
    "<sample approximation>": [],
    "<designated device>": [],
    "<orthogonal normalization>": [],
    "<average sentence>": [
        "For many corpora, average sentence length is six words.",
        "A:If average sentence contains 6 words, and window size is 5, maximum theoretical number of 10 skipgram rounds will be achieved on 0 words."
    ],
    "<max capacity>": [],
    "<indexing tensors>": [],
    "<model path>": [],
    "<randomness seed>": [],
    "<PSNR>": [],
    "<Main evaluation>": [],
    "<graph operation>": [
        "PLEASE NOTE: This operation is internal graph operation, and shouldn't be used directly usually."
    ],
    "<predicted label, predicted labels>": [
        "The predicted label is picked by choosing the entry with the highest score."
    ],
    "<approximation approach>": [],
    "<stacked form>": [],
    "<Anomaly Detection, Anomaly detection, anomaly detection>": [
        "Network intrusion, fraud detection, systems monitoring, sensor network event detection (IoT), and unusual trajectory sensing are examples of anomaly detection applications.",
        "Anomaly detection does not require a labeled dataset, and can be undertaken with unsupervised learning, which is helpful because most of the world's data is not labeled."
    ],
    "<image result, image results>": [
        "Adds image summaries to see StarGAN image results."
    ],
    "<volumetric inputs>": [],
    "<Stereogram>": [],
    "<relative accuracy>": [],
    "<random records>": [],
    "<fc1>": [],
    "<standardized image>": [],
    "<AWS>": [],
    "<filter element>": [],
    "<magnitude values>": [],
    "<Premade, premade>": [
        "Premade Estimators are designed to get results out of the box."
    ],
    "<checkpoint path>": [],
    "<bounds positions>": [],
    "<propagation step>": [],
    "<estimated values>": [],
    "<autoregressive model>": [],
    "<negative axis>": [],
    "<large elements>": [],
    "<bias graphs>": [],
    "<dilation arguments>": [],
    "<Performance Models>": [
        "High-Performance Models gets into more details regarding more complex methods that can be used to share and update variables between towers."
    ],
    "<semantic segmentation>": [],
    "<noise patterns>": [],
    "<local statistics>": [],
    "<curve values>": [],
    "<sparse one, sparse ones>": [],
    "<process cluster>": [],
    "<Applies Dropout, applies dropout, apply dropout>": [],
    "<model feeding>": [],
    "<training methods>": [],
    "<mapping tensor>": [
        "The mapping is initialized from a string mapping tensor where each element is a key and corresponding index within the tensor is the value."
    ],
    "<viewing distance>": [],
    "<optional bias>": [],
    "<multiplicative scaling>": [],
    "<backward Function, backward function, backwards function>": [
        "The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value."
    ],
    "<local tensor>": [],
    "<Adagrad, adagrad>": [
        "Momentum and Adagrad use variables to accumulate updates.",
        "Adagrad keeps a history of gradients being passed in."
    ],
    "<SGD, Stochastic Gradient Descent, stochastic gradient descent>": [
        "SGD updater applies a learning rate only.",
        "SGD requires more careful tuning of hyper-parameters and weight initialization.",
        "Stochastic Gradient Descent, the most common learning algorithm in deep learning, relies on Theta (the weights in hidden layers) and alpha (the learning rate)."
    ],
    "<example images>": [],
    "<clipped gradient>": [],
    "<TFX>": [
        "Magenta, TFX, Learn more about machine learning."
    ],
    "<Hardtanh, hardtanh>": [],
    "<Gating Gradients>": [],
    "<load process>": [],
    "<Single step, single step>": [
        "Note however that a single step needs to process the full input at once."
    ],
    "<Input Exercises>": [],
    "<real convolution>": [],
    "<compute mode>": [],
    "<Device memory, device memory>": [
        "Device memory will be used for cache (if current backend support such differentiation).",
        "DEVICE memory will probably have the same size, but won't be accounted in this value."
    ],
    "<density unit>": [],
    "<duplicate models>": [
        "After a specified number of iterations (in this case 3), all models will be averaged and workers will receive duplicate models."
    ],
    "<positive examples>": [],
    "<Started guide, started guide>": [],
    "<aggressive type>": [],
    "<valued distribution>": [],
    "<machine failures>": [],
    "<pam>": [],
    "<s32>": [
        "s32 elements become f32 elements via bitcast routine.",
        "s32 elements become f32 elements via an s32-to-f32 conversion routine."
    ],
    "<Numeric threshold>": [],
    "<distributed process, distributed processes>": [
        "If the utility is used for GPU training, each distributed process will be operating on a single GPU."
    ],
    "<log value>": [],
    "<validation data>": [
        "The validation data is selected from the last samples in the x and y data provided, before shuffling."
    ],
    "<vocabulary values>": [],
    "<FULL model, full model>": [],
    "<potrf, potrs, pstrf>": [],
    "<encoder name>": [],
    "<loop condition>": [],
    "<initial learning>": [],
    "<autotuning>": [],
    "<sequence Layout>": [],
    "<TF, Tf, tensor filter, tf>": [
        "TF Lite allows mobile developers to do inference efficiently on mobile devices.",
        "You'll see 'TF Speech' appear in your app list, and opening it will show you the same list of action words we've just trained our model on, starting with \"Yes\" and \"No\"."
    ],
    "<GPU systems>": [],
    "<produce device>": [],
    "<TANH, Tanh, tanh>": [
        "Hard tanh elementwise derivative function.",
        "For activation functions, identity and perhaps tanh are typical - though tanh (unlike identity) implies a minimum/maximum possible value for mean and log variance.",
        "Some other activation functions (tanh, sigmoid, etc) are more prone to vanishing gradient problems, which can make learning much harder in deep neural networks.",
        "Obviously, I should change the code in lstm part of my model to match dimension, it seems that tanh make it fit for the next lstm which is not right because I want to feed it to Linear."
    ],
    "<reduced dimensions, reducing dimensions>": [
        "Add-reducing dimensions 0 and 1 produces the 1D array | 20 28 36 |."
    ],
    "<label equality>": [],
    "<image mode, image model>": [
        "The image is then converted back to original image mode."
    ],
    "<rank-1>": [],
    "<Learning Representations>": [],
    "<muxer>": [],
    "<update step>": [],
    "<full input>": [],
    "<dump root>": [],
    "<RL, Reinforcement Learning, reinforcement learning>": [],
    "<cores value>": [],
    "<Backprop, Backpropagation, backprop, backpropagation>": [
        "Backpropagation will happen only into logits.",
        "Truncated backpropagation through time (BPTT) was developed in order to reduce the computational complexity of each parameter update in a recurrent neural network.",
        "Backpropagation will compute the gradients automatically for us.",
        "The backpropagation and learning procedure is otherwise as described there.",
        "Unfortunately, this makes backpropagation computation difficult.",
        "However the backpropagation does not seem to be happening as the loss does not change with epochs.",
        "Backpropagation will happen into both logits and labels.",
        "In this case, standard backpropagation through time would require 10,000 time steps for each of the forward and backward passes for each and every parameter update.",
        "Backpropagation involves the multiplication of very small gradients, due to limited precision when representing real numbers values very close to zero can not be represented.",
        "Backprop is skipped since parameters are not be updated."
    ],
    "<single batch>": [],
    "<one accordion>": [],
    "<CNN, CNNs, Cnn, cnn, convolutional neural network, convolutional neural networks>": [
        "In practice, convolutional neural networks (CNN's) are better suited for image classification tasks.",
        "In a dense layer, every node in the layer is connected to Typically, a CNN is composed of a stack of convolutional modules that perform feature extraction.",
        "Convolutional neural networks (CNNs) are the current state-of-the-art model architecture for image classification tasks.",
        "We've coded the CNN model function, Estimator, and the training/evaluation Note: Training CNNs is quite computationally intensive.",
        "CNNs apply a series of filters to the raw pixel data of an image to extract and learn higher-level features, which Convolutional layers, which apply a specified number of convolution filters to the image.",
        "We can interpret the softmax values for a given image as relative measurements of how likely it is that the image falls into each target Note: For a more comprehensive walkthrough of CNN architecture, see Stanford Convolutional Neural Networks for Visual Recognition course materials.",
        "It can be a useful mechanism because CNNs are not invariant to rotation and scale and more general affine transformations.",
        "Since CNNs can take a while to train, let's set up some logging so we can track progress during training."
    ],
    "<individual examples>": [],
    "<final binary>": [],
    "<actual computation>": [],
    "<jobz>": [],
    "<finite amount>": [],
    "<sample outputs>": [],
    "<unit Tensor>": [],
    "<classification evaluation>": [],
    "<repmat>": [],
    "<stride array>": [],
    "<slight bias>": [],
    "<temporal order>": [],
    "<counter variable>": [],
    "<random transformations>": [],
    "<sample dimension, samples dimension>": [],
    "<wargs>": [],
    "<Uniform distribution, Uniform distributions, uniform distribution>": [],
    "<near feature>": [],
    "<Tensor image, tensor image, tensor images>": [],
    "<flat vector, float vector>": [],
    "<function interface>": [],
    "<outer list>": [],
    "<key tensors>": [],
    "<deps>": [],
    "<line tutorial>": [],
    "<temperature tensor>": [],
    "<Save distribution>": [],
    "<trim operation>": [],
    "<Conditional Accumulators, conditional accumulator>": [],
    "<sequence inputs>": [],
    "<Spark training>": [
        "DL4J Spark training can also be performed using GPUs.",
        "DL4J Spark training supports the ability to load data serialized in a custom format."
    ],
    "<model trainer>": [],
    "<input space>": [],
    "<pip3>": [],
    "<distributed computation>": [],
    "<mask tensor>": [],
    "<Recurrent instance>": [],
    "<identical behavior>": [],
    "<bad data>": [
        "That way users can more easily track down where the bad data came from."
    ],
    "<frac>": [],
    "<slogdet>": [],
    "<reduction mode>": [],
    "<allocation point>": [],
    "<hinge loss>": [],
    "<Split elements>": [],
    "<state memory>": [],
    "<matrix equations>": [],
    "<input exception>": [],
    "<output keys>": [],
    "<batch index>": [],
    "<AddN>": [],
    "<historical state>": [],
    "<normal options>": [],
    "<step mean>": [],
    "<random index>": [],
    "<floor function>": [],
    "<model architectures>": [
        "Since each model architecture is different, there is no boilerplate finetuning code that will work in all scenarios."
    ],
    "<temporal dimension>": [
        "The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension."
    ],
    "<Inverse Discrete, inverse discrete>": [],
    "<scoring sequence>": [],
    "<sepal length>": [],
    "<semantic similarity>": [],
    "<reversing dimensions>": [],
    "<RELU, RRELU, RReLU, ReLU, Rectified Linear Unit, Rectified Linear Units, Rectified linear unit, Rectified linear units, Relu, rectified linear unit, relu, rrelu>": [
        "Motivation: ReLU is piecewise linear, so maybe it doesn't help that much.",
        "There are many available activations, but ReLU is common for hidden layers.",
        "If nonlinearity is 'relu', then ReLU is used instead of tanh.",
        "Relu in this The variable net here signifies the current top layer of the network."
    ],
    "<character frequency>": [],
    "<highest frequency>": [],
    "<cumsum>": [],
    "<normal arrays>": [],
    "<contracted Tensor>": [],
    "<filter data>": [],
    "<vector multiplication>": [],
    "<dimensional space>": [],
    "<CPU>": [
        "CPU backend supports multiple CPU ISAs.",
        "CPU info will not be logged.",
        "CPU backend it might be ignored, depending on Aggregate.",
        "If this cannot be loaded, the CPU (nd4j-native) backend will be loaded second.",
        "While the accelerator is performing training step N, the CPU is preparing the data for step N+1.",
        "On the other hand, higher values can increase contention if CPU is scarce.",
        "This function may be used when CPU time is scarce and inputs are trusted or unimportant."
    ],
    "<Scope name, scope name>": [],
    "<train call>": [],
    "<initial matrix>": [],
    "<Polynomial decay, polynomial decay>": [],
    "<equal number>": [],
    "<resulting behavior>": [
        "The resulting behavior may be undefined."
    ],
    "<alternating convolutions>": [],
    "<filter algorithm>": [],
    "<orange line>": [
        "The orange line represents training.",
        "During training, summaries (the orange line) are recorded periodically as batches are processed, which is why it becomes a graph spanning x-axis range."
    ],
    "<individual graph>": [],
    "<Invalid device>": [],
    "<exact calculation>": [],
    "<intrinsic parameters>": [],
    "<graph function>": [
        "Note: This function is executed as a graph function in graph mode."
    ],
    "<squared sum>": [],
    "<MIN, minimax>": [],
    "<old behavior>": [],
    "<Proguard, proguard>": [],
    "<Multinomial distribution, multinomial distribution>": [],
    "<certain shape>": [],
    "<second axis>": [],
    "<training master>": [
        "The training master will setup the workers with the desired hooks for training."
    ],
    "<Four Tensor>": [],
    "<Injective, injective>": [],
    "<multi array>": [],
    "<graph node, graph nodes>": [
        "This method checks, if specified graph node is active (as in - located within active code branch, and was NOT left in inactive branch)."
    ],
    "<Randomization seed>": [],
    "<diagonal tensor>": [],
    "<known value>": [],
    "<MNIST data, Mnist data>": [
        "Although the MNIST data isn't time series in nature, we can interpret it as such since there are 784 inputs.",
        "MNIST data set iterator - 60000 training digits, 10000 test digits, 10 classes."
    ],
    "<global batch>": [],
    "<Data augmentation, data augmentation>": [],
    "<infs>": [],
    "<Applied element, Applies element, applied element, applies element>": [],
    "<various factors>": [],
    "<Weight parameter, weight parameter, weight parameters>": [
        "Weight parameter keys given the layer configuration."
    ],
    "<multiple replicas>": [],
    "<Keras activation>": [],
    "<process termination>": [
        "If they don't, and the first process does not terminate, the process termination will go unnoticed."
    ],
    "<Input vertices, input vertex, input vertices>": [],
    "<prediction output>": [],
    "<empty Tensor, empty tensor>": [
        "The tensor can either be a scalar default value (if the column is optional), or an empty tensor (if the column is required)."
    ],
    "<continuous parameter, continuous parameters>": [
        "The discretization count determines how many values a continuous parameter is binned into."
    ],
    "<fake batch>": [],
    "<simulated quantization>": [],
    "<training worker, training workers>": [
        "For a three-training-worker distributed configuration, each training worker is likely to go through the whole epoch independently."
    ],
    "<normalizing flow>": [],
    "<pre processing>": [],
    "<Distribution strategy>": [
        "This parameter is only used when Distribution Strategy is used with estimator or keras."
    ],
    "<Input layer, input layer, input layers>": [
        "Hidden layers can learn features from the input layer and it can send those features to be analyzed by our output layer to get the corresponding outputs.",
        "The input layer is only a set of inputs values fed into the network."
    ],
    "<multiple towers>": [],
    "<human labor>": [],
    "<distribution type, distribution types>": [],
    "<column embedding>": [],
    "<MCC>": [],
    "<gRPC>": [],
    "<complex networks>": [
        "Note that more complex networks and problems may never yield an optimal score."
    ],
    "<gradient update, gradient updates, gradients updates>": [
        "Otherwise, gradient updates are applied asynchronous.",
        "Collect gradients updates to apply them with the last tower.",
        "If the argument is supplied, gradient updates will be synchronous.",
        "If left as None, gradient updates will be asynchronous."
    ],
    "<parameter graphs>": [],
    "<general solution>": [],
    "<rank approximation>": [],
    "<maximal number>": [],
    "<computational power>": [],
    "<single model>": [],
    "<Adaptive Gradients, adaptive gradients>": [
        "Defines, if adaptive gradients should be created during vocabulary mastering.",
        "This method defines whether adaptive gradients should be used or not.",
        "This method defines if Adaptive Gradients should be used in calculations.",
        "This method specifies, if adaptive gradients should be used during model training."
    ],
    "<deploy model, deploy models>": [],
    "<hidden state, hidden states>": [
        "The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state).",
        "The hidden state vector is then passed to the next time step, while the output vector is recorded.",
        "After predicting the next word, the modified hidden states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words."
    ],
    "<Optional message>": [],
    "<unknown tensor>": [],
    "<incremental behavior>": [],
    "<ONNX, onnx>": [
        "ONNX (Open Neural Network Exchange) is an open format to represent deep learning models.",
        "ONNX is developed and supported by a community of partners.",
        "Since my target ONNX runtime does not support onnx::Shape, I'd like to export IR with hard-coded shape."
    ],
    "<tune learning>": [],
    "<parameterized distribution>": [],
    "<quantile, quantiles>": [],
    "<Single Layer, single layer>": [],
    "<classification matrix>": [],
    "<Tensor Shapes, Tensor shapes, tensor shape, tensor shapes>": [
        "During execution any unknown shape dimensions are determined dynamically, see Tensor Shapes for more details."
    ],
    "<Probability Mass, probability mass>": [],
    "<entropy term>": [],
    "<belief network>": [],
    "<correct predictions>": [
        "Correct prediction labels are blue and incorrect prediction labels are red.",
        "If the prediction is correct, we add the sample to the list of correct predictions."
    ],
    "<positive labels>": [],
    "<intermediate layers>": [
        "These layers are not scoring/output layers: that is, they should be used as the intermediate layer in a network only."
    ],
    "<Script function, script function>": [],
    "<pseudo inverse>": [],
    "<loop counter>": [
        "However, because the loop counter at one loop iteration depends on the value at the previous iteration, the loop counter itself cannot be incremented in parallel."
    ],
    "<Automatic variable>": [
        "Automatic variable lifting makes it possible to compile these APIs without extra effort, at the cost of introducing a discrepancy between the semantics of executing Python functions and their corresponding compiled functions."
    ],
    "<side signal>": [],
    "<hidden consists>": [],
    "<connected components>": [],
    "<model heads>": [],
    "<Scalar Tensor, Scalar tensor, scalar Tensor, scalar Tensors, scalar tensor, scalar tensors>": [],
    "<dimensional shapes>": [],
    "<significant number>": [],
    "<average frequency, averaging frequency>": [],
    "<one platform>": [],
    "<mat2>": [
        "If mat1 is a \\((n \\times m)\\) tensor, mat2 is a Note."
    ],
    "<hidden distribution>": [],
    "<portion shape>": [],
    "<data Tensors, data tensor>": [],
    "<HDF5 format>": [],
    "<TSNE>": [],
    "<downsample, downsampling>": [],
    "<filter step>": [],
    "<Tailweight parameter>": [],
    "<GPU compute>": [],
    "<composed shape>": [
        "Even if the composed shape is well defined, the composed operator's methods may fail due to lack of broadcasting ability in the defining operators' methods."
    ],
    "<Networks Tutorial, networks tutorial>": [],
    "<Network Latency>": [],
    "<Style algorithm>": [],
    "<Bounding Boxes, Bounding boxes, bounding boxes>": [
        "Controls behavior if no bounding boxes supplied.",
        "Bounding box annotations are often supplied in addition to ground-truth labels in image recognition or object localization tasks."
    ],
    "<epoch count>": [],
    "<dimensional loss>": [],
    "<multiple devices>": [],
    "<multiple countries>": [],
    "<multi dimensional>": [],
    "<InfoQ>": [],
    "<gradients point>": [
        "The gradients point in the direction of steepest ascent\u2014so we'll travel the opposite way and move down the hill."
    ],
    "<Distributed Evaluation, distributed evaluation>": [],
    "<old inputs>": [],
    "<Tensor index, tensor indexing>": [
        "Torch Script currently does not support mutating tensors in place, so any tensor indexing can only appear on the right-hand size of an expression."
    ],
    "<supported distribution>": [],
    "<dense labels>": [],
    "<approximate duality>": [],
    "<log1p>": [],
    "<regression evaluation>": [],
    "<larger indices>": [],
    "<Run Multiple, run multiple>": [],
    "<word vector, word vectors>": [
        "Whether normalized word vectors should be used."
    ],
    "<accuracy monitoring>": [],
    "<compilation process>": [],
    "<particular shape>": [],
    "<untar>": [],
    "<untyped>": [],
    "<dynamic condition>": [],
    "<Scalability ratio>": [],
    "<Riba>": [],
    "<combinatorial coefficient>": [],
    "<overall computation>": [],
    "<degenerate dimension, degenerate dimensions>": [],
    "<second vertex>": [],
    "<wave length>": [],
    "<apply decay>": [],
    "<SBT>": [],
    "<deterministic evaluation>": [],
    "<computations involving>": [
        "By default, computations involving variables that require gradients will keep history."
    ],
    "<training libraries>": [],
    "<complex methods>": [],
    "<batch distribution>": [],
    "<partial run, partial runs>": [],
    "<batch member>": [],
    "<training progress, training progresses>": [],
    "<early experimentation>": [],
    "<final activation>": [
        "Also, the final activation becomes a Sigmoid, which squashes values into a range between 0 and 1."
    ],
    "<transition matrix>": [],
    "<Maximum function, maximum function>": [],
    "<PMF, pmf, probability mass function>": [
        "The probability mass function (pmf) is."
    ],
    "<RFFM>": [],
    "<inference step>": [],
    "<Probability distributions, probability distribution, probability distributions>": [
        "Distribution is the abstract base class for probability distributions."
    ],
    "<One tensor, one Tensor, one tensor>": [],
    "<real environment, real environments>": [],
    "<loss value, loss values>": [
        "The loss value that will be minimized by the model will then be the sum of all individual losses."
    ],
    "<ceiling function>": [],
    "<normalization dimension>": [],
    "<Validation accuracy, validation accuracies, validation accuracy>": [],
    "<data scientists>": [],
    "<sparse input, sparse inputs>": [],
    "<mean number>": [],
    "<Github, github>": [],
    "<complex architecture>": [],
    "<Op, one prediction, op>": [
        "Those attrs can be inferred when the op is added to the graph and so don't appear in the op's function.",
        "This op is ELU activation function.",
        "This op will be removed after the deprecation date.",
        "The Operation objects on which this op has a control dependency.",
        "That Op just has no effect.",
        "This op has no output.",
        "This op cuts a rectangular part out of image.",
        "Where Op is the same as above.",
        "This op is only defined for complex matrices.",
        "This op takes either 1 argument and 1 scalar or 1 argument and another comparison array and runs a pre defined conditional op.",
        "When true, custom ops are created for any op that is unknown.",
        "When this op finishes, all ops in inputs have finished.",
        "That op will be run if the enqueue ops raise exceptions.",
        "If the op has multiple outputs, the gradient function will take op and grads, where grads is a list of gradients with respect to each output.",
        "When building ops to compute gradients, this op prevents the contribution of its inputs to be taken into account.",
        "Base operation for two vectors is: For tensors of more complicated shape this op is computed pairwise.",
        "When that Op is run it tries to increment the variable by 1.",
        "This op creates w and optionally b.",
        "This op can be used to override the gradient for complicated functions.",
        "The Op for shard i is colocated with the inputs for shard i.",
        "This op is intended to save memory during initialization.",
        "The op uses LU decomposition with partial pivoting to compute the inverses.",
        "When executed in a graph, this op outputs its input tensor as-is.",
        "This operation casts elements of input array to int32 data type PLEASE NOTE: This op is disabled atm, and reserved for future releases.",
        "This op boosts specified input up to specified shape PLEASE NOTE: This op is disabled atm, and reserved for future releases.",
        "This op produces binary matrix wrt to target dimension.",
        "If no error is raised, the Op outputs the value of the variable before the increment.",
        "The op used to prefetch new data into the state saver.",
        "The example above assumed that the op applied to a tensor of any shape.",
        "This op changes data type of input array.",
        "WARNING: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.",
        "Op to initialize worker state before starting column updates.",
        "One example is a subgraph representing a single operation and its inputs and outputs: all the input and output tensors of the op are \"connected\".",
        "In some cases, an op has no well-defined gradient but can be involved in the computation of the gradient.",
        "To handle that situation, this op also provides dicts of shape tensors as part of the output.",
        "The op uses two different If the input values are all positive, they are rescaled so the largest one is 255.",
        "If there is a danger that values would over or underflow in the cast, this op applies the appropriate clamping before the cast.",
        "This op allows for evaluating metrics that cannot be updated incrementally using the same framework as other streaming metrics.",
        "Arguments: updates: Update op, or list/tuple of update ops.",
        "This op takes 2 equally shaped arrays as input, and provides binary matrix as output.",
        "This op changes order of given array to specified order.",
        "This op fills Z with bernoulli trial results, so 0, or 1, each element will have it's own success probability defined in prob array.",
        "This op must be run for every iteration that accesses data from the state saver (otherwise the state saver will never progress through its states and run out of capacity).",
        "This op does the same as tear, just uses different input format: \\tparam T.",
        "The op also uses a shape function to ensure that the output tensor is the same shape as the input tensor.",
        "This op fills Z with random values within 0...to.",
        "This op has its own internal threads that are dominated by I/O time that consume minimal CPU, which allows it to run smoothly in parallel with the rest of the model.",
        "This op fills Z with binomial distribution over given trials with single given probability for all trials.",
        "Any new attrs added to an operation must have default values defined, and with that default value the op must have the original behavior.",
        "With this constructor, op will check each X element against given Condition, and if condition met, element will be replaced with Set value Pseudocode: PLEASE NOTE: X will be modified inplace.",
        "An op involves iterating over 2 buffers (x,y) up to n elements and applying a transform or accumulating a result.",
        "This op describes Axpy call PLEASE NOTE: this method is especially important for CUDA backend.",
        "Note that this op *may* dynamically generate variable outputs.",
        "This op translates a tensor containing Example records, encoded using the standard JSON mapping, into a tensor containing the same records encoded as binary protocol Example-parsing ops.",
        "Op names starting with an underscore are reserved for internal use.",
        "When Op is Rem, the sign of the result is taken from the dividend, and the absolute value of the result is always less than the divisor's absolute value.",
        "This Op generates binomial distribution.",
        "PLEASE NOTE: This op is available for CPU only, and should NOT be ever called manually, unless you know why you're using it.",
        "Limitation: this Op only broadcasts the dense side to the sparse side, but not the other direction.",
        "This op fills Z with bernoulli trial results, so 0, or 1, depending by common probability.",
        "This op calculates cross-product between input arguments Input arguments 0 - vector or tensor A 1 - vector or tensor B.",
        "This op is optimized for the case where at least one of \"a\" or \"b\" is sparse, in the sense that they have a large proportion of zero values.",
        "More specifically, this op outputs a copy of the input tensor where values from the height and width dimensions are moved to the batch dimension.",
        "This Op generates normal distribution over provided mean and stddev.",
        "Rather than computing AUC directly, this Op maintains Variables containing histograms of the scores associated with True and False labels.",
        "This should be -1 when the Op is running on a TPU device, and >= 0 when the Op is running on the CPU device.",
        "This op fills Z with random values within stddev..0..stddev.",
        "The op accepts, for example, input types (float, double, float) and in that case the output type would also be (float, double, float).",
        "Where Op is one of Eq (equal-to), Ne (not equal-to), Ge (greater-or-equal-than), Gt (greater-than), Le (less-or-equal-than), Lt (less-than)."
    ],
    "<particular step>": [],
    "<MKL, mkl>": [
        "The MKL is optimized for NCHW and Intel is working to get near performance parity when using NHWC.",
        "However, MKL is liked with by default (when available) so setting this option explicitly is not usually required."
    ],
    "<encoder model>": [
        "The encoder model takes an input sequence and a corresponding lengths tensor."
    ],
    "<partial updates>": [],
    "<sparse behavior>": [],
    "<comparison array>": [],
    "<Stackoverflow, stackoverflow>": [],
    "<compute shift>": [],
    "<Asynchronous parameters>": [],
    "<input signature>": [],
    "<static batch>": [],
    "<batched inverse>": [
        "For a batch of matrices, the batched inverse (if invertible) is raised to the power n."
    ],
    "<Tensor name, tensor name, tensor names>": [],
    "<device field>": [],
    "<Early Stopping, Early stopping, early stopping>": [
        "Early stopping is a useful technique to prevent overfitting.",
        "If both early stopping and number of epochs is present: early stopping will be used.",
        "Thus care and early stopping are important.",
        "Early stopping is one mechanism used to manually set the number of epochs to prevent underfitting and overfitting."
    ],
    "<Weighted loss, weighted loss, weighted losses>": [],
    "<mode loss, model loss>": [],
    "<automatic name>": [],
    "<fixed memory>": [],
    "<sending state>": [],
    "<Average recall, average recall>": [],
    "<tfrecord>": [],
    "<remote devices>": [],
    "<fixed graph>": [],
    "<cell clipping>": [],
    "<split images>": [],
    "<FN, False Negative, False negative, False negatives, Fn, false negative, false negatives, fn>": [
        "Get the false negatives count for the specified output.",
        "When FN is instantiated with attr \"foo\" set to \"bar\", the instantiated node N's attr A will have been given the value \"bar\"."
    ],
    "<Single example, single Example, single example, single examples>": [],
    "<Argmin, argmin>": [],
    "<scalar weight>": [],
    "<probability histogram>": [],
    "<multiple independent>": [],
    "<image domain>": [],
    "<Kullback>": [],
    "<recall function>": [],
    "<binary dropout>": [],
    "<aeron channel, aeron channels>": [],
    "<model formats>": [],
    "<primitive operation>": [],
    "<feature type>": [],
    "<trained values>": [],
    "<dim1>": [],
    "<sparse column, sparse columns>": [
        "combiner: A string specifying how to reduce if the sparse column is multivalent."
    ],
    "<invalid graphs>": [],
    "<primary distributions>": [],
    "<column analysis>": [],
    "<INT8, int8>": [],
    "<normalized array>": [],
    "<ntuple>": [],
    "<moving statistics>": [],
    "<multiple dependencies>": [],
    "<Algorithmic Engineering>": [],
    "<Input matrix, input matrices, input matrix>": [
        "Since the input matrix input is supposed to be symmetric, only the upper triangular portion is used by default.",
        "Input matrix must have full column rank.",
        "Input matrix must have full row rank."
    ],
    "<time window>": [],
    "<poor result>": [],
    "<onesided>": [],
    "<step count>": [],
    "<input Examples, input example, input examples>": [
        "Suppose our input examples consist of different words from a limited palette of only 81 words."
    ],
    "<fourth dimensions>": [],
    "<Training model, Training models, training mode, training model, training models>": [
        "The training argument takes a boolean specifying whether or not the model is currently being run in training mode; dropout will only be performed if training is True.",
        "Example 3: Training models with weights merge on GPU (recommended for NV-link)."
    ],
    "<total precision>": [],
    "<batch come>": [],
    "<reduce batch>": [],
    "<regularization arguments>": [],
    "<loading part>": [],
    "<projection layer>": [],
    "<AVX>": [],
    "<batch mean>": [],
    "<discretization count>": [
        "The discretization count determines how many values a continuous parameter is binned into."
    ],
    "<Riemann sum, Riemann summation>": [],
    "<select function>": [
        "The binary select function is used to select an element from each window by applying it across each window, and it is called with the property that the first parameter's index vector is lexicographically less than the second parameter's index vector."
    ],
    "<False Shape>": [],
    "<word index, word indices>": [],
    "<orthogonal decomposition>": [],
    "<convex combination>": [],
    "<sparse arrays>": [],
    "<Random methods>": [],
    "<Categorical distribution, Categorical distributions, categorical distribution>": [
        "The categorical distribution is parameterized by the log-probabilities of a set of classes."
    ],
    "<Update rule, update rule>": [],
    "<range inputs>": [],
    "<unconstrained tensor>": [],
    "<Binary Classification, binary classification>": [],
    "<sparse updates>": [
        "Useful for avoiding \"leaky\" frequent exogenous updates when sparse updates are desired."
    ],
    "<gradient variable>": [],
    "<geqrf>": [],
    "<unknown data>": [],
    "<TPU system>": [],
    "<random noise>": [],
    "<loads data>": [],
    "<network security>": [],
    "<high padding>": [
        "The low padding is applied in the direction of lower indices while the high padding is applied in the direction of higher indices."
    ],
    "<global pooling>": [
        "Global pooling layer can also handle mask arrays when dealing with variable length inputs."
    ],
    "<top prediction>": [
        "It calculates the precision at 1: how often the top prediction matches the true label of the image."
    ],
    "<CPU mode>": [],
    "<functional layer, functional layers>": [],
    "<Negative Sampling, negative sampling>": [],
    "<integration result>": [],
    "<parameter shapes>": [],
    "<surrouding state>": [],
    "<neural Tensor, neural tensor>": [],
    "<input gate>": [],
    "<pruning functions>": [],
    "<Softmax, softmax>": [
        "See how the weights and biases are connected, and how softmax is applied to give the probability distribution.",
        "The point is, even though logsoftmax and softmax are monotonic, their effect on the relative values of the loss function changes.",
        "Adaptive softmax is an approximate strategy for training models with large output spaces.",
        "Adaptive softmax partitions the labels into several clusters, according to their frequency."
    ],
    "<Dk, dK>": [],
    "<Shape vector, shape vector>": [],
    "<training cluster>": [],
    "<computation shape>": [],
    "<batch prediction>": [],
    "<score examples>": [],
    "<profiling mode>": [],
    "<update gradient, updated gradient>": [],
    "<time axis>": [
        "If you increase this value, then fewer samples will be taken for a given duration, and the time axis of the input will shrink."
    ],
    "<recod>": [],
    "<jitter saturation>": [],
    "<maximum rank>": [],
    "<Network Training, network trained, network training>": [
        "The network will be trained similarly to the network trained tutorial 15.",
        "Training statistics may include things like per-epoch run times, These statistics are primarily used for debugging and optimization, in order to gain some insight into what aspects of network training are taking the most time."
    ],
    "<data anyway>": [],
    "<Automatic Differentiation, Automatic differentiation, automatic differentiation>": [],
    "<Attention Layer, attention layer>": [],
    "<technical data>": [],
    "<partial differential>": [],
    "<multiple heads>": [],
    "<ML model, ML models>": [
        "So, although raw data can be numerical or categorical, an ML model represents all features as numbers.",
        "ML models generally represent categorical values as simple vectors in which a 1 represents the presence of a value and a 0 represents the absence of a value."
    ],
    "<rsqrt>": [],
    "<matching type>": [],
    "<multiple shapes>": [],
    "<expanded dimensions>": [],
    "<multiplication algorithm>": [],
    "<Distributed Representations>": [],
    "<Bucketized column>": [],
    "<underlie tensors>": [],
    "<overlap values>": [],
    "<CRF, CRFs>": [
        "Recall that the CRF computes a conditional probability.",
        "The LSTM tagger above is typically sufficient for part-of-speech tagging, but a sequence model like the CRF is really essential for strong performance on NER."
    ],
    "<Gamma distributed>": [],
    "<final calculation>": [],
    "<batched tensors>": [],
    "<checkpoint data>": [],
    "<training result, training results>": [],
    "<insertion indices>": [
        "The insertion indices of the examples (when they were first added)."
    ],
    "<one label>": [],
    "<log determinant>": [],
    "<sign decay>": [],
    "<crossed column, crossed columns>": [],
    "<List values>": [],
    "<one minority>": [],
    "<LRN, local response normalization, lrn>": [],
    "<model developers>": [],
    "<human intuition>": [],
    "<temporal step>": [],
    "<sequence columns>": [],
    "<central result>": [
        "Finally, the central result of this tutorial comes from the test function."
    ],
    "<real distribution>": [],
    "<cflags>": [],
    "<deeper layers>": [],
    "<static value>": [],
    "<aka linear>": [],
    "<edge values>": [],
    "<ndarray, ndarrays>": [
        "If the ndarray isn't compressed this will do nothing.",
        "Reshapes the ndarray (can't change the length of the ndarray).",
        "This is a proxy class so that ndarrays can be serialized and deserialized independent of the backend Be it cpu or gpu."
    ],
    "<VisualVM>": [],
    "<batched diagonal>": [],
    "<Udacity>": [],
    "<Clips values, clip value, clipping value>": [],
    "<Classification loss, classification loss>": [],
    "<Lapack, lapack>": [],
    "<Scalar integer, scalar integer>": [],
    "<gradient computated>": [],
    "<Linear function, Linear functions, linear function, linear functions>": [],
    "<lower range>": [],
    "<single equation>": [],
    "<collect statistics>": [],
    "<epsilon input>": [],
    "<input gradient>": [
        "The gradient computation of this operation will only take advantage of sparsity in the input gradient when that gradient comes from a Relu."
    ],
    "<df1>": [],
    "<values inputs>": [],
    "<transform tensor, transformed tensor>": [],
    "<naming sequence>": [],
    "<fast convergence>": [],
    "<model components>": [],
    "<symbolic shape>": [],
    "<Minimum function, minimum function>": [],
    "<Android device, Android devices>": [
        "If it does, move all files to one or the other as some Android devices will have problems with both present."
    ],
    "<Removes dimensions>": [],
    "<positive count>": [],
    "<logsumexp>": [],
    "<ifconfig>": [],
    "<detection threshold>": [],
    "<Cluster nodes, cluster Nodes, cluster node, cluster nodes>": [
        "Case 1: Cluster nodes have CUDA toolkit installed on the master and worker nodes."
    ],
    "<index tensor>": [],
    "<downscaling>": [],
    "<Replicated training>": [],
    "<Shared name>": [],
    "<graph computation>": [],
    "<ring order>": [],
    "<float multiplier>": [],
    "<multiple array>": [
        "Note that multiple array types may be stored in the one underlying workspace."
    ],
    "<complicated node>": [],
    "<normalized image>": [],
    "<Allocation mode, allocation mode, allocation model>": [],
    "<Laplace distributed>": [],
    "<feed values>": [],
    "<equivalent integer>": [],
    "<unpartitioned shape>": [],
    "<multiple workers>": [
        "While the GPU is busy your multiple workers can load a new batch of images instead of a large file.",
        "If multiple workers or threads all execute count in parallel, there is no guarantee that access to the variable v is atomic at any point within any thread's calculation of count."
    ],
    "<trained word>": [],
    "<trained variable, trained variables>": [],
    "<Mx1x2>": [],
    "<Probability code>": [],
    "<GAN, GANs, Generative Adversarial Networks>": [
        "A GAN consists of two distinct neural network models: a generator and a discriminator.",
        "GANs are a framework for teaching a DL model to capture the training data's distribution so we can generate new data from that same distribution.",
        "GANs were invented by Ian Goodfellow in 2014 and first Nets."
    ],
    "<word nodes>": [],
    "<generalized contraction>": [],
    "<Max Pumperla>": [],
    "<dev1>": [],
    "<NCDHW>": [],
    "<right examples>": [],
    "<energies tensor>": [
        "This attention energies tensor is the same size as the encoder output, and the two are ultimately multiplied, resulting in a weighted tensor whose largest values represent the most important parts of the query sentence at a particular time-step of decoding."
    ],
    "<decoding iterations>": [],
    "<updated list>": [],
    "<beta Tensor>": [],
    "<Google Cloud, Google cloud>": [
        "Google Cloud TPU Documentation \u2014Set up and run a Google Cloud TPU.",
        "Google Cloud provides no setup required, pre-configured virtual machines to help you build your deep learning projects."
    ],
    "<attention weights>": [
        "Calculating the attention weights is done with another feed-forward layer attn, using the decoder's input and hidden state as inputs.",
        "The output attention weights have the same shape as the input sequence, allowing us to multiply them by the encoder outputs, giving us a weighted sum which indicates the Robertson's figure describes this very well: Luong et al."
    ],
    "<PCA, pca>": [
        "PCA tends to highlight large-scale structure in the data, but can distort local neighborhoods."
    ],
    "<Transforms vectors>": [],
    "<spatial problem>": [],
    "<matrix norm, matrix norms>": [
        "If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension."
    ],
    "<required parameter>": [],
    "<forward inference>": [],
    "<local rank>": [],
    "<FFT, fft>": [
        "FFT object, transform amplitudes to frequency intensities.",
        "Default convolution instance (FFT based)."
    ],
    "<worker device>": [],
    "<Variational, variational>": [],
    "<particular devices>": [],
    "<diagonal elements>": [
        "This operator is non-singular if and only if its diagonal elements are all non-zero."
    ],
    "<partial execution>": [],
    "<mapped indices>": [],
    "<full graph>": [],
    "<model variable, model variables>": [],
    "<initial coordination>": [],
    "<Expected inputs, expected inputs>": [
        "expected inputs are 3-D, 4-D or 5-D in shape.",
        "Expected inputs are spatial (4 dimensional)."
    ],
    "<GPU machines>": [],
    "<Download data>": [],
    "<gradient terms>": [],
    "<full gradient>": [],
    "<Kernel Methods>": [
        "If you are new to kernel methods, refer to either of the If you have a strong mathematical background: Kernel Methods in Machine Learning."
    ],
    "<perturbations start>": [],
    "<full sequences>": [],
    "<normal numbers>": [],
    "<one metric>": [],
    "<dimension tensor, dimensional Tensor, dimensional Tensors, dimensional tensor, dimensional tensors>": [],
    "<protos>": [],
    "<initial iterations>": [],
    "<iters>": [],
    "<YUV>": [],
    "<square crop>": [
        "If int, square crop is made."
    ],
    "<one batch>": [],
    "<input index, input indices>": [
        "Find the input index corresponding to the given input tensor t.",
        "If the input indices is rank N, the output will have rank N+1."
    ],
    "<batch message>": [],
    "<validation batch>": [],
    "<image randomly>": [],
    "<previous steps>": [],
    "<post update>": [],
    "<constant length>": [],
    "<applied computation>": [],
    "<label field>": [],
    "<mirrored values>": [],
    "<normalization values>": [],
    "<unraveling indices>": [],
    "<clipping operation>": [],
    "<sample field>": [
        "Also, if your sample field is blank/null, it will also become the default type."
    ],
    "<Tensor arguments, tensor argument, tensor arguments, tensors argument>": [],
    "<single functions>": [],
    "<valid one>": [],
    "<canonical ordering>": [],
    "<Keras guide>": [],
    "<slack channel>": [],
    "<multiple epochs>": [],
    "<loss criterion>": [
        "But in the NumPy code, the loss criterion is set to the difference without the absolute, so the loss could be back propagated."
    ],
    "<continuous distributions>": [],
    "<Linear Operator, linear operation, linear operator, linear operators>": [],
    "<computing batches>": [],
    "<GPU support, gpu support>": [],
    "<weight associated>": [],
    "<predictions arguments>": [],
    "<negative determinant>": [],
    "<identical length>": [],
    "<spatial data>": [],
    "<Categorical features, categorical feature, categorical features>": [],
    "<shift values>": [
        "Negative shift values will shift elements in the opposite direction."
    ],
    "<side inputs>": [],
    "<graph structure>": [
        "The graph structure is like assembly code: inspecting it can convey some useful information, but it does not contain all of the useful context that source code conveys."
    ],
    "<double scalar>": [],
    "<loss calculated>": [
        "But loss calculated for only one iteration, dont know what is wrong here."
    ],
    "<batch version>": [],
    "<intermediate model>": [
        "If True, performs intermediate model checkpoints and exports during the training process, rather than only once model training is complete."
    ],
    "<regularization multiplier>": [],
    "<profile step>": [],
    "<label classes, label classification>": [
        "Multi-label classification handles the case where each example may have zero or more associated labels, from a discrete set."
    ],
    "<training batch, training batches>": [],
    "<neutral type>": [],
    "<bias weights>": [],
    "<submatrices>": [],
    "<constrained problem>": [],
    "<neural vocoder>": [],
    "<large models>": [],
    "<contracting dimension, contracting dimensions>": [],
    "<vector shape>": [],
    "<inference phase>": [],
    "<Mixture Density, mixture density>": [],
    "<automatic rank>": [
        "Note that automatic rank assignment is not supported anymore in the latest Warning."
    ],
    "<discrete staircase>": [],
    "<global flow>": [],
    "<output symbols>": [],
    "<CuBLAS, Cublas, cuBLAS, cublas>": [],
    "<network criterion>": [],
    "<decoding step>": [],
    "<filter Tensor, filter tensor, filter tensors>": [],
    "<diffeomorphic>": [],
    "<stacked Tensor>": [],
    "<positive diagonals>": [],
    "<per layer>": [],
    "<hot encoding>": [],
    "<Denoising, denoising>": [],
    "<output integers>": [],
    "<dim Tensor>": [],
    "<parameter histograms>": [],
    "<hanged process>": [],
    "<tax rate>": [],
    "<reconstruction distribution, reconstruction distributions>": [],
    "<computing entropy>": [],
    "<Threshold location>": [],
    "<Neural Transfer, neural transfer>": [],
    "<RPCs>": [],
    "<Explicit padding>": [],
    "<wrong rank>": [],
    "<output functions>": [],
    "<distributed mode, distributed model>": [],
    "<Quantization interval>": [],
    "<ifft, irfft>": [],
    "<checkpoint state>": [],
    "<directed graph>": [],
    "<large degree>": [],
    "<Dropout functions>": [],
    "<weight lookup>": [],
    "<optimization runner>": [],
    "<positive function>": [],
    "<final epoch>": [],
    "<perform training>": [],
    "<time sequence, time sequences>": [],
    "<reinforcement algorithm>": [],
    "<Input Sequences, input sequences>": [
        "Note that the resulting sequence may be of length 0, if the input sequence is less than or equal to N.",
        "In addition, once an input sequence element is attended to at a given output timestep, elements occurring before it cannot be attended to at subsequent output timesteps.",
        "Monotonic attention implies that the input sequence is processed in an explicitly left-to-right manner when generating the output sequence.",
        "It is recommended to use truncated BPTT when your input sequences are long (typically, more than a few hundred time steps)."
    ],
    "<Final batch, final batch>": [
        "If True, allow the final batch to be smaller if there are insufficient items left in the queue."
    ],
    "<interpolant>": [],
    "<cost array>": [
        "Consequently, an array of all 1s (or, indeed any array of equal values) will result in the same performance as no cost array; non- Evaluation.",
        "A cost array can be used to bias the multi class predictions towards or away from certain classes."
    ],
    "<parameterized model>": [],
    "<policy gradient>": [],
    "<Regexp, regexp, regexs>": [],
    "<flattening order>": [
        "Order is specified to ensure flattening order is consistent across."
    ],
    "<tensor count>": [],
    "<loss module>": [],
    "<Pooling2D>": [],
    "<numerical data>": [],
    "<Evaluable>": [],
    "<bicubic>": [],
    "<Padding layers, padding layer>": [],
    "<identical name>": [],
    "<random inputs>": [],
    "<forward algorithm>": [],
    "<truth labels>": [],
    "<predicted probabilities, predicted probability>": [],
    "<edge information>": [],
    "<input row, input rows>": [],
    "<array activations>": [],
    "<log2>": [],
    "<load Parallelism>": [],
    "<value feature>": [],
    "<single length>": [],
    "<scalar scalar>": [],
    "<Activation Functions, Activation function, Activation functions, activation function, activation functions>": [
        "Asymmetric activation functions such as sigmoid or relu should be avoided.",
        "Activation layer is a simple layer that applies the specified activation function to the input activations.",
        "At a simple level, activation functions help decide whether a neuron should be activated.",
        "However, for LSTM layers, the tanh activation function is still commonly used.",
        "The activation function determines the output shape of each node in the layer.",
        "Regarding the choice of activation function: the parameterization above supports gamma in the range (-infinity,infinity) therefore a symmetric activation function such as \"identity\" or perhaps \"tanh\" is preferred.",
        "The activation function is a non-linear transformation that happens over an input signal, and the transformed output is sent to the next neuron.",
        "Activation functions also introduce non-linearities in our network so that we can learn on more complex features present in our data.",
        "Some other activation functions (tanh, sigmoid, etc) are more prone to vanishing gradient problems, which can make learning much harder in deep neural networks.",
        "Activation functions that do not produce outputs in the range of 0 to 1 (including relu, tanh, and many others) should be avoided.",
        "Activation functions are the exception, because they take strings such as \"relu\" or \"tanh\"."
    ],
    "<Thx>": [],
    "<updaters>": [
        "All updaters that support a learning rate also support learning rate schedules (the Nesterov momentum updater also supports a momentum schedule).",
        "Updater: updates the model.",
        "SGD updater applies a learning rate only.",
        "If false, don't retain (updaters will be reinitalized in each worker after averaging).",
        "The updater is not required to use the network at test time; it is saved in case further training is required."
    ],
    "<processing steps>": [],
    "<beta value>": [],
    "<compute predicted>": [],
    "<segfaults>": [],
    "<absolute difference>": [],
    "<froot>": [],
    "<synchronous mode>": [],
    "<POJO, pojo>": [],
    "<calling distribution>": [],
    "<evaluation data>": [],
    "<saving data>": [],
    "<desactivate>": [],
    "<running models>": [],
    "<Calculate Loss, calculate loss, calculating loss>": [],
    "<labels source>": [],
    "<right value>": [],
    "<Normalizing data>": [
        "Normalizing data is pretty easy in DL4J."
    ],
    "<Scale Transforms, scale transformation>": [],
    "<place tensor>": [],
    "<good examples>": [],
    "<final step>": [
        "Now that the model structure is correct, the final step for finetuning and feature extracting is to create an optimizer that only updates the desired parameters."
    ],
    "<shift terms>": [],
    "<model frequently>": [],
    "<lxtGH>": [],
    "<uint16>": [],
    "<parallel version>": [],
    "<single tower>": [],
    "<loss component>": [
        "However this is not the desired behaviour, since the missing loss component should be treated as unknown rather than zero."
    ],
    "<one loads>": [],
    "<Simple Model, simple model, simplest model>": [
        "That said, the simplest model is still the value semantics model shown in the introduction to C++ modules."
    ],
    "<performing computation>": [],
    "<Network outputs, network output, network outputs>": [
        "Network outputs are for output layers only.",
        "the network output doesn't have to be in any specific range.",
        "Each network output is assumed to be a separate/independent binary class, with probability 0 to 1 independent of all other outputs."
    ],
    "<Average loss, average loss>": [],
    "<image projective>": [],
    "<scale matrices, scale matrix>": [],
    "<logspace>": [],
    "<Constant distribution, constant distribution>": [],
    "<many steps>": [],
    "<saver state>": [],
    "<standard implicit>": [],
    "<CIDR>": [],
    "<reduced Tensor, reduced tensor>": [],
    "<symbolic function>": [],
    "<anchor images>": [],
    "<Optional name>": [],
    "<stated value>": [],
    "<Stochastic Optimization>": [],
    "<Neural Computation>": [],
    "<trace operation>": [],
    "<Recurrent Network, recurrent Network, recurrent network, recurrent networks>": [
        "My recurrent network doesn't work with data parallelism\u00b6.",
        "In addition, LSTM networks avoid vanishing and exploding gradients and are able to effectively capture long term dependencies due to its cell state, a feature not present in typical recurrent networks.",
        "See My recurrent network doesn't work with data parallelism section in FAQ for details."
    ],
    "<ACGAN>": [],
    "<dy2>": [],
    "<likelihood operation>": [],
    "<scalar numeric>": [],
    "<Stop value>": [],
    "<dump data>": [],
    "<inverse permutation>": [],
    "<training details>": [],
    "<binary state>": [],
    "<complete index>": [],
    "<Labels Tensor, label Tensors, label tensor, label tensors, labels Tensor, labels tensor, labels tensors>": [
        "Measures the loss given an input tensor x and a labels tensor y containing values (1 or -1).",
        "labels tensor are directly broadcasted to all the TPU cores since the partition dims is None."
    ],
    "<F1 Score, F1 score, f1 score>": [],
    "<Scalar subtraction, scalar subtraction>": [],
    "<Distance functions>": [],
    "<training module>": [],
    "<train function>": [
        "This function is quite self explanatory, as we have done the heavy lifting with the train function.",
        "The train function contains the algorithm for a single training iteration (a single batch of inputs)."
    ],
    "<map transformation>": [
        "If the user-defined function passed into the map transformation changes the size of the elements, then the ordering of the map transformation and the transformations that buffer elements affects the memory usage."
    ],
    "<total weight>": [],
    "<Cropped image, cropped image>": [
        "All cropped image patches are resized to this size."
    ],
    "<masking value>": [],
    "<fill value>": [],
    "<Cosine distance, cosine distance>": [
        "dim: The dimension along which the cosine distance is computed.",
        "Cosine distance Note that you need to initialize a scaling constant equal to the norm2 of the vector."
    ],
    "<permutation indices>": [],
    "<cell function>": [],
    "<log softmax>": [],
    "<difference operation>": [],
    "<single prediction>": [],
    "<Boltzmann Machine, Boltzmann Machines, Boltzmann machines>": [],
    "<Noise Ratio>": [],
    "<Vector Formats, vector format>": [],
    "<features axis>": [],
    "<preactivation values>": [],
    "<constant vector>": [],
    "<join examples>": [],
    "<compute predictions>": [],
    "<False Alarm>": [],
    "<natural exponential>": [],
    "<integer count>": [],
    "<Build models, build models>": [],
    "<replica index>": [],
    "<normal Variable, normal Variables>": [],
    "<CS321n>": [],
    "<background process>": [],
    "<flattened arrays>": [],
    "<constant term>": [
        "If false, a constant term is dropped in favor of more efficient optimization."
    ],
    "<Pooling functions>": [],
    "<multiple graphs>": [],
    "<GPU collective>": [],
    "<Deeplearning layer>": [],
    "<correct device>": [],
    "<Multiple Layers, Multiply layer, multiple layers>": [],
    "<Initial input, initial input>": [
        "The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state)."
    ],
    "<Computation Graph, Computation Graphs, computation graph, computation graphs, computational graph, computational graphs>": [
        "Computation Graph to tweak for transfer learning.",
        "A computation graph is simply a specification of how your data is combined to give you the output."
    ],
    "<machine training>": [],
    "<VBN>": [],
    "<sequential inference>": [],
    "<next channel>": [],
    "<Client communication>": [],
    "<total lines>": [],
    "<local directory>": [],
    "<ground truth>": [],
    "<stable number>": [],
    "<plane convolution>": [],
    "<weights array>": [],
    "<Auto correlation>": [],
    "<least rank>": [],
    "<numerical feature>": [
        "Although numerical feature columns model the lengths of petals and sepals effectively, real world data sets contain all kinds of features, many of which are non-numerical."
    ],
    "<one head>": [],
    "<continuous value>": [],
    "<advanced model, advanced models>": [],
    "<split dimension>": [],
    "<summary value, summary values>": [
        "Summary has one summary value containing a histogram for values."
    ],
    "<ideal value>": [
        "Its ideal value varies depending on models to run."
    ],
    "<generic distribution>": [],
    "<multiple measurements>": [],
    "<training percentage>": [
        "I knew training percentage is correct, but couldn't figure out why it's wrong for validation and test."
    ],
    "<probabilistic model>": [],
    "<one partition>": [],
    "<gray image>": [],
    "<UNK, unk>": [
        "Specifies, if UNK word should be used instead of words that are absent in vocab.",
        "This method allows you to specify, if UNK word should be used internally."
    ],
    "<distributed module>": [],
    "<window stride>": [],
    "<left data>": [],
    "<final dimension>": [
        "The final dimension contains the indices of top-k labels.",
        "The final dimension contains the top k predicted class indices.",
        "The final dimension contains the logit values for each class."
    ],
    "<updates array>": [],
    "<training services>": [],
    "<faster training>": [],
    "<Run images>": [],
    "<Optional transform>": [],
    "<space vector, spaced vector>": [],
    "<Loop Nest, loop nest>": [],
    "<anomalous trajectories>": [],
    "<classes Tensor>": [
        "The classes Tensor must provide string labels, not integer class IDs."
    ],
    "<shuffling tensors>": [],
    "<traced function>": [],
    "<model detects>": [],
    "<EC2>": [],
    "<log potentials>": [],
    "<Optional filter>": [],
    "<pooling window>": [],
    "<persistence level>": [],
    "<valid device>": [],
    "<BPTT, bptt>": [
        "This tradeoff is usually worth it, and (as long as the truncated BPTT lengths are set appropriately), truncated BPTT works well in practice.",
        "Truncated backpropagation through time (BPTT) was developed in order to reduce the computational complexity of each parameter update in a recurrent neural network."
    ],
    "<one checkpoint>": [],
    "<one go>": [],
    "<original tensor>": [],
    "<compute time>": [],
    "<disabling gradient>": [],
    "<full update>": [],
    "<one training>": [],
    "<hspmm>": [],
    "<integer vector>": [],
    "<minlength>": [
        "If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros."
    ],
    "<Jozefowicz>": [],
    "<Gram training>": [],
    "<classifier tutorial>": [],
    "<output range>": [],
    "<dense feature, dense features>": [],
    "<NAS cell>": [],
    "<evaluation step, evaluation steps>": [],
    "<unit variance>": [],
    "<resnet50>": [],
    "<latent vector>": [
        "The strided conv-transpose layers allow the latent vector to be transformed into a volume with the same shape as an image."
    ],
    "<Classification logits>": [],
    "<Forums distributed>": [],
    "<Bahdanau>": [],
    "<AUPRC>": [],
    "<Summary Tensor, summary Tensor>": [],
    "<input channel, input channels>": [],
    "<fast solution>": [],
    "<tapering window>": [],
    "<max norm, max norms>": [],
    "<extract data>": [],
    "<internal tensor>": [
        "It is useful to describe three other kind of tensors: internal: an internal tensor is a tensor connecting operations contained in the subgraph."
    ],
    "<Zero gradients, zero gradient, zero gradients>": [],
    "<total shift>": [
        "If the same axis is referenced more than once, the total shift for that axis will be the sum of all the shifts that belong to that axis."
    ],
    "<distributed network>": [],
    "<training begins>": [],
    "<separate GPU>": [],
    "<GPU usage>": [],
    "<sample likelihood>": [],
    "<time difference>": [],
    "<final evaluation>": [],
    "<Model Results, model results>": [],
    "<Real Images, Real images, real image, real images>": [],
    "<device scope>": [],
    "<sequence label, sequence labels>": [],
    "<inner sum>": [],
    "<affine matrices>": [],
    "<maximum norm>": [],
    "<cluster modes>": [],
    "<gradient outputs>": [],
    "<kernel function>": [],
    "<connected Network, connected network>": [],
    "<dimensional outputs>": [],
    "<identity values>": [],
    "<multiple axes>": [
        "Multiple shifts along multiple axes may be specified."
    ],
    "<unknown shape>": [
        "During execution any unknown shape dimensions are determined dynamically, see Tensor Shapes for more details.",
        "Unknown shape: has an unknown number of dimensions, and an unknown If a tensor is produced by an operation of type \"Foo\", its shape may be inferred if there is a registered shape function for \"Foo\"."
    ],
    "<input details>": [],
    "<execution turned>": [],
    "<distributed inputs>": [],
    "<spatial transformation>": [],
    "<Base distribution, base distribution>": [
        "The base distribution for this operation is constructed on the fly during training.",
        "The transform is typically an instance of the Bijector class and the base distribution is typically an instance of the Distribution class.",
        "If provided, base distribution's prob should be defined at low.",
        "The base distribution is not saved to checkpoints, so it is reset when the model is reloaded.",
        "The base distribution is read from a file or passed in as an in-memory array."
    ],
    "<standard gradient>": [],
    "<bottom pixel>": [],
    "<feature cross>": [
        "This feature cross enables the model to train on pricing conditions related to each individual section, which is a much stronger signal than latitude and longitude alone."
    ],
    "<doc count>": [],
    "<standard analysis>": [],
    "<morphological dilation>": [],
    "<data meaning>": [],
    "<addbmm, addmm, addmv>": [],
    "<Tensor vector>": [],
    "<Tensor Along, Tensor along, tensor along>": [
        "Tensor along dimension is a powerful technique, but can be a little hard to understand at first."
    ],
    "<accuracy calculation>": [],
    "<matrix implied>": [],
    "<Autograd, autograd>": [
        "Autograd is reverse automatic differentiation system.",
        "Recall that Function s are what autograd uses to compute the results and gradients, and encode the operation history.",
        "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.",
        "Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU.",
        "Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors."
    ],
    "<clip norm>": [],
    "<Complete Model, complete model>": [],
    "<reduce product>": [],
    "<Adamax>": [
        "Adamax is sometimes superior to adam, specially in models with embeddings, see Kingma et al., 2014 (pdf)."
    ],
    "<view layer>": [],
    "<zeros left>": [],
    "<output histogram>": [],
    "<time inference>": [],
    "<normalizing constant>": [],
    "<Device index, device index>": [],
    "<rightmost dimension, rightmost dimensions>": [
        "The rightmost dimensions of a value to be scored via these methods must agree with the distribution's batch and event shapes."
    ],
    "<training procedure>": [],
    "<step Step, step step>": [],
    "<False Positive, False positive, false positive, false positives>": [
        "Get the false positives count for the specified output."
    ],
    "<Loading Model, Loading Models, loading model, loading models>": [],
    "<Deep Learning, Deep learning, deep learning>": [
        "Deep learning is the de facto standard for face recognition.",
        "While handwriting recognition has been attempted by different machine learning algorithms over the years, deep learning performs MNIST dataset.",
        "Publisher's note: Deep Learning with Python introduces the field of deep Keras creator and Google AI researcher Fran\u00e7ois Chollet, this book builds your understanding through intuitive explanations and practical examples.",
        "Deep learning consists of composing linearities with non-linearities in clever ways.",
        "Sometimes, deep learning is just one piece of the whole project.",
        "Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.",
        "Before deep learning was used, other techniques like dynamic time warping were used for measuring similarity between sequences.",
        "All deep learning is based on vectors and tensors, and DL4J relies on a tensor library called ND4J.",
        "Furthermore, some deep learning distributions may need to be specifically compiled to provide support for hardware features such as AVX2 (note that recent version of ND4J are packaged with binaries for CPUs that support these features)."
    ],
    "<sampling probability>": [],
    "<ccache>": [
        "How ccache is used to speed up build times."
    ],
    "<sparse vector, sparse vectors>": [],
    "<huge gradients>": [],
    "<tower device>": [],
    "<remote task>": [],
    "<GCP>": [],
    "<Mean Absolute, mean absolute>": [],
    "<output masking>": [
        "- Per output masking: Where each output activation value is present or not - mask shape [n,c,h,w] (same as output)."
    ],
    "<covariance type>": [
        "Raises: Exception if covariance type is unknown."
    ],
    "<replica training>": [],
    "<training checkpoint, training checkpoints>": [],
    "<dimenional array>": [],
    "<fixed time>": [],
    "<semantic name>": [],
    "<blank labels>": [],
    "<zero weight>": [],
    "<Nadam>": [],
    "<GPU application, GPU applications>": [],
    "<Spectral operators>": [],
    "<matrix logarithm>": [],
    "<gamma parameter>": [],
    "<evaluation workers>": [],
    "<mean magnitudes>": [],
    "<main channel>": [],
    "<column transform>": [],
    "<Identification System>": [
        "Marine Automatic Identification System (AIS) is an open system for marine broadcasting of positions."
    ],
    "<Float column, Float columns, float column>": [],
    "<interval level>": [
        "alpha: Confidence interval level desired."
    ],
    "<forward function, forward functions>": [],
    "<VGG11, vgg11>": [
        "VGG11 performed best set at 1ms."
    ],
    "<GPU operations>": [
        "GPU operations are called in an asynchronously way, so that they are not blocking your CPU.",
        "By default, GPU operations are asynchronous."
    ],
    "<identity operator>": [],
    "<colorbar>": [],
    "<functor>": [],
    "<Identity matrix, identity matrices, identity matrix>": [
        "May have shape [B1, ..., Bb, r], b >= 0, and characterizes b-batches of r x r None, an identity matrix is used inside the perturbation."
    ],
    "<initial model>": [],
    "<Conda, cond, conda>": [],
    "<sequence step, sequence steps>": [],
    "<Pearson Correlation>": [],
    "<read tensor>": [],
    "<single conditions>": [],
    "<timing data>": [],
    "<CPU version>": [],
    "<Run Training, Run training, run training, running training, runs training>": [],
    "<scalar element>": [],
    "<integer label, integer labels>": [],
    "<Gaussian mixture, gaussian mixture>": [
        "Therefore by minimizing the negative log likelihood, we get to a position of highest probability that the gaussian mixture explains the phenomenon."
    ],
    "<RSE>": [],
    "<Adversarial training, adversarial training>": [],
    "<discriminator steps>": [],
    "<original device>": [],
    "<Basic sampling>": [],
    "<physical checkpoint>": [],
    "<Calculate padding>": [],
    "<classification network>": [],
    "<second batch>": [
        "First and second batch of matrices have to be arrays of same length and each pair taken from these sets has to have dimensions (M, N) and (N, K), respectively."
    ],
    "<Random array, random array>": [],
    "<Linear Regression, Linear regression, linear regression>": [],
    "<input filters>": [],
    "<diag>": [],
    "<Batch Normalization, Batch norm, Batch normalization, batch norm, batch normalization, batch normalizations, batch norms>": [
        "Fused batch norm combines the multiple operations needed to do batch normalization into a single kernel.",
        "If present, then the batch normalization uses weighted mean and variance.",
        "Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and This layer uses statistics computed from input data in both training and evaluation modes.",
        "Because the Batch Normalization is done over the `C` dimension, computing statistics on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.",
        "Batch norm is an expensive process that for some models makes up a large percentage of the operation time.",
        "In many cases, the inference graph will be different from the training graph: for example, techniques like dropout and batch normalization use different operations in each case."
    ],
    "<systemd>": [],
    "<Model quality>": [
        "Model quality can be very sensitive to these parameters."
    ],
    "<border values>": [],
    "<Scaling factor, scaling factor>": [],
    "<variable window>": [],
    "<consistency loss>": [],
    "<negative example>": [],
    "<training looks>": [],
    "<basically learning>": [],
    "<squeeze operation>": [
        "When dim is given, a squeeze operation is done only in the given dimension."
    ],
    "<absolute error>": [],
    "<input names>": [
        "The input name should be based on a tensorflow type or onnx type, not the nd4j name."
    ],
    "<adversarial classification>": [],
    "<vector length>": [],
    "<corresponding Tensor, corresponding tensor, corresponding tensors>": [],
    "<RMSE, root mean squared error>": [],
    "<self Tensor, self tensor>": [
        "For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor.",
        "self tensor and the given tensor must be broadcastable.",
        "If source is a tensor, self tensor will share the same storage and have the same size and strides as source.",
        "Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]."
    ],
    "<regression problem, regression problems>": [],
    "<Kronecker product>": [],
    "<averaged dimensions>": [],
    "<binary case>": [],
    "<constant development>": [],
    "<loop iterations>": [],
    "<precision array>": [],
    "<inference features>": [],
    "<arrow columns>": [],
    "<known shape>": [
        "Partially-known shape: has a known number of dimensions, and an unknown size for one or more dimension."
    ],
    "<sentence labels>": [
        "Label template will be used for sentence labels generation."
    ],
    "<terminal state>": [],
    "<Data dependencies, data dependencies, data dependency, data dependent>": [
        "XLA ignores control dependencies and so this data dependency is necessary.",
        "Data dependencies show the flow of tensors between two ops and are shown as solid arrows, while control dependencies use dotted lines."
    ],
    "<Dual Optimization>": [],
    "<Caffe2>": [],
    "<plain mode>": [
        "Plain mode is to be used when the number of nodes in the cluster are < 32 nodes and mesh mode is to be used for larger clusters."
    ],
    "<orthogonal columns>": [],
    "<Sequential module>": [
        "A Sequential module simply performs function composition."
    ],
    "<Tensor diagonal>": [],
    "<moving average, moving averages>": [
        "This makes moving averages move faster.",
        "The moving averages are computed using exponential decay."
    ],
    "<closest bound>": [],
    "<text corpora>": [],
    "<Static Graphs, static graph>": [],
    "<Activation Layer, Activation Layers, Activation layer>": [
        "Activation layer is a simple layer that applies the specified activation function to the input activations.",
        "Activation Layer Used to apply activation on input and corresponding derivative on epsilon."
    ],
    "<output dependencies>": [],
    "<wrong shape>": [],
    "<convolution weight>": [],
    "<Layer Normalization, Layer normalization, layer normalization>": [
        "Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and This layer uses statistics computed from input data in both training and evaluation modes."
    ],
    "<feature data>": [],
    "<final variable>": [],
    "<binary potentials>": [],
    "<Higher ranks, higher rank>": [
        "higher rank input), computes the top k entries in each row (resp."
    ],
    "<single spatial>": [],
    "<average gradient>": [],
    "<tuple elements>": [],
    "<corresponding dimension, corresponding dimensions>": [],
    "<interactive node>": [],
    "<bias shape>": [],
    "<maximum vocabulary>": [],
    "<bucket values>": [],
    "<calculation early>": [],
    "<dy1>": [],
    "<attention distribution, attention distributions>": [],
    "<permuted dimensions>": [],
    "<trainer program>": [],
    "<sequence architectures>": [],
    "<VAE>": [],
    "<Upsampling3D>": [],
    "<output sentence>": [],
    "<invalid inputs>": [
        "When False invalid inputs may silently render incorrect outputs.",
        "DL4J Exception thrown when invalid input is provided (wrong rank, wrong size, etc)."
    ],
    "<tcas>": [],
    "<ngrams>": [],
    "<permutation operation>": [
        "Array permutation operation: permute the dimensions according to the specified permutation indices."
    ],
    "<Hessian matrices, Hessian matrix>": [],
    "<pretrained model, pretrained models>": [],
    "<Input shape, input shape, input shapes, inputs shape, inputs shapes>": [
        "If input shapes have rank-R, then output shape will have rank-(R+1).",
        "Note that for convolutional models, input shape information follows the NCHW convention."
    ],
    "<original column, original columns>": [
        "Note that the original column is removed in the process."
    ],
    "<entire layer>": [
        "the entire layer graph is retrievable from that layer, recursively."
    ],
    "<True Model>": [],
    "<new axis>": [
        "The new axis is created at dimension axis (default: the new axis is appended at the end)."
    ],
    "<bijective>": [],
    "<Conversion Transforms, conversion transform>": [],
    "<input resolution>": [
        "If the input resolution to the model is fixed and known, this may be set."
    ],
    "<computing multilinear>": [],
    "<single kernel>": [],
    "<Kryo>": [],
    "<Reliability diagram, reliability diagram>": [],
    "<batch variable>": [],
    "<negative samples>": [],
    "<tuple1>": [],
    "<dense component>": [],
    "<IDFT>": [],
    "<generating layer>": [],
    "<pandas data>": [],
    "<good start>": [],
    "<local Translation>": [],
    "<lengths tensor>": [],
    "<one computer>": [],
    "<training progression>": [],
    "<Spark distribution>": [
        "Note that Spark submit is a script that comes with a Spark distribution that users submit their job (in the form of a JAR file) to, in order to begin execution of their Spark job."
    ],
    "<dynamic graph>": [],
    "<Sigmoid loss, sigmoid loss>": [],
    "<sequences left>": [],
    "<Layer Activations>": [],
    "<generative chatbot>": [],
    "<Markdown, markdown>": [],
    "<computation history>": [],
    "<Plz, plz>": [],
    "<TRAIN mode, Trained Model, train mode, train model, train models, trained model, trained models>": [
        "The trained model can then be used as an application resource."
    ],
    "<symmetric window>": [],
    "<model producers>": [],
    "<negative edge>": [],
    "<shape equal>": [],
    "<input summaries>": [],
    "<gradient Methods, gradient methods>": [],
    "<GPU trainer>": [],
    "<sampling distribution>": [],
    "<learnable temperature>": [
        "If None, a learnable temperature is created."
    ],
    "<simple training>": [],
    "<Randomly sample, randomly sample>": [],
    "<argsort>": [],
    "<generating time>": [],
    "<metric names>": [],
    "<Mixed Precision, mixed precision>": [],
    "<Estimators program>": [],
    "<aeron context>": [],
    "<training objective>": [],
    "<zero determinant>": [],
    "<left memory>": [],
    "<varying order>": [],
    "<SVD, singular value decomposition, svd>": [
        "SVD decomposiiton of a matrix The decomposition is such that: A = U x S x VT L will be the same dimensions as A.",
        "Analogously, the SVD on GPU uses the MAGMA routine gesdd as well."
    ],
    "<Keras function>": [],
    "<nonlinear models>": [],
    "<train steps>": [],
    "<Poisson, poisson>": [
        "Base Poisson parameterized by a quadrature grid."
    ],
    "<Instance Type, Instance type, instance type>": [],
    "<layer Input, layer input, layer inputs>": [
        "This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs."
    ],
    "<arbitrary tensors>": [],
    "<GDN, gdn>": [],
    "<GPU, GPUS, GPUs, Gpu, gpu, gpus>": [
        "The GPUs have to wait for that CPU to finish.",
        "GPU operations are called in an asynchronously way, so that they are not blocking your CPU.",
        "In a workstation with multiple GPU cards, each GPU will have similar speed and contain enough memory to run an entire CIFAR-10 model.",
        "Note, a GPU with CUDA is not critical for this tutorial as a CPU will not take much time.",
        "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default.",
        "Below are some other Check if a GPU is underutilized by running nvidia-smi -l 2.",
        "When GPU RAM is less than CPU RAM, you need to monitor how much RAM is being used off-heap.",
        "On the server there are 8 GPU installed and they are all the same type 1080Ti.",
        "As GPUs and other hardware accelerators get faster, preprocessing of data can be a bottleneck.",
        "If no GPUs are available, then the model is going to be placed on the CPU.",
        "When each GPU completes the computation, gradients are going to be reduced (added) onto the master GPU.",
        "By default, GPU operations are asynchronous.",
        "If you do so, your GPU will run out of RAM when trying to run jobs.",
        "GPUs are run in their default Boost.",
        "In this design, each GPU on the server has its own copy of each variable.",
        "GPUs and a single aggregated gradient is sent to the parameter server.",
        "The GPUs are synchronized in operation.",
        "While the process is running, the GPU has still 80% memory blocked and pytorch is using this space.",
        "This setup requires that all GPUs share the model parameters.",
        "Update model parameters synchronously by waiting for all GPUs to finish Here is a diagram of this model: Note that each GPU computes inference as well as the gradients for a unique batch of data.",
        "If JavaCPP or your GPU throw an out-of-memory error (OOM), or even if your compute slows down due to GPU memory being limited, then you may want to either decrease batch size or increase the amount of off-heap memory that JavaCPP is allowed to allocate, if that's possible.",
        "This utilty and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend.",
        "Get device (GPU, etc) current bytes - may be null if no compute devices are present in the system.",
        "While the GPU is busy your multiple workers can load a new batch of images instead of a large file.",
        "After the GPU finishes it can just get the new batch, which is probably already waiting for it.",
        "These are used at each layer for doing things like normalization and NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST).",
        "For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won't be enough for modern deep learning.",
        "If GPU utilization is not approaching 80-100%, then the input pipeline may be the bottleneck.",
        "Note that values set here on the layer will be applied to all relevant layers - unless the value is overridden Note: values set by this method will be applied to all applicable layers in the network, unless a different Note: values set by this method will be applied to all applicable layers in the network, unless a different NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST)."
    ],
    "<pacman>": [],
    "<DOT format, dot format>": [],
    "<vgg13>": [],
    "<embedding dimension, embedding dimensions>": [],
    "<sparse data>": [],
    "<separable filters>": [],
    "<compute norms>": [],
    "<reduction ratio>": [
        "Fractional max pooling, as you might expect from the word \"fractional\", means that the overall reduction ratio N does not have to be an integer."
    ],
    "<inverted dropout>": [],
    "<low accuracy>": [],
    "<accumulation operation>": [],
    "<Complex128, complex128>": [
        "Complex128 elements must be written as two consecutive DOUBLE values, real component first."
    ],
    "<final name>": [],
    "<multiple images>": [],
    "<gradient implementation>": [],
    "<libgomp>": [],
    "<bias variable, biases variable>": [],
    "<accordion element>": [],
    "<hot vector, hot vectors>": [],
    "<labels nearest>": [],
    "<actual lengths>": [],
    "<transform data, transformed data>": [],
    "<building process>": [],
    "<fundamental unit>": [],
    "<full variable>": [],
    "<isinf>": [],
    "<gradient values>": [],
    "<maximum suppression>": [],
    "<trivial examples>": [],
    "<adjusted image>": [],
    "<nnz>": [],
    "<binary prediction>": [],
    "<final prediction>": [],
    "<Residual plot, residual plot>": [],
    "<ELU, ELUs, Exponential Linear Unit, Exponential Linear Units, elu>": [],
    "<loading code>": [],
    "<tensor pool>": [],
    "<triplet loss>": [
        "a triplet loss architecture is that a center loss layer stores its own parameters.",
        "Center loss is similar to triplet loss except that it enforces intraclass consistency and doesn't require feed forward of multiple examples."
    ],
    "<hot Tensor, hot tensor>": [],
    "<multiple models, multiple modes>": [],
    "<Korrine>": [
        "Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad."
    ],
    "<btrifact>": [],
    "<performance distribution>": [],
    "<termination tolerance>": [],
    "<single scalar>": [],
    "<trop>": [],
    "<computation leads>": [],
    "<perform operation>": [],
    "<Wishart distribution, Wishart distributions>": [],
    "<output arguments, outputs argument>": [],
    "<symmetric padding>": [
        "- If int: the same symmetric padding is applied to height and width."
    ],
    "<predictions parameter>": [],
    "<model pruning>": [],
    "<recognition network>": [],
    "<linear memory>": [],
    "<time frame>": [],
    "<normalized copy>": [],
    "<model accuracy>": [],
    "<variable indices>": [],
    "<stable results>": [],
    "<Pathwise, pathwise>": [],
    "<distributed inference>": [
        "This distributed inference can also be used for networks trained on a single machine and loaded for Spark (see the the saving/loading section for details on how to load a saved network for use with Spark)."
    ],
    "<computational efficiency>": [],
    "<orthonormal>": [],
    "<Sparse gradients, sparse gradient, sparse gradients>": [
        "A sparse gradient is represented by its indices, values and possibly empty or None shape."
    ],
    "<time update, tu>": [],
    "<Max operation, max operation>": [
        "This operation applies Max operation to specific inputs through given indices Expected arguments: input: array to be updated indices: array containing indexes for first dimension of input updates: array containing elements to be interfered with input."
    ],
    "<individual dimension, individual dimensions>": [],
    "<PEP8>": [],
    "<parameter groups>": [],
    "<NCHW>": [
        "NHWC (default) and NCHW are supported.",
        "On GPU, NCHW is faster.",
        "'NHWC' and 'NCHW' are supported.",
        "NCHW should always be used when training with GPUs."
    ],
    "<vector operation, vector operations>": [
        "Elementwise scalar multiplication looks like this: And produces this: Subtraction and division follow a similar pattern: If you perform all these operations on your initial 2 x 2 matrix, you should end up with this matrix: Elementwise vector operations."
    ],
    "<Dense Tensor, Dense tensor, dense Tensor, dense Tensors, dense tensor, dense tensors>": [],
    "<trtrs>": [],
    "<headed models>": [
        "In a multi-headed model, each head is represented by an entry in this dict.",
        "Single-headed models only need to specify one entry in this dictionary."
    ],
    "<dimension number>": [
        "By convention, dimensions are listed in increasing order of dimension number.",
        "The lowest dimension number in dimensions is the slowest varying dimension (most major) in the loop nest which collapses these dimension, and the highest dimension number is fastest varying (most minor).",
        "Example with contracting dimension numbers: Associated batch dimension numbers from the 'lhs' and 'rhs' must have the same dimension number, must be listed in the same order in both arrays, must have the same dimension sizes, and must be ordered before contracting and non-contracting/non-batch dimension numbers."
    ],
    "<domain prediction>": [],
    "<repo, repro>": [
        "The repo includes a complete program for experimenting with this type of model."
    ],
    "<detection model>": [],
    "<DFT2>": [],
    "<running sample>": [],
    "<output training>": [],
    "<circulant matrix>": [],
    "<Frechet>": [],
    "<level systems>": [],
    "<Input Tensor, Input tensor, Input tensors, input Tensor, input Tensors, input tensor, input tensors, inputs Tensors, inputs tensor, inputs tensors>": [
        "The input tensor is treated as if it were viewed as a 1-D tensor.",
        "Therefore, the input tensor in the tensor list needs to be GPU tensors.",
        "The shapes of the mask tensor and the input tensor don't need Note.",
        "When all the input tensors are finished, the output tensor is passed along in the graph.",
        "The input tensors real and imag must have the same shape.",
        "If this is a scalar, then the components input tensors should not have a prepended batch dimension.",
        "Output tensor has one more dimension than input tensor, the first dimension indicates the partition.",
        "If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",
        "Stacks n input tensors along provided axis.",
        "For instance, some input tensor can be omitted.",
        "The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.",
        "The 4-D input tensor is treated as a 3-D array of 1-D vectors (along the last dimension), and each vector is normalized independently.",
        "If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",
        "The input tensor should be a tensor containing probabilities to be used for drawing the binary random number.",
        "If input has n dimensions, then the resulting indices tensor out is of size \\((z \\times n)\\), where \\(z\\) is the total number of non-zero elements in the input tensor.",
        "If specified, the input tensor is casted to :attr:`dtype` before the operation is performed.",
        "Currently, only 4-D input tensors (batched image-like tensors) are supported.",
        "This function will modify the tensors passed in as it adds more operations and hence changing the consumers of the operations of the input tensors.",
        "Repeat (tile) the input tensor the specified number of times.",
        "To help improve the results of our model, we also apply dropout regularization to our dense layer, using the dropout method in layers: Again, inputs specifies the input tensor, which is the output tensor from our dense layer (dense).",
        "Our input tensor range is then [-m, m].",
        "Input tensor dimensions (batch, height, width, channels) are expected to be known at graph construction time."
    ],
    "<activated feature>": [
        "If you try to feed the networks with 0 to 255 tensor images, then the activated feature maps will be unable sense the intended content and style."
    ],
    "<Numerical optimization>": [],
    "<Cifar data>": [],
    "<delayed weights>": [],
    "<respective shape>": [],
    "<computing iterations>": [],
    "<mean intersection>": [],
    "<type distribution>": [],
    "<data exploration>": [],
    "<maximum device>": [],
    "<GPU Memory, GPU memory, gpu memory>": [
        "My GPU memory isn't freed properly\u00b6.",
        "the cifar10 training example with -opencl, the GPU memory usage grows at a rate of 200MB per second (looking at nvidia-smi) until the lack of memory crashes the program.",
        "If JavaCPP or your GPU throw an out-of-memory error (OOM), or even if your compute slows down due to GPU memory being limited, then you may want to either decrease batch size or increase the amount of off-heap memory that JavaCPP is allowed to allocate, if that's possible.",
        "I found that after I passed the output of A to B, the GPU memory increases 4 GB.",
        "If your GPU memory isn't freed even after Python quits, it is very likely that some Python subprocesses are still alive.",
        "These are used at each layer for doing things like normalization and NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST).",
        "Note that values set here on the layer will be applied to all relevant layers - unless the value is overridden Note: values set by this method will be applied to all applicable layers in the network, unless a different Note: values set by this method will be applied to all applicable layers in the network, unless a different NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST)."
    ],
    "<Hidden Layers, Hidden layers, hidden layer, hidden layers>": [
        "No hidden layer should be less than a quarter of the input layer's nodes.",
        "Hidden layers can learn features from the input layer and it can send those features to be analyzed by our output layer to get the corresponding outputs.",
        "In theory, this context vector (the final hidden layer of the RNN) will contain semantic information about the query sentence that is input to the bot."
    ],
    "<linear unit>": [],
    "<RMS, rmsprop>": [],
    "<select tensors, selected tensor>": [],
    "<GGT>": [
        "GGT has an advantage over sgd and adam on large models with poor conditioning, for example language models and CNNs, see [ABCHSZZ 2018]."
    ],
    "<initial one, initial ones>": [],
    "<diagonal part>": [],
    "<model periodically>": [],
    "<sub tensors>": [],
    "<column equal>": [],
    "<Scalar Value, Scalar value, scalar value, scalar valued, scalar values>": [],
    "<Cropping3D>": [],
    "<ML algorithms>": [],
    "<vector one>": [],
    "<Load Training, load training>": [],
    "<dimension length>": [],
    "<complex frequency>": [],
    "<integer distributions>": [],
    "<via list>": [],
    "<SQSS>": [
        "Internally, SQSS has a queue for the input examples."
    ],
    "<execute operation>": [],
    "<model creation>": [],
    "<one values>": [],
    "<full batch>": [],
    "<tensor length>": [
        "Suppose tensor length is n, there are d devices and g gather shards."
    ],
    "<next states>": [],
    "<evaluation networks>": [],
    "<correct name>": [],
    "<Stabilization function>": [],
    "<positive definite>": [],
    "<scalar threshold>": [],
    "<autoregressive>": [
        "Regarding terminology, \"Autoregressive models decompose the joint density as a product of conditionals, and model each conditional in turn."
    ],
    "<policy network>": [],
    "<perturbed image>": [
        "The resulting perturbed image, \\(x'\\), is then misclassified by the target network as a \"gibbon\" when it is still clearly a \"panda\"."
    ],
    "<performs function>": [],
    "<pre train, pre training>": [
        "Whether to do pre train or not."
    ],
    "<backward graph>": [],
    "<Bessel function>": [],
    "<reduced array>": [],
    "<survival function>": [],
    "<optional float>": [],
    "<loss reported>": [
        "loss: The loss reported."
    ],
    "<filename1>": [],
    "<executing training>": [],
    "<controls number>": [],
    "<ref tensor>": [
        "Initialize 'ref' with all zeros, ref tensor should be uninitialized."
    ],
    "<truncated normal>": [],
    "<Stochastic gradient, stochastic gradient>": [],
    "<reduction algo>": [],
    "<NLTK>": [],
    "<Square function, square function>": [],
    "<multiple cores>": [],
    "<try training>": [],
    "<log factorial>": [],
    "<weight column>": [],
    "<inner layer>": [],
    "<model robust>": [],
    "<repeat transformation>": [
        "If the repeat transformation is applied before the shuffle transformation, then the epoch boundaries are blurred."
    ],
    "<precompute>": [],
    "<CUDNN, CuDNN, CuDNNGRU, Cudnn, cuDNN, cudnn>": [
        "Right now, this works only if the module is on the GPU and cuDNN is enabled."
    ],
    "<Adaptive Probabilities>": [],
    "<broadcastable dimension>": [],
    "<auxiliary entropy>": [],
    "<curve type>": [],
    "<Jacq, jacq>": [],
    "<manual variable>": [],
    "<SVM, SVMs, svm>": [],
    "<pair mean>": [],
    "<affine map>": [],
    "<initial startup>": [],
    "<numel>": [],
    "<fatal signal>": [],
    "<parameter loc>": [],
    "<small effect>": [],
    "<fast response>": [],
    "<linear scaling>": [],
    "<fail training>": [],
    "<temporal slice>": [],
    "<model parallelism>": [],
    "<constant rank>": [],
    "<dropout technique>": [],
    "<Inception distance>": [],
    "<actual machine>": [],
    "<Conjugate Gradient>": [
        "Only applies for line search optimizers: Line Search SGD, Conjugate Gradient, LBFGS is NOT applicable for standard SGD."
    ],
    "<Hamming distance, hamming distance>": [],
    "<Tensor operations, Tensor operator, tensor operation, tensor operations>": [
        "Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history."
    ],
    "<generalized version>": [],
    "<batches parameter>": [],
    "<Input Pre, Input pre, input pre>": [],
    "<Implementation Model, Implementation mode>": [],
    "<trained checkpoints>": [],
    "<Inception model>": [],
    "<autogenerated>": [],
    "<hyperthreading>": [],
    "<Basic loop>": [],
    "<accelerated computing>": [],
    "<random Shard, random shard>": [],
    "<clustering loss>": [],
    "<example graph>": [],
    "<Calculate gradients, calculate gradients, calculated gradients, calculating gradients>": [],
    "<Max Pooling, Max pooling, max pooling>": [
        "Fractional max pooling is slightly different than regular max pooling.",
        "axis: The dimension where max pooling will be performed.",
        "Global Max pooling operation for 3D data."
    ],
    "<chatbot model>": [
        "The model that we will convert is the chatbot model from the Chatbot tutorial."
    ],
    "<tensor variable>": [],
    "<dimensional case>": [],
    "<inverse functions>": [
        "From the bijective, nonzero differentiability of g, the inverse function theorem implies g^{-1} is differentiable in the image of g."
    ],
    "<num1>": [],
    "<output image>": [
        "If false or omitted, make the output image the same size as the input image.",
        "For gamma less than 1, the histogram will shift towards right and the output image will be brighter than the input image."
    ],
    "<Compute activations, compute activations>": [],
    "<standalone model>": [],
    "<input network>": [],
    "<processing units>": [],
    "<output variable>": [
        "The output variable will have the same values as the input, but with the specified shape."
    ],
    "<Woodbury matrix>": [],
    "<weighted matrix>": [],
    "<dependency graph>": [],
    "<Cropping layer, cropping layer>": [],
    "<nuclear norm>": [],
    "<celeba directory>": [],
    "<length rank>": [],
    "<Output Shape, Output shape, output shape, output shapes>": [
        "The output shape is the same as the input shape.",
        "In terms of this constant, the output shape is.",
        "If input shapes have rank-R, then output shape will have rank-(R+1).",
        "If indices is a scalar the output shape will be a vector of length depth.",
        "The parameters \\(\\mu\\) and \\(\\sigma\\), and output shape have to have a floating point elemental type.",
        "Output shape is [1, height, width, channels]."
    ],
    "<next data>": [],
    "<tensor details>": [],
    "<step Tensor>": [],
    "<example number>": [],
    "<running tally>": [],
    "<Gaussians>": [],
    "<OMP>": [],
    "<embedding results>": [],
    "<Generate weights>": [],
    "<Output lists>": [
        "Note: output list includes the original input."
    ],
    "<Matrix multiply, matrix multiplies, matrix multiply>": [],
    "<training flag>": [],
    "<shape Tensor, shape tensor, shape tensors, shaped Tensor>": [],
    "<label binary>": [],
    "<Nesterov, nesterov>": [],
    "<norm problem>": [],
    "<Outer product, outer product>": [],
    "<model based>": [
        "Builds the model based on input shapes received."
    ],
    "<binary vector>": [],
    "<pooling filter>": [],
    "<optimal batches>": [],
    "<device type>": [],
    "<stochastic approximation>": [],
    "<computation results>": [],
    "<data copy>": [],
    "<Discounting factor>": [],
    "<cumulative probability>": [],
    "<natural parameters>": [],
    "<input dimension, input dimensions>": [
        "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width."
    ],
    "<unique graph>": [],
    "<multiple sessions>": [],
    "<based False>": [],
    "<learned array>": [],
    "<modulus operation>": [],
    "<larger checkpoint>": [],
    "<Blocking point>": [],
    "<input dropout>": [],
    "<CudnnRNN>": [],
    "<Poisson distribution, poisson distribution>": [
        "The poisson distribution is defined over the integers."
    ],
    "<calculate sum>": [],
    "<Shape functions, shape function, shape functions>": [
        "Since shape inference is an optional feature, and the shapes of tensors may vary dynamically, shape functions must be robust to incomplete shape information for any of the inputs.",
        "Unknown shape: has an unknown number of dimensions, and an unknown If a tensor is produced by an operation of type \"Foo\", its shape may be inferred if there is a registered shape function for \"Foo\"."
    ],
    "<decay function>": [
        "This function applies a cosine decay function to a provided initial learning rate."
    ],
    "<Computational processes, computational process>": [
        "Computational processes are abstract beings that inhabit computers."
    ],
    "<linear approximation>": [],
    "<multiple features>": [],
    "<shape data>": [],
    "<DCGAN>": [
        "A DCGAN is a direct extension of the GAN described above, except that it explicitly uses convolutional and convolutional-transpose layers in the Radford et."
    ],
    "<Data Loading, Data loading, data loading>": [],
    "<nested parameter>": [
        "Note that leaf parameters are parameters that do not have any nested parameter spaces."
    ],
    "<Hermitian matrix>": [],
    "<x1>": [],
    "<likelihood loss>": [],
    "<one ground>": [],
    "<manip>": [],
    "<Location parameter, location parameter>": [],
    "<Weighted sum, weighted sum>": [
        "\"mean\" is the weighted sum divided by the total weight.",
        "\"sum\" computes the weighted sum of the embedding results for each row.",
        "Weighted sum refers to logits in classification problems."
    ],
    "<Scale parameter, scaling parameters>": [],
    "<HWC>": [],
    "<weight argument, weights argument>": [],
    "<weight rows>": [],
    "<Static shapes, static Shape, static shape>": [
        "If None (default), static shape information for batch sizes is omitted."
    ],
    "<rounded numbers>": [],
    "<neither name>": [],
    "<final network>": [],
    "<column count>": [],
    "<Input Node, input node, input nodes>": [],
    "<applying layer>": [],
    "<cube function>": [],
    "<training take>": [],
    "<output labels>": [],
    "<resulting directory>": [],
    "<predictions negative>": [],
    "<distance bound>": [],
    "<binned values>": [],
    "<TPU device, TPU devices>": [
        "tasks is the number of tasks in the TPU cluster, devices is the number of TPU devices per task, and axis is the number of axes in the TPU cluster topology."
    ],
    "<triu>": [],
    "<extra step>": [],
    "<database type>": [],
    "<weights data>": [],
    "<normalized model>": [],
    "<exact inverse>": [],
    "<Dot layer>": [],
    "<GPU location>": [],
    "<Binary model>": [],
    "<output index>": [
        "Find the output index corresponding to given output tensor t.",
        "For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim."
    ],
    "<gradient computed, gradients computed>": [],
    "<outcome matrices, outcome matrix>": [],
    "<auxiliary name>": [],
    "<model attempts>": [],
    "<popularity nodes>": [
        "MAXIMUM: top-popularity nodes will be considered.",
        "MINIMUM: low-popularity nodes will be considered."
    ],
    "<calculating top>": [],
    "<atol>": [],
    "<atan, atanh>": [],
    "<Input pipeline, input pipeline, input pipelines>": [
        "Determining if the input pipeline is the bottleneck can be complicated.",
        "If the input pipeline is shared between training and validation, restoring the checkpoint during validation may override the validation input pipeline.",
        "To make the image processing pipeline easier to explain, assume that the input pipeline is targeting 8 GPUs with a batch size of 256 (32 per GPU).",
        "This hook saves the state of the iterators in the Graph so that when training is resumed the input pipeline continues from where it left off.",
        "Before the model starts running all the stages, the input pipeline stages are warmed up to prime the staging buffers in between with one set of data.",
        "If the difference in examples per second for the full model and the trivial model is minimal then the input pipeline is likely a bottleneck.",
        "Example of checkpointing the training pipeline: This hook should be used if the input pipeline state needs to be saved separate from the model checkpoint.",
        "If GPU utilization is not approaching 80-100%, then the input pipeline may be the bottleneck.",
        "The input pipeline checkpoint may be large, if there are large shuffle or prefetch buffers for instance, and may bloat the checkpoint size."
    ],
    "<running example>": [],
    "<Classifier Calibration>": [],
    "<discretized>": [],
    "<Regularizer, Regularizers, regularizer, regularizers>": [
        "Similarly, if the regularizer is None (the default), the default regularizer passed in the variable scope will be used (if that is None too, then by default no regularization is performed).",
        "regularizers module: Built-in regularizers."
    ],
    "<loop derivative>": [],
    "<overall amount>": [],
    "<generalized eigenvalues>": [],
    "<ntc>": [],
    "<thresholding>": [],
    "<summation index>": [],
    "<unknown rank>": [],
    "<models mean>": [],
    "<composite distribution>": [],
    "<low volume>": [],
    "<outcome vectors>": [],
    "<batch length>": [],
    "<initial vector>": [],
    "<Tensor contraction>": [],
    "<one operand>": [
        "At least one operand must be sparse."
    ],
    "<Sequence Network>": [
        "A Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder."
    ],
    "<padding dimension>": [],
    "<Distributed Training, Distributed training, distributed training>": [
        "If the network takes 100ms or longer to perform one iteration (100ms per fit operation on each minibatch), distributed training should work well with good scalability."
    ],
    "<classification performance>": [],
    "<example path>": [],
    "<multivariate series>": [
        "Multivariate series are fit with full covariance matrices for observation and latent state transition noise, each feature of the multivariate series having its own latent components."
    ],
    "<ROC>": [
        "As ROC metrics are only defined for binary classification, this treats the multi-class output as a set of 'one-vs-all' binary classification problems.",
        "ROC has 2 modes of operation: (a) Thresholded (less memory) Thresholded Is an approximate method, that (for large datasets) may use significantly less memory than exact.."
    ],
    "<large batch, large batches, larger batch>": [],
    "<Tensor matrix, tensor matrix>": [],
    "<rank shape>": [],
    "<closest cluster>": [],
    "<largest interval>": [],
    "<computation nodes>": [],
    "<enough rate>": [],
    "<Level2>": [],
    "<column conditions>": [],
    "<level intuitive>": [],
    "<distributed tasks>": [],
    "<Sequence layer>": [],
    "<padded elements>": [],
    "<Labels shape, labels shape>": [],
    "<QR, qr>": [],
    "<Indices matrix>": [],
    "<tensor proto>": [],
    "<hue channel>": [],
    "<dense multiplication>": [],
    "<Cepstral>": [],
    "<sampling vectors>": [],
    "<theta vector>": [
        "Errors are thrown if the theta vector does not exactly fill the matrices."
    ],
    "<predictive power>": [],
    "<checkpoint value>": [],
    "<binary scores>": [],
    "<batched input, batched inputs>": [],
    "<Tensor result, tensor result, tensor results>": [],
    "<input pixels>": [],
    "<regression model, regressive model>": [],
    "<Module Parameters>": [],
    "<probability function, probability functions>": [
        "The probability function is, Examples."
    ],
    "<Post step>": [],
    "<broadcast dimension, broadcast dimensions, broadcasted dimensions>": [],
    "<reconstruction function>": [],
    "<one replica>": [
        "Since the replicas are all running the same program, there are not a lot of ways for that to happen, but it is possible when a while loop's condition depends on data from infeed and the data that is infed causes the while loop to iterate more times on one replica than another."
    ],
    "<true classes>": [
        "If not provided, the true class distribution is estimated live in a streaming fashion.",
        "That is, soft classes are not allowed, and the labels vector must provide a single specific index for the true class for each row of logits (each minibatch entry)."
    ],
    "<aspect ratios>": [
        "The aspect ratio of the image content is not preserved.",
        "Resized images will be distorted if their original aspect ratio is not the same as size."
    ],
    "<data frame>": [],
    "<Convolution dimensionality, convolution dimensionality>": [],
    "<Importance sampling, importance sampling>": [
        "Importance sampling with a positive function, in log-space."
    ],
    "<Laplacian level>": [],
    "<training pipeline, training pipelines>": [
        "But if your training pipeline is doing that every time, Deeplearning4j will seem about 10x slower than other frameworks, because you're spending your time creating datasets."
    ],
    "<onto CPU>": [],
    "<fixed grid>": [],
    "<second aspect>": [],
    "<untrained model>": [],
    "<mean vector, mean vectors>": [],
    "<rank Tensor, rank Tensors, rank tensors>": [],
    "<split line>": [],
    "<coming gradient>": [],
    "<normalization applied>": [],
    "<predicted word>": [],
    "<clipping function>": [],
    "<hot columns>": [],
    "<Unstack, unstack, unstacking>": [],
    "<decreasing frequency>": [],
    "<natural left>": [],
    "<save multiple>": [],
    "<hermitian>": [],
    "<frame length>": [],
    "<aggregate gradients>": [
        "This includes the operations to synchronize replicas: aggregate gradients, apply to variables, increment global step, insert tokens to token queue."
    ],
    "<merged result>": [
        "If the third component is the members of a mirrored variable (v maps d0 to v0, d1 to v1, etc.), then the merged result will be that mirrored variable (v)."
    ],
    "<scale variable, scale variables>": [],
    "<Image Classification, Image classification, image classification>": [],
    "<multiple indices>": [
        "So, when multiple indices in updates refer to the same index in operand, the corresponding value in output will be non-deterministic.",
        "If indices is rank 1 (a vector), then for each position i, out[indices[i], ...] = updates[i, ...] Note that if multiple indices refer to the same location, the output at those locations is undefined - different updates may occur in different orders.",
        "If indices is rank 1 (a vector), then for each position i, out[indices[i], ...] -= updates[i, ...] Note that if multiple indices refer to the same location, the contributions from each is handled correctly."
    ],
    "<value matrix, valued matrix>": [],
    "<nn module>": [],
    "<natural number>": [],
    "<Data processing, data processing>": [
        "The data processing procedure will expand data X20-30 times so it's better to dump them."
    ],
    "<accuracy score>": [],
    "<point comparison>": [
        "NOTE Floating point comparison to zero is done by exact floating point equality check."
    ],
    "<difference quotient>": [],
    "<vector norm>": [
        "If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",
        "If the input tensor has more than two dimensions, the vector norm will be applied to last dimension."
    ],
    "<leftmost dimensions>": [],
    "<reference batch>": [],
    "<GPU tensors>": [],
    "<monitor similarity>": [],
    "<training program, training programs>": [
        "This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.",
        "A training program that saves regularly looks like: In addition to checkpoint files, savers keep a protocol buffer on disk with the list of recent checkpoints."
    ],
    "<one inbound>": [],
    "<normal implementation>": [],
    "<data normalizers>": [],
    "<Various function>": [],
    "<single run>": [],
    "<Prepare Data, Prepare data, prepare data>": [],
    "<fast reply>": [],
    "<real numbers>": [],
    "<buil>": [],
    "<Huber Loss, Huber loss, huber loss>": [
        "The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large - this makes it more robust to outliers when the estimates of \\(Q\\) are very noisy."
    ],
    "<proper shape>": [],
    "<single OS>": [],
    "<external model>": [],
    "<THNN>": [],
    "<Learnable Parameters, learnable parameter, learnable parameters>": [],
    "<random variable, random variables>": [
        "If some random variable X has normal distribution, Then Y will have half normal distribution."
    ],
    "<norm regularization>": [],
    "<critical step>": [
        "The next critical step is the setup of each node."
    ],
    "<Optional shape, optional shape>": [],
    "<topN>": [],
    "<Cumulative sum, cumulative sum>": [],
    "<column indices>": [],
    "<output feature, output features>": [],
    "<absolute weights>": [],
    "<collapse dimensions>": [],
    "<valid message>": [],
    "<rint>": [],
    "<singular vectors>": [],
    "<initiate training>": [],
    "<average worker>": [],
    "<flow boundary>": [],
    "<basic understanding>": [],
    "<beam search>": [
        "For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.",
        "For instance, a beam search in sequence-to-sequence translation is a loop over the (varying) sequence length of inputs."
    ],
    "<Android development>": [],
    "<probability matrix>": [],
    "<max time>": [],
    "<multiple column>": [],
    "<basic training>": [],
    "<Fixed ratio>": [],
    "<single histogram>": [],
    "<drawn samples>": [],
    "<normalized amplitude>": [],
    "<Restored model, restored model>": [],
    "<Pearson Correlation Coefficient, Pearson correlation coefficient, pcc, pearson correlation coefficient>": [],
    "<loaded graph>": [],
    "<step counter>": [
        "A step counter might fall into this category."
    ],
    "<Run inference, run inference, running inference>": [],
    "<summary operation>": [],
    "<final signal>": [],
    "<diagonal covariance>": [],
    "<embedding variable>": [],
    "<Model evaluation, model evaluation, model evaluator>": [],
    "<capacitance matrix>": [],
    "<Trained network, train networks, trained network>": [
        "The trained network is passed live onboard video and decisions based on object detection from the Neural Net determine the vehicles actions."
    ],
    "<padding arrays>": [],
    "<long matrix>": [],
    "<Graph Visualization>": [
        "The graph visualization can help you understand and debug them."
    ],
    "<savefile>": [
        "The savefile includes: - The model architecture, allowing to re-instantiate the model."
    ],
    "<Wasserstein distance>": [],
    "<Viterbi, viterbi>": [],
    "<TRAIN, Trainable, trainable>": [],
    "<CPU performance>": [],
    "<linear policy>": [],
    "<Output logs>": [],
    "<feature array, feature arrays, features array, features arrays>": [],
    "<full data>": [],
    "<single draw>": [],
    "<deep model, deep models>": [],
    "<cluster memory>": [],
    "<Representation Learning, representation learning>": [],
    "<bmm>": [],
    "<square weights>": [],
    "<output diagonal>": [],
    "<intensity distribution>": [],
    "<undefined value>": [],
    "<Laplace distribution, Laplace distributions, laplace distribution>": [
        "Note that the Laplace distribution can be thought of two exponential distributions spliced together \"back-to-back.\"."
    ],
    "<Batched matrix, batched matrix>": [],
    "<variant type>": [],
    "<Reduce function, reduce function>": [],
    "<absolute accuracy>": [],
    "<varying probability>": [],
    "<Restart training, Restarting training, restart training>": [
        "I though if there is anything related to accumulated memory which slows down the training, the restart training will help."
    ],
    "<concrete examples>": [],
    "<save path>": [],
    "<bilinear transformation>": [],
    "<input records>": [],
    "<normalization part>": [],
    "<built batch>": [],
    "<model code>": [
        "Most model code works the same during eager and graph execution, but there are exceptions."
    ],
    "<evaluation statistics>": [],
    "<latent state>": [],
    "<Negative slope, negative slope>": [],
    "<hot matrix>": [],
    "<LSTM model>": [],
    "<CPU copy>": [],
    "<Tensor Values, Tensor value, Tensor values, tensor value, tensor values, tensors values>": [
        "Timestamp of when this tensor value was dumped.",
        "Output slot index from which the tensor value was dumped."
    ],
    "<array dimension, array dimensions>": [],
    "<Gaussian process>": [],
    "<automatic termination>": [],
    "<copying data>": [],
    "<weighted average>": [],
    "<densenet121>": [],
    "<Parameter Spaces, parameter space, parameter spaces>": [],
    "<single GPU, single gpu>": [
        "Applicability: nd4j-cuda-xx, when multiple backends are on classpath Usage: A fallback for determining the local IP the parameter server, if other approaches fail to determine the Description: If set, only a single GPU will be used by ND4J, even if multiple GPUs are available in the system."
    ],
    "<robin balancing>": [
        "Round-robin balancing used here."
    ],
    "<Generative Model, generative model, generative models>": [],
    "<Keras Model, Keras Models, Keras model, Keras models, keras model, keras models>": [
        "As user you just have to provide your model Keras models into DL4J.",
        "Keras models are made by connecting configurable building blocks together, with few restrictions."
    ],
    "<componentwise>": [],
    "<Normalized Tensor, normalized Tensor, normalized tensor>": [],
    "<artificial neuron>": [
        "The biological terms show how this artificial neuron loosely maps to a neuron in the human brain."
    ],
    "<checkpoint name>": [],
    "<sigmoid transform>": [
        "This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses."
    ],
    "<evaluation run>": [],
    "<scatter function>": [
        "Therefore, the scatter function should not be overly sensitive to reassociation."
    ],
    "<ngpu>": [],
    "<data generator>": [],
    "<vector pair>": [],
    "<view array>": [],
    "<sequence tensor>": [],
    "<prediction probabilities, prediction probability>": [],
    "<Input image, Input images, input image, input images>": [
        "The input is a 3x64x64 input image and the output is a scalar probability that the input is from the real data distribution.",
        "Input images can be of different types but output images are always float.",
        "The input image to a U-Net model is a set of RGB images, and training mask is a 10-channel mask corresponding to the 10 different classes that I want to segment in the image.",
        "Once the input images are processed and concatenated together by the CPU, we have 8 tensors each with a batch-size of 32."
    ],
    "<single Tensor, single tensor>": [
        "One example is a subgraph representing a single tensor: this tensor is passthrough.",
        "In the next example, the input is a list of tensors (with length \"N\") of the same (but unspecified) type (\"T\"), and the output is a single tensor of matching type: By default, tensor lists have a minimum length of 1."
    ],
    "<large messages>": [],
    "<parameter synchronization>": [],
    "<parameterized graph>": [],
    "<train data>": [],
    "<adb>": [
        "NOTE: for Android development, adb shell is needed otherwise the following section of tutorial will not run."
    ],
    "<level seed>": [],
    "<operation time>": [],
    "<numerical instabilities>": [],
    "<based models>": [],
    "<multiple series>": [],
    "<scatter occurs>": [],
    "<left boundary>": [],
    "<Image Segmentation, image segmentation>": [],
    "<Functional Model, functional model>": [
        "Model Requirements: - Model must be a sequential model or functional model."
    ],
    "<data distribution>": [],
    "<rotation angle>": [],
    "<various values>": [],
    "<input labels>": [],
    "<update vector, updated vectors>": [],
    "<Kumaraswamy>": [],
    "<snapshot value>": [],
    "<semantic information>": [],
    "<large sequences>": [],
    "<matching rank>": [],
    "<CUDA, Cuda, cuda>": [
        "CUDA semantics has more details about working with CUDA.",
        "Compiling from source should be pretty straighforward if if you already have cuda installed.",
        "CUDA API requires that the allocation exported to other processes remains CUDA tensors you shared don't go out of scope as long as it's necessary.",
        "CUDA support with mixed compilation is provided.",
        "This includes the CUDA include path, library path and runtime library.",
        "It's safe to call this function if CUDA is not available; in that Warning.",
        "warning:: CUDA API requires that the allocation exported to other processes remains valid as long as it's used by them.",
        "It seems that cuda is at least partly working, either for your model or your training data.",
        "Specifically, the values are from cublas if cuda is Warning."
    ],
    "<dW, double weights>": [],
    "<mask array, mask arrays>": [
        "To is the array to apply the indexing to from is a condition mask array (0 or 1).",
        "Merging operation may introduce mask arrays (when necessary) for time series data that has different lengths; if mask arrays already exist, these will be merged also.",
        "These mask arrays identify whether an input/label is actually present, or whether the value is merely masked.",
        "In this case the present input (mask array = 1) will be reverted in place and the padding (mask array = 0) will be left untouched at the same place.",
        "Note that when a CNN layer changes the shape of the activations (for example, stride > 1) the corresponding mask array needs to change shape also (as there is a correspondence between the two)."
    ],
    "<parameter table>": [],
    "<data retrieved>": [],
    "<batch value, batch values>": [],
    "<precision complex>": [],
    "<input queues>": [],
    "<CPU one>": [],
    "<numeric differences>": [],
    "<Final state, final state>": [],
    "<encoded labels>": [
        "If you don't use one-hot encoded labels you won't need to get the topk/argmax, since the labels already point to the class index."
    ],
    "<Optional Variable, Optional variable, optional variable>": [],
    "<complex model>": [],
    "<Multiple Inputs, Multiple inputs, multiple inputs>": [
        "Basically multiple inputs can be added to the same hint for parallel operations that will eventually be combined.",
        "BATCHED: Multiple inputs will be packed into single batch, and sent to last-used device."
    ],
    "<exogenous features>": [
        "Any exogenous features must have their shapes prefixed by the shape of the times feature.",
        "Times and all exogenous features must be provided for these steps."
    ],
    "<model zoo>": [
        "The model zoo also includes pretrained weights for different datasets that are downloaded automatically and checked for integrity using a checksum mechanism.",
        "The model zoo comes with well-known image recognition configurations in the deep learning community."
    ],
    "<Minimum learning, minimum learning>": [],
    "<Xms>": [
        "Xms sets the minimum heap space."
    ],
    "<group path>": [
        "Check whether group path contains string attribute."
    ],
    "<larger variance>": [
        "A larger variance (99%) will result in a higher order feature set."
    ],
    "<Xmx>": [],
    "<Dual Coordinate>": [],
    "<Upper boundary, upper boundary>": [],
    "<variable batch>": [],
    "<growing list>": [],
    "<data pointers>": [],
    "<control edges>": [],
    "<num2>": [],
    "<Categorical column, Categorical columns, categorical column, categorical columns>": [
        "Each column is plotted separately; only numerical and categorical columns are plotted.",
        "Indicator columns and embedding columns never work on features directly, but instead take categorical columns as input."
    ],
    "<Greff>": [],
    "<artificial inputs>": [],
    "<MIN value, Min value, min value>": [],
    "<Row vector>": [
        "The sum matrix represents that row vector falling down the matrix from top to bottom, adding itself at each level."
    ],
    "<length training>": [],
    "<big data>": [],
    "<Sequence lengths, sequence length, sequence lengths>": [
        "The sequence length feature is required for batches of varying sizes.",
        "The sequence lengths determined for each beam during decode."
    ],
    "<sparse control>": [],
    "<Popular models, popular model>": [
        "And the most popular model used so far is Google News model."
    ],
    "<Dense Word>": [
        "Getting Dense Word Embeddings An Example: N-Gram Language Modeling Exercise: Computing Word Embeddings: Continuous Bag-of-Words."
    ],
    "<Beta distribution, beta distribution>": [],
    "<residual layer, residual layers>": [],
    "<singular dimension>": [],
    "<npairs loss>": [],
    "<gradient descent, gradient descents>": [
        "Note: A gradient is not defined for this function, so it can't be used in training models that need gradient descent."
    ],
    "<regression algorithms>": [],
    "<network layers>": [],
    "<false models>": [],
    "<pooling sequence>": [],
    "<boxes tensor>": [],
    "<length multiple>": [],
    "<staging inputs>": [],
    "<integer scalar>": [],
    "<stable state>": [],
    "<Integer Tensor, integer Tensor, integer tensor, integer tensors>": [],
    "<weighting function>": [],
    "<LTS>": [],
    "<time transformations>": [],
    "<recursive structure>": [],
    "<unknown word>": [
        "One use case is that a special unknown word token is used as ID 0."
    ],
    "<large feature>": [],
    "<sync training>": [],
    "<complex Tensor>": [],
    "<polygamma>": [],
    "<zero diagonal>": [],
    "<training labels>": [],
    "<compound distribution>": [],
    "<Toe status>": [],
    "<multiplying scale>": [],
    "<Negative parameters>": [
        "Negative parameters will be replaced with 0."
    ],
    "<precision matrix>": [],
    "<dimensional distribution>": [],
    "<DBOW>": [
        "DBOW has no reasons for early termination.",
        "DBOW doesn't involves any pretraining."
    ],
    "<evaluation loop>": [],
    "<square tensor, square tensors>": [],
    "<model cloning>": [],
    "<batch matrices, batch matrix>": [
        "Examples: x is a batch matrix with compatible shape for matmul if."
    ],
    "<Atan2, atan2>": [],
    "<Kotlin>": [],
    "<correlation score>": [],
    "<norm distance>": [],
    "<chain rule>": [],
    "<Bidirectional Layer, Bidirectional layer>": [],
    "<Lpalce distribution>": [],
    "<logistic loss>": [],
    "<Optional device, optional device>": [],
    "<Model Files, model files>": [
        "Checkpoint model file must exist."
    ],
    "<KL, Keras layer, Keras layers, keras layers>": [
        "In this case, KL in a relaxed ELBO is no longer a lower bound on the log marginal probability of the observation."
    ],
    "<demuxer>": [],
    "<Gama function, Gamma function, gamma function>": [],
    "<integer multiple>": [],
    "<tower model>": [],
    "<Random Fourier, random Fourier>": [],
    "<numerical values>": [
        "Bad numerical values include nans and infs."
    ],
    "<called hidden>": [],
    "<Bellman equation>": [],
    "<second array>": [],
    "<product name>": [],
    "<original model>": [],
    "<large images>": [],
    "<band matrix>": [],
    "<early gradients>": [],
    "<learning process>": [],
    "<initial observation>": [],
    "<behind tensor>": [],
    "<reduce sum, reduced sum>": [],
    "<binary metrics>": [],
    "<shape dimension, shape dimensions>": [
        "Shapes of fixed rank but variable size are allowed by setting any shape dimension to None.",
        "All shape dimensions must be fully defined."
    ],
    "<Collective functions, collective function>": [],
    "<ASGD>": [],
    "<negative scalar>": [],
    "<projection matrices, projection matrix>": [],
    "<stable operator>": [],
    "<prediction time>": [],
    "<VSMs>": [
        "VSMs have a long, rich history in NLP, but all methods depend in some way or Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning.",
        "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other')."
    ],
    "<rank loss>": [],
    "<tower call>": [],
    "<Image Generation>": [],
    "<second output>": [],
    "<central crop>": [],
    "<STFT, stft>": [],
    "<maps Tensor>": [],
    "<varying parameter, varying parameters>": [],
    "<Separable convolution, separable convolution>": [],
    "<sgv1>": [
        "Note that sgv1 is modified in place."
    ],
    "<copy elements>": [],
    "<distance weights>": [],
    "<recall values>": [],
    "<bound index, bound indices>": [
        "On GPU, if an out of bound index is found, the index is ignored."
    ],
    "<Bias vector, bias vector>": [],
    "<valid approach>": [],
    "<output elements>": [
        "The output elements will be resorted to preserve the sort order along increasing dimension number.",
        "In the figure, the element of value 9 is selected by both of the top windows (blue and red) and the binary addition scatter function produces the output element of value 8 (2 + 6).",
        "The parameters and output element type have to be a boolean type, an integral type or a floating point types, and the types have to be consistent."
    ],
    "<Goodfellow>": [],
    "<Graphviz>": [],
    "<point addition>": [
        "However, if the range of the data is limited, floating-point addition is close enough to being associative for most practical uses."
    ],
    "<random shape>": [],
    "<device tag>": [],
    "<variable scope>": [
        "Variable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident.",
        "Note, if the current variable scope is marked as non-trainable then this parameter is ignored and any added variables are also marked as non-trainable."
    ],
    "<normalization methods>": [],
    "<negative means>": [],
    "<Confusion Matrix, Confusion matrix, confusion Matrix, confusion matrix>": [
        "The confusion matrix is always a 2-D array of shape [n, n], where n is the number of valid labels for a given classification task.",
        "Additionaly the confusion matrix can be accessed directly, converted to csv or html using."
    ],
    "<Multidimensional indices>": [
        "Multidimensional indices include a int64 index for each dimension."
    ],
    "<TPU Tensor>": [],
    "<quantized training>": [],
    "<gnuplot>": [],
    "<valid part>": [
        "Any not valid part of speech tags become NONE."
    ],
    "<valid checkpoint>": [],
    "<gradient function>": [
        "If the gradient function already exists, this method is a no-op.",
        "If the op has multiple outputs, the gradient function will take op and grads, where grads is a list of gradients with respect to each output.",
        "Here you can use Note that at the time the gradient function is called, only the data flow graph of ops is available, not the tensor data itself."
    ],
    "<Got error>": [],
    "<combined value>": [],
    "<weight distributions>": [],
    "<padded tensor>": [],
    "<one GPU>": [
        "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default."
    ],
    "<allocation operation>": [],
    "<weights connecting>": [],
    "<MNIST model, Mnist model>": [
        "As mentioned, the model under attack is the same MNIST model from pytorch/examples/mnist."
    ],
    "<statistic type>": [],
    "<algorithmic part>": [],
    "<locally connected>": [],
    "<backticks>": [],
    "<GPU scenarios>": [],
    "<LSTM, LSTMs, lstm>": [
        "Get LSTM forget gate bias initialization from Keras layer configuration.",
        "Then LSTM layers are applied and the sum of the outputs of all LSTM steps is fed into a softmax layer to make a classification decision among the classes of drawings that we know.",
        "Because of these long and short term dependencies, a LSTM is fitting for this task too.",
        "Get whether LSTM layer should be unrolled (for truncated BPTT).",
        "In addition, LSTM networks avoid vanishing and exploding gradients and are able to effectively capture long term dependencies due to its cell state, a feature not present in typical recurrent networks.",
        "We suspect that a LSTM will be effective for this task, because of the temporal dependencies in the data.",
        "We can use the hidden state to predict words in a language model, LSTM's in Pytorch\u00b6.",
        "Although this name sounds scary, all the model is is a CRF but where an LSTM provides the features.",
        "Does not support CuDNN (thus for GPUs, LSTM should be used in preference).",
        "A LSTM is well suited for this type of problem due to the sequential nature of the data.",
        "Note that other architectures (LSTM, etc) are usually more effective, especially for longer time series."
    ],
    "<Training Data, training data>": [
        "The training data is much larger and must be converted from CSV lists to matrices through an iterative for loop.",
        "The training data has four samples, and you must mention it in your code."
    ],
    "<timestep>": [],
    "<trim values>": [],
    "<preprint>": [],
    "<distribution models>": [],
    "<factor matrix>": [
        "A tensor: The row factor matrix is initialized to this tensor, A numpy constant, \"random\": The rows are initialized using a normal distribution."
    ],
    "<nested range>": [],
    "<model progressed, models progress>": [],
    "<tuple length>": [],
    "<running graphs>": [],
    "<total memory>": [],
    "<Average precision, average precision>": [],
    "<representing data>": [],
    "<Atrous convolution, atrous convolution>": [
        "More specifically: Atrous convolution allows us to explicitly control how densely to compute feature responses in fully convolutional networks.",
        "Atrous convolution is also closely related to the so-called noble identities in multi-rate signal processing."
    ],
    "<datapoint>": [],
    "<inputs data>": [],
    "<LSH>": [],
    "<Operating Characteristic>": [],
    "<entropy value>": [],
    "<array shape, array shapes>": [],
    "<task loss>": [],
    "<GCE>": [],
    "<backward networks>": [],
    "<empty rows>": [],
    "<hundred steps>": [],
    "<streaming value>": [],
    "<mean Tensor>": [],
    "<zero entries>": [],
    "<integration process>": [],
    "<conjugate symmetry>": [],
    "<loss go>": [],
    "<coded vectors>": [],
    "<Label data, label data>": [
        "The features dictionary maps feature column names to Tensors containing the corresponding feature data, and labels is a Tensor containing the batch's label data.",
        "Label data will be converted to a one-hot representation automatically."
    ],
    "<doubling algorithm>": [],
    "<identical rank>": [],
    "<Analysis Explained>": [],
    "<distributed processing>": [],
    "<Bias parameter, bias parameter, bias parameters>": [
        "Bias parameter keys given the layer configuration."
    ],
    "<slave node, slave nodes>": [],
    "<TPU model>": [],
    "<generic Tensor>": [],
    "<complicated network>": [],
    "<Multilabel, multilabel, multilayer>": [
        "Multilayer Network to tweak for transfer learning."
    ],
    "<network configurations>": [
        "Now that the network configuration is set up and instantiated along with our MNIST test/train iterators, training takes just a few lines of code.",
        "Note 1: The network configuration may be incomplete, but the inputs have been added to the layer already."
    ],
    "<Variable inputs>": [],
    "<removing examples>": [],
    "<losses dimension>": [],
    "<highest probability>": [],
    "<dx2>": [],
    "<interop>": [],
    "<device function>": [],
    "<SSIM>": [],
    "<usually channel>": [],
    "<Weighted mean, weighted mean>": [],
    "<Probs, prob, probs>": [
        "If probs is 2D, it is treated as a batch of relative probability Note.",
        "probs must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1.",
        "If probs is 1D with length-K, each element is the relative probability of sampling the class at that index."
    ],
    "<Rectified Linear, rectified linear>": [],
    "<error correction>": [],
    "<Fourier Transform, Fourier transform>": [],
    "<Standard memory, standard memory>": [],
    "<word tensor>": [
        "The word tensor derives from the Latin tendere, or \"to stretch\"; therefore, tensor relates to that which stretches, the stretcher."
    ],
    "<multiple batches>": [],
    "<Group Normalization, group normalization>": [],
    "<recognition systems>": [],
    "<DOT>": [],
    "<Luong attention>": [],
    "<Absolute Difference loss>": [],
    "<behavior Dropout>": [],
    "<particular problem>": [],
    "<predictive performance>": [],
    "<Loss Function, Loss Functions, Loss function, Loss functions, loss function, loss functions>": [
        "The loss function then becomes: ..",
        "The loss function for each sample is: ..",
        "From the paper, the GAN loss function is.",
        "Loss function \u2014This measures how accurate the model is during training.",
        "Loss functions are provided by Torch in the nn package.",
        "In many case loss functions such as these are invariant under invertible transformations of the random variables.",
        "The loss function for :math:`n`-th sample in the mini-batch is ..",
        "This loss function trains the network to produce a higher value for For a more detailed explanation and the actual formulas, read the original paper by Zhang and Zhou.",
        "The loss function for \\(n\\)-th sample in the mini-batch is.",
        "L1 loss function where each the output is (optionally) weighted/scaled by a flags scalar value.",
        "This loss function is important because we're training to predict the next character, and the number of characters is a discrete number (similar to a classification problem).",
        "Loss functions for each neural network layer can either be used in pretraining, to learn better weights, or in classification (on the output layer) for achieving some result.",
        "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target."
    ],
    "<Sequence features, sequence features>": [],
    "<GPU version>": [
        "The GPU version is still a bit slower than the CPU version, but not by much."
    ],
    "<single distribution>": [],
    "<classification head>": [],
    "<content distance>": [],
    "<False weights>": [],
    "<input dictionary>": [],
    "<strange part>": [],
    "<output vector, output vectors>": [
        "The hidden state vector is then passed to the next time step, while the output vector is recorded."
    ],
    "<shape value>": [
        "Its first shape value is N, the minibatch size."
    ],
    "<Training loss, training loss>": [
        "Now, the training loss decreases nicely but suddenly becomes a huge positive value."
    ],
    "<linear activation, linear activations>": [],
    "<actor gradient>": [],
    "<one fine>": [],
    "<Alpha Dropout, Alpha dropout, alpha dropout>": [
        "Alpha Dropout fits well to Scaled Exponential Linear Units by randomly setting activations to the negative saturation value.",
        "Alpha Dropout is a dropout that maintains the self-normalizing property.",
        "Alpha Dropout maintains the original mean and standard deviation of the input.",
        "Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation."
    ],
    "<automated number>": [],
    "<entire variable>": [],
    "<one vector>": [],
    "<compatible shape>": [],
    "<stopping gradient>": [],
    "<single machine>": [
        "WorkerID: A unique identifier for workers, within a session For example, single machine training (with 1 listener) would have 1 session ID, 1 type ID, 1 worker ID, and multiple timestamps."
    ],
    "<dx1>": [],
    "<Create input>": [],
    "<asynchronous training>": [],
    "<sequence order>": [],
    "<embedding column, embedding columns>": [
        "By permitting a richer palette of numbers for every cell, an embedding column contains far fewer cells than an indicator column.",
        "Indicator columns and embedding columns never work on features directly, but instead take categorical columns as input.",
        "Instead of representing the data as a one-hot vector of many dimensions, an embedding column represents that data as a lower-dimensional, ordinary vector in which each cell can contain any number, not just 0 or 1."
    ],
    "<Grayscale, grayscale, grayscaled>": [],
    "<operation Motivation>": [],
    "<Snedecor distribution>": [],
    "<classifier Estimators>": [],
    "<complex transformation, complex transformations>": [],
    "<search state>": [],
    "<col2im>": [],
    "<tuning procedure>": [],
    "<positive cloud>": [],
    "<horizontal graph>": [],
    "<Loading Networks, loading network>": [],
    "<square identity>": [],
    "<training example, training examples>": [],
    "<hand function>": [],
    "<weight constraint, weight constraints>": [],
    "<Computation flow>": [],
    "<taking arguments>": [],
    "<continuous vector, continuous vectors>": [],
    "<classification models>": [],
    "<noisy example>": [],
    "<Converts value>": [],
    "<cosine part>": [],
    "<loading rows>": [],
    "<output dropout>": [],
    "<correct gradients>": [],
    "<shrinkage parameter>": [],
    "<pixel flow>": [],
    "<SGDW>": [],
    "<Long column, Long columns, long column>": [],
    "<equal results>": [],
    "<Bayesian computation>": [],
    "<input multiple, input multiples>": [],
    "<Optional graph>": [],
    "<model involves>": [
        "Suppose our model involves roughly the following We build the tree bottom up."
    ],
    "<partition number>": [],
    "<prevent numerical>": [],
    "<Multiple sequence, multiple sequences>": [],
    "<full rank>": [],
    "<summary type>": [],
    "<map function>": [],
    "<dynamic decoding>": [],
    "<zero indices>": [],
    "<trained estimator>": [],
    "<scalar Variable, scalar variable>": [],
    "<prediction batch>": [],
    "<data sequentially>": [],
    "<column dimension>": [],
    "<bad behavior>": [],
    "<L1 norm, l1 norm>": [],
    "<arrow Tensor>": [],
    "<weight shapes>": [],
    "<Iris classification>": [
        "The Iris classification problem is an example of supervised machine learning: the model is trained from examples that contain labels."
    ],
    "<svg1>": [],
    "<cmake>": [],
    "<storage level>": [],
    "<distribution functions>": [
        "The distribution functions can be evaluated on counts."
    ],
    "<OpenBLAS, openblas>": [],
    "<Score function, score function, score functions>": [],
    "<aggregation process>": [
        "Vector aggregations are saved only by Shards started aggregation process."
    ],
    "<addcmul>": [],
    "<OpenCV, opencl>": [
        "For images, packages such as Pillow, OpenCV are useful."
    ],
    "<linear projection>": [],
    "<input list>": [],
    "<training easier>": [],
    "<Created matrix>": [
        "Created matrix can be lower- or upper-triangular."
    ],
    "<persistent tensor>": [],
    "<evaluate function>": [
        "The evaluate function manages the low-level process of handling the input sentence."
    ],
    "<gradient noise>": [],
    "<generator output>": [
        "This style of comparison is useful for image translation problems, where the generator input is a corrupted image, the generator output is the reconstruction, and the real data is the target.",
        "The equilibrium of this game is when the generator is generating perfect fakes that look as if they came directly from the training data, and the discriminator is left to always guess at 50% confidence that the generator output is real or fake."
    ],
    "<larger clusters>": [],
    "<particular examples>": [],
    "<Dataframe, dataframe>": [],
    "<Training Guides, Training guide>": [],
    "<DET, Deterministically, det, deterministically>": [],
    "<GPU optimized>": [],
    "<assignment values>": [],
    "<good accuracy>": [],
    "<constrained update>": [],
    "<form one>": [],
    "<total lengths>": [],
    "<GAN step>": [],
    "<Controls behavior, controls behavior>": [],
    "<computation layer, computation layers>": [],
    "<convolution window>": [],
    "<classification problem, classification problems>": [],
    "<subsequent layers>": [
        "If this is True then all subsequent layers in the model need to support masking or an exception will be raised."
    ],
    "<validation performance>": [],
    "<normalization window>": [],
    "<Recall curve, Recall curves, recall curve, recall curves>": [],
    "<weighted tensor>": [],
    "<learn classifier>": [],
    "<L2 norm, l2 norm>": [
        "If the L2 norm exceeds the specified value, the weights will be scaled down to satisfy the constraint."
    ],
    "<Normalization Layers, Normalization layers, normalization layer>": [],
    "<dropout rate>": [],
    "<total records>": [],
    "<ldflags>": [],
    "<immediate error>": [],
    "<Evaluation metrics, evaluation metrics>": [
        "Similarly, evaluation metrics used for regression differ from classification.",
        "Evaluation metrics are an essential part of training a model."
    ],
    "<prediction call>": [],
    "<control dependencies, control dependency>": [
        "Data dependencies show the flow of tensors between two ops and are shown as solid arrows, while control dependencies use dotted lines."
    ],
    "<Prefetching, prefetching>": [
        "The capacity argument controls the how long the prefetching is allowed to grow the queues."
    ],
    "<edge padding>": [
        "Interior padding occurs logically before edge padding, so in the case of negative edge padding, elements are removed from the interior-padded operand."
    ],
    "<single version>": [],
    "<dimensional feature>": [],
    "<reduced memory>": [],
    "<minibatch Tensor, minibatch tensor, minibatch tensors>": [],
    "<data sparsity>": [],
    "<mean estimate>": [],
    "<dynamicaly>": [],
    "<PR curve>": [],
    "<variable call>": [],
    "<variable mode>": [],
    "<weight quantization>": [],
    "<valid sample>": [],
    "<configs, configuraiton>": [
        "Checks whether layer config contains unsupported options.",
        "The config of a layer does not include connectivity information, nor the layer class name.",
        "Estimator makes config available to the model (for instance, to allow specialization based on the number of workers available), and also uses some of its fields to control internals, especially regarding checkpointing.",
        "Learning configs (like updaters, learning rate etc) specified with the layer here will be honored."
    ],
    "<convolution mode>": [
        "Check that the convolution mode is consistent with the padding specification."
    ],
    "<Feature tensors, feature Tensors>": [],
    "<detailed shape>": [],
    "<Convolution layer, Convolution layers, Convolutional layers, convolution layer, convolution layers, convolutional layers>": [],
    "<algorithmic core>": [],
    "<Conv2D, Conv2d, conv2, conv2d>": [],
    "<flip mode>": [],
    "<unit norm>": [],
    "<output split>": [],
    "<segmentation task, segmentation tasks>": [],
    "<fixed dimension>": [],
    "<learnable weights>": [],
    "<lower performance>": [],
    "<channel image>": [],
    "<Upsampling1D>": [],
    "<duplicate data>": [],
    "<Output array, Output arrays, output array, output arrays, outputs array>": [
        "Output array: Two input and one output arrays must have the same shape."
    ],
    "<jittered image>": [],
    "<broadcasting dimension, broadcasting dimensions>": [
        "The broadcasting dimensions can be a tuple that describes how a smaller rank shape is broadcast into a larger rank shape.",
        "Note: when adding a 2x3 matrix to a 3-element vector, a broadcasting dimension of 0 is invalid."
    ],
    "<runtime shape>": [],
    "<dynamic Shape, dynamic shape, dynamic shapes>": [],
    "<model metrics>": [
        "The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch."
    ],
    "<computing machines>": [],
    "<initial network>": [],
    "<training compared>": [],
    "<unique values>": [],
    "<Random Seed, Random seed, random seed, random seeds>": [],
    "<log data>": [],
    "<preceding function>": [],
    "<unseen sentence>": [],
    "<Prefetch, prefetch>": [],
    "<average time>": [],
    "<data examples>": [],
    "<SUM, summand>": [],
    "<learning models>": [],
    "<true elements>": [],
    "<load steps>": [],
    "<NDHWC>": [
        "NDHWC (default) and NCDHW are supported."
    ],
    "<log normalizing>": [],
    "<SRU cell, sru cell>": [],
    "<reconstruction error>": [],
    "<Samples elements>": [],
    "<looking curve>": [],
    "<resulting transformation>": [
        "Sometimes, the resulting transformation is not the intended result."
    ],
    "<expected sum>": [
        "The scaling is so that the expected sum is unchanged."
    ],
    "<matrix rank>": [],
    "<Variable Resolution, variable resolution>": [],
    "<decision process>": [],
    "<nullptr>": [],
    "<seed entropy>": [],
    "<external list>": [],
    "<prior distribution>": [],
    "<nd2>": [],
    "<GRU, Gated Recurrent Unit, gated Recurrent Unit, gated recurrent unit>": [],
    "<IGDN>": [],
    "<Matrix product, matrix product>": [],
    "<Resampling, resampling>": [
        "NOTE Resampling is performed via rejection sampling; some fraction of the input values will be dropped."
    ],
    "<update types>": [
        "Update types are for instantiating various kinds of update types."
    ],
    "<real matrix>": [],
    "<Normalization function>": [],
    "<next image>": [],
    "<single computer>": [
        "Sometimes a single computer doesn't cut it for munging your data."
    ],
    "<network processing>": [
        "Basic idea is simple: network processing should be handled in background, without slowing caller thread."
    ],
    "<piecewise linear>": [],
    "<Attn>": [],
    "<applying inverse>": [],
    "<random projection, random projections>": [],
    "<small multiple>": [],
    "<batch vector>": [],
    "<cast operation>": [],
    "<automatic mechanism>": [],
    "<error normalized>": [],
    "<real shape>": [],
    "<next layer>": [],
    "<training occurs>": [],
    "<global rank>": [],
    "<Preloaded data>": [
        "See the guides: Inputs and Readers > Input pipeline, Reading data > Preloaded data."
    ],
    "<longest sequence>": [
        "The longest sequence in the data is 36,958 time steps!"
    ],
    "<pooling dimensions>": [],
    "<TPU name>": [],
    "<Estimation Algorithm>": [],
    "<random accuracy>": [],
    "<parameter search>": [],
    "<sspaddmm>": [],
    "<underlying tensor>": [],
    "<squares problem, squares problems>": [],
    "<coordinate value>": [],
    "<unstructured noise>": [],
    "<Bregman>": [],
    "<data later>": [],
    "<negative sum>": [],
    "<place operation>": [],
    "<inp, inplace>": [
        "Default: 1e-2 inplace: can optionally do the operation in-place."
    ],
    "<combination feature>": [],
    "<word count>": [
        "Increment a word count by 1."
    ],
    "<Linear linear>": [],
    "<final model>": [],
    "<computation time, computational time>": [
        "If the computation time takes longer than the copy and aggregation, the copy itself becomes essentially free."
    ],
    "<Variant tensor>": [],
    "<decay computation>": [],
    "<Prepare sample>": [],
    "<Keras constraints>": [],
    "<common shape>": [],
    "<GPU mode, GPU model>": [],
    "<Data Parallel, Data parallel, data parallel>": [],
    "<maximum scoring>": [],
    "<point communcation, point communication>": [],
    "<MLN>": [],
    "<lower triangle>": [],
    "<global list>": [],
    "<codomain>": [
        "Both the domain and the codomain of the mapping is [-inf, inf], however, the input of the forward mapping must be strictly increasing."
    ]
}