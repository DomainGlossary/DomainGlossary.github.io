<ACGAN>
<AFAIK>
<AIS data>
<AIS>
<AOT>
<ARFF>
<ASGD>
<AUC>
<AUPRC>
<AVG, Avg, avg>
<AVX>
<AWS>
<Absolute Difference loss>
<Absolute sum>
<Absolute tolerance, absolute tolerance>
<Accuracy Tensor>
<Accurate Computation>
<Activation Functions, Activation function, Activation functions, activation function, activation functions>
<Activation Layer, Activation Layers, Activation layer>
<Activation parameter>
<Adadelta>
<Adagrad, adagrad>
<Adam algorithm>
<Adam optimizer>
<Adamax>
<Adaptive Gradients, adaptive gradients>
<Adaptive Probabilities>
<Add layer>
<Add matrix>
<AddN>
<Adjust contrast>
<Advanced Activations>
<Advanced Indexing, advanced indexing>
<Adversarial training, adversarial training>
<Algorithmic Engineering>
<Alignment modes, alignment mode>
<Allocation mode, allocation mode, allocation model>
<Alpha Dropout, Alpha dropout, alpha dropout>
<Analysis Explained>
<Android development>
<Android device, Android devices>
<Annotation Step>
<Anomaly Detection, Anomaly detection, anomaly detection>
<Another tensor, another Tensor, another tensor>
<Anton, anton>
<Application Step>
<Applied element, Applies element, applied element, applies element>
<Applies Dropout, applies dropout, apply dropout>
<Applies normalization, applies normalization, apply normalization>
<Argmax, argmax>
<Argmin, argmin>
<Array format, array format>
<Asynchronous parameters>
<Atan2, atan2>
<Atrous convolution, atrous convolution>
<Attention Decoder, attention decoder>
<Attention Layer, attention layer>
<Attention Mechanism, Attention mechanism, attention mechanism, attention mechanisms>
<Attn>
<Audio Recognition>
<Auto correlation>
<Autoencoder, Autoencoders, autoencoder, autoencoders>
<Autograd, autograd>
<Automatic Differentiation, Automatic differentiation, automatic differentiation>
<Automatic variable>
<Autoregressive Density>
<Autoregressive Flow, autoregressive flow>
<Autoregressive distribution, Autoregressive distributions>
<Average Pooling, Average pooling, average pooling>
<Average layer>
<Average loss, average loss>
<Average precision, average precision>
<Average recall, average recall>
<BFGS>
<BLAS, Blas, blas>
<BPTT, bptt>
<BTW, Btw>
<Back Propagation, back propagation>
<Backend, Backends, backend, backends>
<Backprop, Backpropagation, backprop, backpropagation>
<Backup node>
<Backward computation, backward computation>
<Bahdanau>
<Baroni>
<Base distribution, base distribution>
<Basic loop>
<Basic sampling>
<Batch Normalization, Batch norm, Batch normalization, batch norm, batch normalization, batch normalizations, batch norms>
<Batch elements, batch element, batch elements>
<Batch shape, batch shape, batch shapes>
<Batched matrix, batched matrix>
<Batching tensors, batch Tensor, batch tensor>
<Bayesian computation>
<Bazel>
<Bellman equation>
<Bengio>
<Bernoulli Bernoulli>
<Bernoulli distribution, Bernoulli distributions, bernoulli distribution>
<Bernoulli trial, Bernoulli trials, bernoulli trial>
<Bessel function>
<Beta distributed>
<Beta distribution, beta distribution>
<Beta1, beta1>
<Bias parameter, bias parameter, bias parameters>
<Bias tensor, bias tensor, bias tensors>
<Bias vector, bias vector>
<Bidirectional Layer, Bidirectional layer>
<Bigtable, bigtable>
<Binary Classification, binary classification>
<Binary Classifiers, binary classifier, binary classifiers>
<Binary Encoding>
<Binary matrix, binary matrix>
<Binary model>
<Binary tensor>
<Binomial Probabilities>
<Binomial distribution, Binomial distributions, binomial distribution>
<Blocking point>
<Boltzmann Machine, Boltzmann Machines, Boltzmann machines>
<Bool, bool, bools>
<Bounding Boxes, Bounding boxes, bounding boxes>
<Bregman>
<Bucketized column>
<Build System, build system>
<Build models, build models>
<Building model, building model>
<CBOW>
<CELU, celu>
<CIDR>
<CIFAR>
<CMU>
<CNN, CNNs, Cnn, cnn, convolutional neural network, convolutional neural networks>
<CNN3D>
<CPU architecture, CPU architectures>
<CPU backend>
<CPU copy>
<CPU cores, cpu cores>
<CPU device, CPU devices>
<CPU host>
<CPU kernel, CPU kernels>
<CPU memory, cpu memory>
<CPU mode>
<CPU one>
<CPU performance>
<CPU tensors>
<CPU version>
<CPU>
<CRF, CRFs>
<CS231n, cs231n>
<CS321n>
<CSC>
<CUDA, Cuda, cuda>
<CUDNN, CuDNN, CuDNNGRU, Cudnn, cuDNN, cudnn>
<Caffe2>
<Caffe>
<Calculate Loss, calculate loss, calculating loss>
<Calculate activation>
<Calculate gradients, calculate gradients, calculated gradients, calculating gradients>
<Calculate padding>
<Carlo approximation>
<Categorical column, Categorical columns, categorical column, categorical columns>
<Categorical distribution, Categorical distributions, categorical distribution>
<Categorical features, categorical feature, categorical features>
<Categorical vocabulary, categorical vocabulary>
<Cauchy, cauchy>
<Center Loss, center loss>
<Cepstral>
<Chatbot, chatbot>
<Chi2>
<Cholesky, cholesky>
<Cifar data>
<Classification logits>
<Classification loss, classification loss>
<Classification statistics, classification statistics>
<Classifier Calibration>
<Client communication>
<Clips values, clip value, clipping value>
<Clojure>
<Cloud Detection>
<Cloud Partners>
<Cluster nodes, cluster Nodes, cluster node, cluster nodes>
<Colab, colab>
<Collapse operation>
<Collective functions, collective function>
<Column vector, column vector, column vectors>
<Complete Model, complete model>
<Complex128, complex128>
<Composable, composable>
<Computation Graph, Computation Graphs, computation graph, computation graphs, computational graph, computational graphs>
<Computation flow>
<Computational processes, computational process>
<Compute Engine>
<Compute activations, compute activations>
<Compute edge>
<Compute gradients, compute gradient, compute gradients, computed gradient, computed gradients, computes gradients, computing gradients>
<Compute log, Computing log, compute log>
<Compute number>
<Compute score>
<Computes dropout>
<Concentration parameter, concentration parameters>
<Concrete Distribution>
<Conda, cond, conda>
<Conditional Accumulators, conditional accumulator>
<Conduct learning>
<Confusion Matrix, Confusion matrix, confusion Matrix, confusion matrix>
<Conjugate Gradient>
<Connectionist Temporal Classification>
<Constant Tensor, constant Tensor, constant tensor>
<Constant distribution, constant distribution>
<Constrained Optimization, constrained optimization>
<Constraint function, constraint function, constraint functions>
<Content Loss>
<Contiguous inputs>
<Continuous Features, Continuous features, continuous feature, continuous features>
<Continuous columns>
<Contrastive Training>
<Contrib, contrib>
<Controls behavior, controls behavior>
<Controls model>
<Conv, Convolutional, conv, convolutional>
<Conv1D, Conv1d, conv1, conv1d>
<Conv2D, Conv2d, conv2, conv2d>
<Conv3D, Conv3d, conv3d>
<Conversion Transforms, conversion transform>
<Converted image>
<Converts value>
<Convolution dimensionality, convolution dimensionality>
<Convolution layer, Convolution layers, Convolutional layers, convolution layer, convolution layers, convolutional layers>
<Convolution operation, convolution operation, convolution operations, convolution operator>
<Convolution2D, convolution2d>
<Coordinate function>
<Copies data, copies data>
<Copies tensor>
<Copy model>
<Core Layers>
<CoreML models>
<Cosine distance, cosine distance>
<Cosine similarity, cosine similarity>
<Count number>
<Covariate Shift>
<Create gradient>
<Create input>
<Created matrix>
<Critic Algorithm>
<Cropped image, cropped image>
<Cropping layer, cropping layer>
<Cropping1D>
<Cropping2D>
<Cropping3D>
<Cross Entropy, cross entropy>
<CuBLAS, Cublas, cuBLAS, cublas>
<CuDNNLSTM>
<CudnnRNN>
<Cumulative product, cumulative product>
<Cumulative sum, cumulative sum>
<CycleGAN training>
<D1, d1>
<DBOW>
<DCGAN>
<DDPG>
<DET, Deterministically, det, deterministically>
<DFT, Discrete Fourier Transform, discrete Fourier transform>
<DFT2>
<DNN, Deep Neural Networks, deep neural network, deep neural networks, dnn>
<DOT format, dot format>
<DOT>
<DQN, dqn>
<Data Loading, Data loading, data loading>
<Data Parallel, Data parallel, data parallel>
<Data Parallelism, Data parallelism, data parallelism>
<Data Science, data science>
<Data Training, data Training>
<Data Transformation, data transformations>
<Data augmentation, data augmentation>
<Data dependencies, data dependencies, data dependency, data dependent>
<Data normalization, data normalization>
<Data pipeline, data pipeline, data pipelines>
<Data processing, data processing>
<Data replication>
<Dataflow, dataflow>
<Dataframe, dataframe>
<Dataset, Datasets, dataset, datasets, data set, Data set>
<Decoding function>
<Deconvolution, deconvolution>
<Deep Belief>
<Deep Learning, Deep learning, deep learning>
<Deep Network, deep net, deep network, deep networks>
<Deeplearning layer>
<Delving Deep, Delving deep>
<Denoising, denoising>
<Dense Layer, Dense layer, Dense layers, dense layer, dense layers>
<Dense Tensor, Dense tensor, dense Tensor, dense Tensors, dense tensor, dense tensors>
<Dense Word>
<Density Estimation>
<Density Modeling>
<Depthwise, depthwise>
<Dequeue, dequeue>
<Desired interpolation>
<Deterministic distribution>
<Deterministic mode>
<Device Placement>
<Device index, device index>
<Device memory, device memory>
<Diagonal matrix, diagonal matrices, diagonal matrix>
<Dilated Convolutions, dilated convolution, dilated convolutions>
<Dirichlet>
<Discounting factor>
<Discrete Cosine>
<Discrete Random, discrete random>
<Discrete parameter>
<Discriminator Loss, discriminator loss>
<Distance functions>
<Distance metric, distance metric>
<Distributed Applications>
<Distributed Data, distributed data>
<Distributed Evaluation, distributed evaluation>
<Distributed Master, distributed master>
<Distributed Representations>
<Distributed Trainer, distributed trainer>
<Distributed Training, Distributed training, distributed training>
<Distributed communication, distributed communication>
<Distributed error>
<Distribution parameter, Distribution parameters, distribution parameter, distribution parameters>
<Distribution sample>
<Distribution strategy>
<Distributions shapes>
<Divergence loss, divergence Loss, divergence loss>
<Division operation, division operation>
<Dk, dK>
<Dot layer>
<Double column, Double columns, double column>
<Download data>
<Dropout functions>
<Dropout layer, Dropout layers, dropout layer, dropout layers>
<Dropout2d, dropout2d>
<Dual Coordinate>
<Dual Optimization>
<E5>
<EC2>
<ELU, ELUs, Exponential Linear Unit, Exponential Linear Units, elu>
<ETL>
<Eager mode, eager mode>
<Early Stopping, Early stopping, early stopping>
<Effective padding>
<Eigenvectors, eigenvectors>
<Einstein sum, Einstein summation>
<Elementwise, elementwise, elems>
<Embedding Layers, Embedding layer, embedding layer>
<Embedding matrix, embedding matrix>
<Embeddings, embeddings>
<Encoding function>
<English word>
<Entropy loss, entropy loss>
<Epsilon value, epsilon value, epsilon values>
<Estimation Algorithm>
<Estimator model, Estimator models>
<Estimators program>
<Euclidean distance, euclidean distance>
<Euclidean norm>
<Evaluable>
<Evaluated values>
<Evaluation metrics, evaluation metrics>
<Expected inputs, expected inputs>
<Explicit padding>
<Exploding gradients, exploding gradient, exploding gradients>
<Exponential distributed>
<Exponential distribution, Exponential distributions, exponential distribution>
<Exponential linear, exponential linear>
<Exponential moving, exponential moving>
<Export classifier>
<F1 Score, F1 score, f1 score>
<FFT, fft>
<FGSM>
<FID, fid>
<FN, False Negative, False negative, False negatives, Fn, false negative, false negatives, fn>
<FP16, fp16>
<FP32>
<FP8>
<FULL model, full model>
<FULL>
<Fake images, fake images>
<False Alarm>
<False Positive, False positive, false positive, false positives>
<False Shape>
<False values>
<False weights>
<Fast Gradient>
<Feature Columns, Feature columns, feature column, feature columns, features columns>
<Feature Mapping>
<Feature matrix, feature matrix>
<Feature tensors, feature Tensors>
<Feature weights>
<Final batch, final batch>
<Final state, final state>
<Finetuning, finetuning>
<Fixed coefficient>
<Fixed ratio>
<Float column, Float columns, float column>
<Float weight, float weights>
<Float32, float32>
<Forums distributed>
<Forward features>
<Four Tensor>
<Fourier Transform, Fourier transform>
<Frechet>
<Frobenius norm>
<Frontend, frontend>
<Ftrl>
<Full examples, full example>
<Functional Model, functional model>
<GAN Training, GAN training>
<GAN step>
<GAN, GANs, Generative Adversarial Networks>
<GCE>
<GCP>
<GDN, gdn>
<GER, ger>
<GGT>
<GMM>
<GPU Acceleration, GPU acceleration>
<GPU Devices, GPU device, GPU devices, gpu device>
<GPU Memory, GPU memory, gpu memory>
<GPU activity>
<GPU application, GPU applications>
<GPU available>
<GPU case>
<GPU collective>
<GPU compute>
<GPU context>
<GPU executions>
<GPU kernel>
<GPU location>
<GPU machines>
<GPU mode, GPU model>
<GPU nodes, gpu nodes>
<GPU operations>
<GPU optimized>
<GPU scenarios>
<GPU slow>
<GPU support, gpu support>
<GPU systems>
<GPU tensors>
<GPU trainer>
<GPU training>
<GPU usage>
<GPU version>
<GPU, GPUS, GPUs, Gpu, gpu, gpus>
<GRU cell, gru cell>
<GRU, Gated Recurrent Unit, gated Recurrent Unit, gated recurrent unit>
<Gama function, Gamma function, gamma function>
<Gamma Distribution, Gamma distribution, gamma distribution>
<Gamma distributed>
<Gating Gradients>
<Gaussian Noise, Gaussian noise, gaussian noise>
<Gaussian distribution, gaussian distribution>
<Gaussian dropout>
<Gaussian filter>
<Gaussian kernel, gaussian kernel>
<Gaussian mixture, gaussian mixture>
<Gaussian patterns>
<Gaussian process>
<Gaussians>
<Generalized Normalization>
<Generate weights>
<Generates values>
<Generative Adversarial, generative adversarial>
<Generative Model, generative model, generative models>
<Geometric distribution, Geometric distributions, geometric distribution>
<Github, github>
<Global Vectors>
<Global step, global step, global steps>
<Gloo>
<Good values>
<Goodfellow>
<Google Cloud, Google cloud>
<Google model>
<Got error>
<Gradient Aggregation>
<Gradient Computation, gradient computation>
<Gradient averaging, gradient averaging>
<Gradient checking, gradient checking>
<Gradient normalization, gradient norm, gradient normalization>
<Gradient optimization>
<Gradient tapes, gradient tape, gradient tapes>
<Gradle, gradle>
<Gram training>
<Graph Visualization>
<Graph level>
<Graphviz>
<GravesLSTM layer>
<Grayscale, grayscale, grayscaled>
<Greff>
<GridRNN>
<Group Normalization, group normalization>
<Group tensors>
<Grt>
<Gumbel>
<HDF5 format>
<HDF5, hdf5>
<HDFS, hdfs>
<HLO>
<HTK>
<HW, hour window>
<HWC>
<Hadamard>
<Hadoop, hadoop>
<Hamming distance, hamming distance>
<Hamming window>
<Handling images>
<Hann window>
<Hard sigmoid, hard sigmoid>
<Hardtanh, hardtanh>
<Hasim>
<Hermite>
<Hermitian matrix>
<Hermitian spectrum>
<Hessian matrices, Hessian matrix>
<Hidden Layers, Hidden layers, hidden layer, hidden layers>
<Hidden output, hidden output>
<Higher ranks, higher rank>
<Highest memory, higher memory>
<Hochreiter>
<Huber Loss, Huber loss, huber loss>
<Hurwitz>
<Hyperparameter, Hyperparameters, hyperparameter, hyperparameters>
<ICML>
<IDCT>
<IDFT>
<IGDN>
<INT16, int16>
<INT32, int32>
<INT8, int8>
<IOT, IoT>
<Identification System>
<Identification layer>
<Identity matrix, identity matrices, identity matrix>
<Image Adjustments>
<Image Classification, Image classification, image classification>
<Image Classifier, image classifier>
<Image Compression, image compression>
<Image Generation>
<Image Recognition, image recognition>
<Image Segmentation, image segmentation>
<Image Transformation, image transformations, image transforms>
<Image grid, image grid>
<Image tensor, image Tensor, image tensor, images tensor>
<Image warping>
<Implementation Model, Implementation mode>
<Importance sampling, importance sampling>
<Inception distance>
<Inception graph>
<Inception model>
<Inception networks>
<Increments number, increment number, increments number>
<Independent distribution, independent distributions>
<Indices matrix>
<Infiniband>
<InfoQ>
<Initial input, initial input>
<Injective, injective>
<Input Exercises>
<Input Node, input node, input nodes>
<Input Pre, Input pre, input pre>
<Input Sequences, input sequences>
<Input Tensor, Input tensor, Input tensors, input Tensor, input Tensors, input tensor, input tensors, inputs Tensors, inputs tensor, inputs tensors>
<Input activations, input activation, input activations>
<Input arrays, input arrays, inputs array>
<Input columns, input columns>
<Input features, input feature, input features>
<Input function, input function, input functions>
<Input image, Input images, input image, input images>
<Input layer, input layer, input layers>
<Input mask, input mask>
<Input matrix, input matrices, input matrix>
<Input pipeline, input pipeline, input pipelines>
<Input points, input points>
<Input samples, input samples>
<Input shape, input shape, input shapes, inputs shape, inputs shapes>
<Input vertices, input vertex, input vertices>
<Input weight, input weight>
<Input1, input1>
<Input2, input2>
<Instance Normalization, instance normalization>
<Instance Type, Instance type, instance type>
<Integer Tensor, integer Tensor, integer tensor, integer tensors>
<Integer column, Integer columns, integer column, integer columns>
<Interpolation mode, interpolation mode>
<Interrupt training>
<Invalid device>
<Inverse Discrete, inverse discrete>
<Iris Data, Iris data, iris data>
<Iris classification>
<Iris problem>
<JDKs>
<JIT, jit>
<Jaccard similarity>
<Jaccard, jaccard>
<Jacobian matrix, jacobian matrix>
<Jacobian, jacobian>
<Jacq, jacq>
<Jang>
<Jascha>
<JavaRDD>
<Jcuda>
<Jozefowicz>
<KL Divergence, KL divergence>
<KL, Keras layer, Keras layers, keras layers>
<KMP>
<Kaggle, kaggle>
<Keras Model, Keras Models, Keras model, Keras models, keras model, keras models>
<Keras activation>
<Keras constraints>
<Keras function>
<Keras guide>
<Keras session>
<Keras tensor, Keras tensors>
<Keras variable>
<Keras version, keras version>
<Kernel Methods>
<Kingma>
<Korrine>
<Kotlin>
<Kronecker product>
<Kryo>
<Kth, kth>
<Kullback>
<Kumaraswamy distribution>
<Kumaraswamy>
<L1 Loss, L1 loss>
<L1 norm, l1 norm>
<L1, l1>
<L2 Loss, L2 loss>
<L2 norm, l2 norm>
<L2, l2>
<LARS>
<LBFGS>
<LFW>
<LLVM>
<LMDB, lmdb>
<Long - Short Term Memory, LSTM, LSTMs, lstm>
<LRN, local response normalization, lrn>
<LSH>
<LSTM cell>
<LSTM model>
<LTS>
<Label data, label data>
<Labels Tensor, label Tensors, label tensor, label tensors, labels Tensor, labels tensor, labels tensors>
<Labels shape, labels shape>
<Lagrange multipliers>
<Lambda layer, lambda layer>
<Language Modeling, Language Models, Language modeling, language modeling, language models>
<Lapack, lapack>
<Laplace distributed>
<Laplace distribution, Laplace distributions, laplace distribution>
<Laplacian level>
<Laplacian pyramid>
<Latent Semantic, latent semantic>
<Layer Activations>
<Layer Normalization, Layer normalization, layer normalization>
<Layer Parameters, layer parameters>
<Layer Types, layer type, layer types>
<Layer name, layer name>
<Learnable Parameters, learnable parameter, learnable parameters>
<Learning Algorithm, learning algorithm, learning algorithms>
<Learning Rate, Learning Rates, Learning rate, learning rate, learning rates>
<Learning Representations>
<Learning example>
<Learning phase, learning phase>
<Least Squares, Least squares, least squares>
<Level1>
<Level2>
<Levenshtein distance>
<Linear Algebra, Linear algebra, linear algebra>
<Linear Models, Linear model, linear model, linear models>
<Linear Operator, linear operation, linear operator, linear operators>
<Linear Regression, Linear regression, linear regression>
<Linear function, Linear functions, linear function, linear functions>
<Linear indices, linear index, linear indexing, linear indices>
<Linear layer, Linear layers, linear layer, linear layers>
<Linear linear>
<Linear module>
<Linear stack, linear stack>
<Linear view, Linear views, linear view>
<List length>
<List values>
<Load Training, load training>
<Load distribution>
<Load graph, loading graph, loading graphs>
<Load model, load model>
<Load multiple, load multiple>
<Load tensor>
<Loading Model, Loading Models, loading model, loading models>
<Loading Networks, loading network>
<Loading data, loading data>
<Local Layers>
<Local transform>
<Location parameter, location parameter>
<Log entropy>
<Log probabilities, Log probability, log probabilities, log probability>
<Logistic Distribution, Logistic distribution, Logistic distributions, logistic distribution>
<Logistic Regression, Logistic regression, logistic regression>
<Long column, Long columns, long column>
<Loop Nest, loop nest>
<Loss Function, Loss Functions, Loss function, Loss functions, loss function, loss functions>
<Loss computation, loss computation>
<Loss scale, loss scale>
<Loss tensor, loss Tensor, loss tensor, loss tensors>
<Lpalce distribution>
<Luong attention>
<Luong>
<MAE, Mean Absolute Error, Mean absolute error, mean absolute error>
<MAF>
<MC2>
<MCC>
<MDP, mdp>
<MFCC, MFCCs>
<MIN value, Min value, min value>
<MIN, minimax>
<MKL, mkl>
<MKV>
<ML algorithms>
<ML model, ML models>
<MLN>
<MLP>
<MNIST data, Mnist data>
<MNIST model, Mnist model>
<MNIST, mnist>
<MPI, mpirun>
<MSE, Mean Squared Error, Mean squared error, mean square error, mean squared error>
<MTGP>
<Machine Learning, Machine learning, machine learning>
<Machine Translation, machine translation>
<Maddison>
<Mahalanobis distance>
<Main evaluation>
<Manhattan distance>
<Manual device>
<MapDB>
<Markdown, markdown>
<Markov Chains>
<Mask shape, mask shape>
<Master node, master node>
<Mathematical Society>
<Mathematical operation, mathematical operation>
<Matlab, matlab>
<Matrix Factorization, matrix factorization>
<Matrix Multiplication, Matrix multiplication, matrix multiplication>
<Matrix multiply, matrix multiplies, matrix multiply>
<Matrix product, matrix product>
<Matrix property, matrix property>
<Matrix transpose, matrix transpose>
<Max Pooling, Max pooling, max pooling>
<Max Pumperla>
<Max number, max number>
<Max operation, max operation>
<Maximum function, maximum function>
<Maximum layer>
<Maximum sentence, maximum sentence>
<Mean Absolute, mean absolute>
<Mean Squared, mean square, mean squared>
<Means clustering>
<Merge layer>
<Merging Tensor, merged tensor>
<Mesh Mode, mesh mode>
<Min operation, min operation>
<Mini batch, mini batch, mini batches>
<Minimum function, minimum function>
<Minimum layer>
<Minimum learning, minimum learning>
<Minimum resolution>
<Mixed Precision, mixed precision>
<Mixture Density, mixture density>
<Mixture distribution, mixture distribution>
<Mnist network>
<Model Deployment>
<Model Files, model files>
<Model Function, Model function, mode function, model function>
<Model Inputs, Model inputs, model input, model inputs>
<Model Parameters, model parameter, model parameters>
<Model Performance, model performance, model performed, model performs>
<Model Persistence>
<Model Results, model results>
<Model Score, model score, model scores>
<Model Trained, Model Training, Model training, model trained, model training, model trains, models trained, models training>
<Model Work, model works>
<Model definition, model definition>
<Model evaluation, model evaluation, model evaluator>
<Model output, model output, model outputs>
<Model prediction, model predictions>
<Model quality>
<Model saved, model saved, model saver>
<Model state, model state>
<Models built, model building>
<Modern Normalizing>
<Module Parameters>
<Momentum value, momentum value>
<Monte Carlo>
<Multi part>
<Multiclass, multiclass>
<Multidimensional indices>
<Multilabel, multilabel, multilayer>
<Multinomial distribution, multinomial distribution>
<Multiple GPU, multiple GPU>
<Multiple Inputs, Multiple inputs, multiple inputs>
<Multiple Layers, Multiply layer, multiple layers>
<Multiple outputs, multiple outputs>
<Multiple sequence, multiple sequences>
<Multiplies matrix, Multiply matrix, multiplied matrices>
<Multiprocessing error>
<Multiscale Computers>
<Multivariate Normal, multivariate Normal, multivariate normal>
<Mx1x2>
<N1>
<NAS cell>
<NAS, Neural Architecture Search>
<NCCL, nccl>
<NCDHW>
<NCE>
<NCHW>
<NCW>
<NDHWC>
<NHWC>
<NLL>
<NLP models>
<NLP, natural language processing>
<NLTK>
<NN, Neural Net, Neural Network, Neural Networks, Neural net, Neural network, Neural networks, neural net, neural nets, neural network, neural networks, nn>
<NPE>
<NVCC, nvcc>
<NVP, nvprof>
<NWC>
<Nadam>
<Nal>
<Native operation, native operation>
<Negative Binomial>
<Negative Log, Negative log, negative log>
<Negative Sampling, negative sampling>
<Negative parameters>
<Negative slope, negative slope>
<Nested structure, nested structure, nested structures>
<Nesterov, nesterov>
<Net model>
<Network Architectures, network architecture, network architectures>
<Network Latency>
<Network Learning, network learn, network learned, network learning, network learnt>
<Network Training, network trained, network training>
<Network outputs, network output, network outputs>
<Networks Tutorial, networks tutorial>
<Neural Computation>
<Neural Transfer, neural transfer>
<Node node, node node>
<Noise Layers>
<Noise Ratio>
<Non negative, non negative>
<None elements>
<None result>
<None shape>
<Normal batch>
<Normal distribution, Normal distributions, normal distribution, normal distributions>
<Normalization Layers, Normalization layers, normalization layer>
<Normalization function>
<Normalized Tensor, normalized Tensor, normalized tensor>
<Normalizing data>
<Numeric Tensor, numeric Tensor, numeric tensor>
<Numeric threshold>
<Numerical gradient, numerical gradient>
<Numerical optimization>
<Numpy, numpy>
<Nvidia, nvidia>
<Nx1>
<Nyquist frequency>
<OMP>
<ONNX, onnx>
<OOM, oom>
<OOV>
<Objective Functions, Objective function, objective function, objective functions>
<One tensor, one Tensor, one tensor>
<Online Learning>
<Oord>
<Op, one prediction, op>
<OpenBLAS, openblas>
<OpenCV, opencl>
<OpenMP, OpenMPI, openMPI, openmp>
<OpenNLP>
<Operating Characteristic>
<Operation copy>
<Operation type, operation type>
<Operations Normalization>
<Optimization Algorithm, Optimization algorithm, optimization algorithm, optimization algorithms>
<Optimizer, Optimizers, optim, optimiser, optimizer, optimizers>
<Optional Tensor, Optional tensor, optional Tensor, optional tensor>
<Optional Variable, Optional variable, optional variable>
<Optional center>
<Optional constraint>
<Optional count>
<Optional device, optional device>
<Optional filter>
<Optional graph>
<Optional inverse>
<Optional list, optional list>
<Optional message>
<Optional name>
<Optional padding>
<Optional scope, optional scope>
<Optional shape, optional shape>
<Optional transform>
<Optional tuple>
<Orthogonal distribution, orthogonal distribution>
<Outer product, outer product>
<Output Layer, Output Layers, Output layer, Output layers, output layer, output layers>
<Output Shape, Output shape, output shape, output shapes>
<Output Tensor, Output tensor, output Tensor, output Tensors, output tensor, output tensors, outputs Tensors, outputs tensor>
<Output array, Output arrays, output array, output arrays, outputs array>
<Output lists>
<Output logs>
<Output strings, output strings>
<Outputs true>
<Overfitting, overfit, overfitting>
<PCA, pca>
<PCIe>
<PDE>
<PEP8>
<PMF, pmf, probability mass function>
<POJO, pojo>
<PR curve>
<PRED, pred, preds>
<PRNGs>
<PReLU, prelu>
<PSNR>
<Padded image, padded image>
<Padding layers, padding layer>
<Pairwise, pairwise>
<Parallel Training, parallel training>
<Parallel arrays>
<Parameter Averaging, parameter averaging>
<Parameter Ratio, parameter ratio>
<Parameter Server, Parameter servers, parameter server, parameter servers>
<Parameter Spaces, parameter space, parameter spaces>
<Parametrized, param, parametrized>
<Pareto distribution>
<Participation Counts>
<Partitioner, partitioner, partitioners>
<Partitions data>
<Passthrough, passthrough>
<Pathwise, pathwise>
<Peak Signal>
<Pearson Correlation Coefficient, Pearson correlation coefficient, pcc, pearson correlation coefficient>
<Pearson Correlation>
<Penrose inverse>
<Perceptron, Perceptrons, perceptron, perceptrons>
<Perform evaluation, perform evaluation, performing evaluation>
<Performance Models>
<Piecewise, piecewise>
<Plotting results, plotting results>
<Plz, plz>
<Poisson distribution, poisson distribution>
<Poisson, poisson>
<Polynomial decay, polynomial decay>
<Pooling Layer, Pooling Layers, Pooling layer, Pooling layers, pooling layer, pooling layers>
<Pooling functions>
<Pooling ratio, pooling ratio>
<Pooling2D>
<Popular models, popular model>
<Post step>
<Predicted values, predicted values>
<Predictive models, prediction model>
<Prefetch, prefetch>
<Prefetching, prefetching>
<Preloaded data>
<Premade, premade>
<Prepare Data, Prepare data, prepare data>
<Prepare sample>
<Preprocess, Preprocessed, Preprocessing, preprocess, preprocessed, preprocessing, preprocessors>
<Pretraining, pretraining>
<Print gradients>
<Probability Mass, probability mass>
<Probability code>
<Probability coding>
<Probability density, probability densities, probability density>
<Probability distributions, probability distribution, probability distributions>
<Probs, prob, probs>
<Processed data, processed data>
<Processing gradients>
<Processing inputs>
<Profile model>
<Proguard, proguard>
<Proximal Stochastic>
<Py3>
<Pytorch, pytorch>
<QR decomposition>
<QR, qr>
<Quantization interval>
<Quantized Distribution>
<Quickstart, quickstart>
<RBF>
<RBMs>
<RDD, RDDs, rdd>
<RDMA>
<RELU, RRELU, RReLU, ReLU, Rectified Linear Unit, Rectified Linear Units, Rectified linear unit, Rectified linear units, Relu, rectified linear unit, relu, rrelu>
<RELU6, relu6>
<RF parameters>
<RFFM>
<RFFT, rfft>
<RL, Reinforcement Learning, reinforcement learning>
<RMS, rmsprop>
<RMSE, root mean squared error>
<RNN cell>
<RNN, RNNs, Recurrent Neural Networks, Recurrent neural networks, Rnn, recurrent neural network, recurrent neural networks, rnn>
<ROC Curve, ROC curve, ROC curves, roc curve>
<ROC calculation>
<ROC>
<RPCs>
<RSE>
<Random Field, random field>
<Random Fourier, random Fourier>
<Random Search, random search>
<Random Seed, Random seed, random seed, random seeds>
<Random Tensors, random Tensors, random tensor>
<Random array, random array>
<Random methods>
<Random normal, random normal>
<Random sampling, random sample, random sampling>
<Random sequence, random sequence>
<Randomization seed>
<Randomly sample, randomly sample>
<Real Images, Real images, real image, real images>
<Real data, real data>
<RealNVP>
<Recall curve, Recall curves, recall curve, recall curves>
<Recall rate>
<Recht>
<Rectified Linear, rectified linear>
<Recurrent Layers, Recurrent layers, recurrent layer, recurrent layers>
<Recurrent Network, recurrent Network, recurrent network, recurrent networks>
<Recurrent Unit, recurrent unit>
<Recurrent computations>
<Recurrent dropout, recurrent dropout>
<Recurrent instance>
<Redmon>
<Reduce function, reduce function>
<Reduce learning, reduce learning>
<Regexp, regexp, regexs>
<Regression Data>
<Regression Examples>
<Regressor, regressor, regressors>
<Regularizer, Regularizers, regularizer, regularizers>
<Relative tolerance, relative tolerance>
<Reliability diagram, reliability diagram>
<Remap, remap, remapped>
<Removes dimensions>
<Removes name>
<Replicated training>
<Representation Learning, representation learning>
<Required image>
<Resample, resample, resampler>
<Resampling, resampling>
<Rescaled image>
<Reshaping distribution>
<Residual plot, residual plot>
<Restart training, Restarting training, restart training>
<Restore graph>
<Restored model, restored model>
<Reverse division>
<Reverse subtraction, reverse subtraction>
<Riba>
<Riemann sum, Riemann summation>
<Roadmap, roadmap>
<Root folder, root folder>
<Root value>
<Rotated image, rotated image>
<Row vector>
<Rprop>
<Run Evaluation, run evaluation>
<Run Multiple, run multiple>
<Run Training, Run training, run training, running training, runs training>
<Run batch>
<Run images>
<Run inference, run inference, running inference>
<Running Estimators, running estimates>
<SBT>
<SELU, SeLU, selu>
<SGD, Stochastic Gradient Descent, stochastic gradient descent>
<SGDR>
<SGDW>
<SIRDS>
<SNE, sne>
<SQSS>
<SRU cell, sru cell>
<SRU>
<SSD>
<SSIM>
<STFT, stft>
<STL>
<STN>
<SUM, summand>
<SVD, singular value decomposition, svd>
<SVDF>
<SVHN>
<SVI>
<SVM, SVMs, svm>
<Sample Adversarial>
<Sample shape>
<Samples elements>
<Sampling Algorithms>
<Save distribution>
<Scala, scala>
<Scalability ratio>
<Scalar SUM, Scalar sum, scalar sum, scalar summary>
<Scalar Tensor, Scalar tensor, scalar Tensor, scalar Tensors, scalar tensor, scalar tensors>
<Scalar Value, Scalar value, scalar value, scalar valued, scalar values>
<Scalar float, scalar float>
<Scalar inputs, scalar inputs>
<Scalar integer, scalar integer>
<Scalar multiplication, scalar multiplication>
<Scalar subtraction, scalar subtraction>
<Scale Transforms, scale transformation>
<Scale parameter, scaling parameters>
<Scaling factor, scaling factor>
<Scope name, scope name>
<Score function, score function, score functions>
<Script function, script function>
<Sender rank>
<Separable convolution, separable convolution>
<Seq2seq, seq2seq>
<Sequence Classification, Sequence classification, sequence classification>
<Sequence Learning, Sequence learning, sequence learning>
<Sequence Models, Sequence models, sequence model, sequence models>
<Sequence Network>
<Sequence features, sequence features>
<Sequence index, sequence index>
<Sequence layer>
<Sequence lengths, sequence length, sequence lengths>
<Sequential Model, Sequential model, Sequential models, sequential model, sequential models>
<Sequential module>
<Shannon Entropy, Shannon entropy>
<Shape functions, shape function, shape functions>
<Shape inference, shape inference>
<Shape manipulation, shape manipulations>
<Shape tuple, shape tuple>
<Shape vector, shape vector>
<Shard function>
<Shared name>
<Sigmoid activation, sigmoid activation>
<Sigmoid loss, sigmoid loss>
<Signal guide>
<Simple Model, simple model, simplest model>
<Simple function, simple function>
<Simple point>
<SimpleCNN, SimpleRNN>
<Single Layer, single layer>
<Single example, single Example, single example, single examples>
<Single step, single step>
<Single training, single training>
<Singular Value, Singular values, singular value, singular values>
<Sinh, sinh>
<Slice function>
<Small gradients, small gradient, small gradients>
<Small model, small model>
<Snedecor distribution>
<Sobel filter>
<Softmax, softmax>
<Softmax2d>
<Softmin, softmin>
<Softplus, softplus>
<Softsign, softsign>
<Source rank>
<Spark cluster>
<Spark distribution>
<Spark function>
<Spark training>
<Sparse Tensor, Sparse tensors, sparse Tensors, sparse tensor, sparse tensors>
<Sparse features, sparse feature, sparse features>
<Sparse functions>
<Sparse gradients, sparse gradient, sparse gradients>
<Sparse layers>
<Spatial Transformer>
<Spatial convolution, spatial convolution>
<Spectral operators>
<Spectrogram data, spectrogram data>
<Split data, split data>
<Split elements>
<Split sequences>
<Square function, square function>
<Square matrices, square matrices, square matrix>
<Squared Error, squared error>
<Squares loss, square loss, squared loss>
<Stabilization function>
<Stackoverflow, stackoverflow>
<Standard deviation, standard deviation, standard deviations>
<Standard memory, standard memory>
<StarGAN>
<Started guide, started guide>
<State dropout>
<Static Graphs, static graph>
<Static shapes, static Shape, static shape>
<Step function, step function, step functions>
<Stereogram>
<Stochastic Neighbor, stochastic neighbor>
<Stochastic Optimization>
<Stochastic dual>
<Stochastic gradient, stochastic gradient>
<Stop conditions, stop condition>
<Stop training, stop training, stopping training>
<Stop value>
<Stops learning>
<StudentT>
<Style algorithm>
<Subtract layer>
<Summary Tensor, summary Tensor>
<Supervised Learning, supervised learning>
<TANH, Tanh, tanh>
<TBPTT length>
<TBPTT>
<TDR>
<TF, Tf, tensor filter, tf>
<TFDBG, tfdbg>
<TFGAN>
<TFT, TFTS>
<TFX>
<THNN>
<THP>
<TOCO>
<TODO>
<TPU Tensor>
<TPU cluster>
<TPU device, TPU devices>
<TPU distribution>
<TPU evaluation>
<TPU job>
<TPU model>
<TPU name>
<TPU system>
<TPU topology>
<TPU training>
<TPU, TPUs, tpu, tpus>
<TRAIN mode, Trained Model, train mode, train model, train models, trained model, trained models>
<TRAIN, Trainable, trainable>
<TSNE>
<TSV>
<Tailweight parameter>
<Task Learning, task learning>
<Task Network, task Networks, task networks>
<Temporal convolution, temporal convolution>
<Tensor Along, Tensor along, tensor along>
<Tensor Methods, Tensor methods, tensor methods>
<Tensor Processing, tensor processing>
<Tensor Shapes, Tensor shapes, tensor shape, tensor shapes>
<Tensor Values, Tensor value, Tensor values, tensor value, tensor values, tensors values>
<Tensor arguments, tensor argument, tensor arguments, tensors argument>
<Tensor arrays, tensor arrays>
<Tensor computation, tensor computation>
<Tensor computing>
<Tensor contraction>
<Tensor data, tensor data>
<Tensor denominator>
<Tensor diagonal>
<Tensor division>
<Tensor format>
<Tensor image, tensor image, tensor images>
<Tensor index, tensor indexing>
<Tensor input, Tensor inputs, tensor input, tensor inputs>
<Tensor manipulation>
<Tensor matrix, tensor matrix>
<Tensor name, tensor name, tensor names>
<Tensor numerator>
<Tensor operations, Tensor operator, tensor operation, tensor operations>
<Tensor output, tensor output>
<Tensor reference, tensor reference>
<Tensor result, tensor result, tensor results>
<Tensor type, Tensor types, tensor type, tensor types>
<Tensor vector>
<TensorRT, Tensordot, tensordot>
<Tensorflow, tensorflow>
<Text Classification, Text classification, text classification>
<Theano, theano>
<Threshold location>
<Thx>
<Time Series, Time series, time series>
<Time column, Time columns, time column, time columns>
<Time step, time step, time steps>
<Toe state>
<Toe status>
<TopK, topk>
<Total loss, total loss>
<Total sum, total sum>
<Total variance, total variance>
<Train generator, train generator>
<Trained network, train networks, trained network>
<Training Data, training data>
<Training Epochs, training epochs>
<Training Guides, Training guide>
<Training Training>
<Training continues, training continues>
<Training iterations, training iterations>
<Training loop, training loop, training loops>
<Training loss, training loss>
<Training model, Training models, training mode, training model, training models>
<Training parameters, training parameters>
<Training processes, training process, training processes>
<Training samples>
<Training statistics, training statistics>
<Training workflows, training works>
<Trajectory Clustering>
<Transfer Learning, Transfer learning, transfer learning>
<Transform diagonal>
<Transforms vectors>
<TriL, tril, trilinear>
<Triangular matrix, triangular matrices, triangular matrix>
<True Model>
<True labels, true label, true labels>
<True negatives, true negatives>
<True positives, true positives>
<Truncated Normal Distribution, truncated normal distribution>
<U32>
<UGRNN>
<UIMA>
<UINT8, uint8>
<UNK, unk>
<URDF>
<Ubuntu, ubuntu>
<Udacity>
<Uniform distribution, Uniform distributions, uniform distribution>
<Unstack, unstack, unstacking>
<Update layer>
<Update rule, update rule>
<Updates Histogram>
<Upper boundary, upper boundary>
<Upper triangle, upper triangle>
<Upsample, upsample, upsampled>
<Upsampling, upsampling>
<Upsampling1D>
<Upsampling2D>
<Upsampling3D>
<V1/V2>
<VAE>
<VBN>
<VCTK>
<VDM>
<VGG, VGG19, vgg19>
<VGG11, vgg11>
<VGG16, vgg16>
<VSMs>
<Validate dimensions>
<Validated type>
<Validation accuracy, validation accuracies, validation accuracy>
<Value storage>
<Values vector>
<Variable Resolution, variable resolution>
<Variable inputs>
<Variable node, Variable nodes>
<Variable tensors, variable tensor>
<Variable vector>
<Variant tensor>
<Variational, variational>
<Various function>
<Vector Formats, vector format>
<Vector Machines>
<Vector Representations, Vector representations, vector representations>
<Vector distributions>
<Vectorization, vectorization>
<Vectorized, vec, vectorized, vectorizer>
<Virginica, virginica>
<Vision functions>
<Vision layers>
<Visual Programming>
<Visual Studio>
<VisualVM>
<Visualizing Learning>
<Viterbi, viterbi>
<Viz, viz>
<WALS>
<WRT, wrt>
<Wasserstein distance>
<Weight Decay, Weight decay, weight decay>
<Weight Normalization, Weight normalization, weight normalization, weight norms>
<Weight matrix, weight matrices, weight matrix, weights matrix>
<Weight parameter, weight parameter, weight parameters>
<Weight updates, weight update, weight updates>
<Weighted loss, weighted loss, weighted losses>
<Weighted mean, weighted mean>
<Weighted sum, weighted sum>
<Wishart distribution, Wishart distributions>
<Woodbury matrix>
<Word2vec, word2vec>
<Wrapped inputs>
<XLA>
<Xcode>
<Xms>
<Xmx>
<YUV>
<Zero gradients, zero gradient, zero gradients>
<Zero padding, zero padding>
<Zoph>
<abs value>
<absolute accuracy>
<absolute difference>
<absolute error>
<absolute mean>
<absolute positions>
<absolute scope>
<absolute weights>
<academic literature>
<accelerated computing>
<access model>
<accidental seed>
<accordion element>
<account statistics>
<accumulated gradients>
<accumulation operation>
<accuracy calculation>
<accuracy evaluation>
<accuracy function>
<accuracy monitoring>
<accuracy score>
<accuracy statistics>
<accurate models>
<acos, acosh>
<acquires shape>
<action list>
<activated feature>
<activation array, activations array>
<activation equal>
<activation frequency>
<activation gradients>
<activation matrix>
<activation value, activation values>
<actor gradient>
<actual batch>
<actual computation>
<actual device>
<actual label>
<actual lengths>
<actual machine>
<actual training>
<adaptive step, adpative step>
<adb>
<addbmm, addmm, addmv>
<addcdiv>
<addcmul>
<adder function>
<additive factor>
<adi>
<adjacent pooling>
<adjoint, adjoints>
<adjustable dimensions>
<adjusted image>
<adjusted tensor>
<adjusting models>
<advanced model, advanced models>
<advanced operations>
<adversarial classification>
<adversarial loss>
<aeron channel, aeron channels>
<aeron context>
<affine layers>
<affine map>
<affine matrices>
<affine parameters>
<aggregate gradients>
<aggregate values>
<aggregated loss>
<aggregation process>
<aggressive type>
<aka linear>
<aka scalar>
<alarm rate>
<alexnet>
<algorithmic core>
<algorithmic part>
<alias input>
<alloc>
<allocation operation>
<allocation point>
<allocation unit>
<allreduce algorithm>
<alternating convolutions>
<amsgrad>
<analysis engine>
<analysis sample>
<analytic knowledge>
<analytical gradients>
<anchor images>
<angle value>
<anomalous trajectories>
<applied computation>
<apply activation>
<apply decay>
<applying inverse>
<applying layer>
<approximate distribution>
<approximate duality>
<approximation approach>
<approximation term>
<arange>
<arbitrary classifier>
<arbitrary dimension, arbitrary dimensions>
<arbitrary labels>
<arbitrary list>
<arbitrary matrix>
<arbitrary name>
<arbitrary tensors>
<area interpolation>
<argsort>
<argument diagonal>
<argument shape>
<argument tensors>
<array activations>
<array dimension, array dimensions>
<array shape, array shapes>
<arrow Tensor>
<arrow columns>
<arrow type>
<art model>
<artificial inputs>
<artificial neuron>
<asd>
<asinh>
<aspect ratios>
<assigned cluster>
<assignment operation>
<assignment values>
<associated list>
<associated name>
<associated step>
<assuming matrix>
<asynch>
<asynchronous GPU>
<asynchronous training>
<atan, atanh>
<atol>
<attention cell>
<attention distribution, attention distributions>
<attention module>
<attention outputs>
<attention vector, attention vectors>
<attention weights>
<attention window>
<auc value>
<autodiff>
<autogenerated>
<automated number>
<automatic dependencies>
<automatic input>
<automatic join>
<automatic kernel>
<automatic mechanism>
<automatic name>
<automatic procedure>
<automatic rank>
<automatic termination>
<autoregressive model>
<autoregressive>
<autotuning>
<auxiliary entropy>
<auxiliary name>
<auxiliary nodes>
<average AUC>
<average discriminator>
<average error>
<average frequency, averaging frequency>
<average gradient>
<average output>
<average popularity>
<average rate>
<average score>
<average sentence>
<average statistics>
<average time>
<average worker>
<averaged dimensions>
<averaged false>
<averaged parameters, averaging parameters>
<averaging calculation>
<averaging training>
<avoids name>
<axis argument>
<axis dimension, axis dimensions>
<axis number>
<background process>
<backticks>
<backward Function, backward function, backwards function>
<backward cell>
<backward correlation>
<backward graph>
<backward networks>
<backward operation>
<backward process>
<bad behavior>
<bad data>
<baddbmm>
<band matrix>
<based False>
<based checkpoint>
<based dimensions>
<based function>
<based models>
<basedir>
<baseline data>
<baseline model>
<baseline steps>
<baseline value>
<basic training>
<basic understanding>
<basically learning>
<batch algorithm>
<batch axis>
<batch come>
<batch dimension, batch dimensions>
<batch distribution>
<batch entries, batch entry>
<batch index>
<batch length>
<batch matrices, batch matrix>
<batch mean>
<batch member>
<batch message>
<batch normalized>
<batch operation>
<batch prediction>
<batch queue>
<batch rank>
<batch sample>
<batch slowly>
<batch state>
<batch statistics>
<batch transformation, batch transformations, batching transformation>
<batch value, batch values>
<batch variable>
<batch variance>
<batch vector>
<batch version>
<batched diagonal>
<batched inference>
<batched input, batched inputs>
<batched inverse>
<batched sequences>
<batched tensors>
<batches parameter>
<beam search>
<behavior Dropout>
<behind padding>
<behind tensor>
<belief network>
<best model, best models, good models>
<best path>
<best score>
<beta Tensor>
<beta function>
<beta integral>
<beta parameter, beta parameters>
<beta value>
<beta weight>
<better results, good results>
<bfloat16>
<bias array>
<bias centering>
<bias correction>
<bias graphs>
<bias matrices>
<bias shape>
<bias term>
<bias variable, biases variable>
<bias weights>
<bicubic>
<bid value>
<big data>
<big feature>
<big number>
<bijective>
<bilinear interpolation>
<bilinear transformation>
<bin values>
<binary case>
<binary dropout>
<binary labels>
<binary metrics>
<binary potentials>
<binary prediction>
<binary scores>
<binary state>
<binary tree>
<binary vector>
<bincount>
<binned values>
<bit index>
<black image>
<blank labels>
<blocking function>
<bmm>
<border values>
<bottom pixel>
<bound constraints>
<bound index, bound indices>
<bound one>
<bound values>
<boundary detection>
<bounds positions>
<box integration>
<boxes tensor>
<broadcast dimension, broadcast dimensions, broadcasted dimensions>
<broadcast shape, broadcasted shape>
<broadcast step>
<broadcastable dimension>
<broadcasting dimension, broadcasting dimensions>
<broadcasting matrix>
<btrifact>
<btriunpack>
<bucket result>
<bucket values>
<buil>
<building conditions>
<building data>
<building graphs>
<building input>
<building layers>
<building process>
<built batch>
<caching device>
<calculate sum>
<calculated mean>
<calculated shape>
<calculating top>
<calculation early>
<calculation errors>
<call matrix>
<call operator>
<called hidden>
<calling distribution>
<calling list>
<candidate cell>
<candidate generator>
<canonical ordering>
<capacitance matrix>
<capacity elements>
<capture state>
<cart left>
<cast operation>
<cat image>
<categorical identity>
<categorical input>
<categorical representation>
<categorical terms>
<categorical value, categorical values>
<ccache>
<ceiling function>
<celeba directory>
<cell clipping>
<cell function>
<cell inputs>
<cell state>
<cell type, cell types>
<centerX>
<central crop>
<central goal>
<central region>
<central result>
<central unit>
<certain shape>
<certain statistics>
<cflags>
<chain rule>
<channel dimension, channels dimension, channels dimensions>
<channel image>
<channel instructions>
<channel representation>
<channel values>
<character frequency>
<chatbot model>
<checkpoint data>
<checkpoint directory>
<checkpoint format>
<checkpoint name>
<checkpoint number>
<checkpoint path>
<checkpoint state>
<checkpoint value>
<checkpointing>
<chief worker>
<cholesterol level>
<cifar10>
<circulant matrix>
<circulant>
<circular mode>
<circular padding>
<clamping range>
<classes Tensor>
<classification difference>
<classification error>
<classification evaluation>
<classification head>
<classification matrix>
<classification models>
<classification network>
<classification one>
<classification performance>
<classification prediction>
<classification problem, classification problems>
<classification report>
<classification signature>
<classification task, classification tasks>
<classifier Estimators>
<classifier distance>
<classifier score>
<classifier trained>
<classifier tutorial>
<classnames>
<clip norm>
<clipped Tensor>
<clipped gradient>
<clipping function>
<clipping operation>
<clipping ratio>
<closest bound>
<closest cluster>
<closest integer>
<cloud storage>
<cluster center, cluster centers>
<cluster indices>
<cluster memory>
<cluster modes>
<clustering algorithm>
<clustering loss>
<cmake>
<cmdline>
<coarse parameter>
<coded vectors>
<codomain>
<coefficients moving>
<coeffs>
<col2im>
<collapse dimensions>
<collect statistics>
<collective operation>
<color channels>
<color saturation>
<colorbar>
<column Keys>
<column analysis>
<column axis>
<column batches>
<column conditions>
<column count>
<column dimension>
<column embedding>
<column equal>
<column factor, column factors>
<column indices>
<column reduction>
<column separator>
<column sweep>
<column transform>
<column vocab>
<column1>
<column2>
<columns tensor>
<columns value>
<combination feature>
<combinatorial coefficient>
<combined shape>
<combined tensor>
<combined value>
<come vector>
<coming gradient>
<common shape>
<communication patterns>
<comp graph>
<comparison array>
<compat>
<compatible shape>
<compilation process>
<complete index>
<complex Tensor>
<complex architecture>
<complex conjugation>
<complex frequency>
<complex graphs>
<complex input>
<complex intrinsic>
<complex methods>
<complex model>
<complex modulus>
<complex networks>
<complex pipeline>
<complex processing>
<complex relationships>
<complex sequence>
<complex transformation, complex transformations>
<complex vector>
<complicated network>
<complicated node>
<component tensor>
<componentwise>
<composed shape>
<composing layers>
<composite distribution>
<compound conditions>
<compound distribution>
<compression statistics>
<computation device>
<computation differed>
<computation history>
<computation layer, computation layers>
<computation leads>
<computation nodes>
<computation replicas>
<computation results>
<computation shape>
<computation time, computational time>
<computational cost>
<computational efficiency>
<computational power>
<computational semantics>
<computations involving>
<compute capabilities>
<compute cluster>
<compute correlation>
<compute derivatives>
<compute devices>
<compute diagonal>
<compute distribution>
<compute inputs>
<compute mean, computed mean, computes mean, computing mean>
<compute mode>
<compute moving>
<compute nodes>
<compute norms>
<compute output>
<compute predicted>
<compute predictions>
<compute shift>
<compute time>
<compute vector>
<compute weighted>
<computed Variable>
<computed index>
<computed loss>
<computing architecture>
<computing batches>
<computing entropy>
<computing iterations>
<computing machines>
<computing multilinear>
<concat>
<concatenated Tensor>
<concatenated result>
<concentration values>
<conceptual level>
<concrete examples>
<concrete shape>
<concrete terms>
<concurrent action>
<condition tensor>
<conditional distribution>
<conditional reduction>
<conditional replacement>
<conditional statement>
<confidence value>
<configs, configuraiton>
<configured probability>
<conjugate symmetry>
<connected Network, connected network>
<connected components>
<connected layer, connected layers>
<connected tensor>
<connection structure>
<connection weights>
<consistency loss>
<constant beta>
<constant development>
<constant length>
<constant memory>
<constant module>
<constant multiplier>
<constant rank>
<constant result>
<constant scalar>
<constant term>
<constant vector>
<constrained problem>
<constrained update>
<constructable>
<constructing data>
<construtor>
<content distance>
<context window>
<contiguous tensor>
<continue training, continuing training>
<continuous distributions>
<continuous function>
<continuous parameter, continuous parameters>
<continuous signal>
<continuous training>
<continuous value>
<continuous vector, continuous vectors>
<contracted Tensor>
<contracted dimensions>
<contracting dimension, contracting dimensions>
<contrastive loss>
<control dependencies, control dependency>
<control edges>
<control inputs>
<control outputs>
<control variable>
<controls number>
<conv layers>
<convergence problems>
<convert Tensor, convert tensor>
<convex combination>
<convolution algorithms>
<convolution arithmetic>
<convolution implementation>
<convolution kernels>
<convolution mode>
<convolution parameters>
<convolution rows>
<convolution weight>
<convolution window>
<cooccurrence>
<cooccurrences list>
<coordinate value>
<copy distribution>
<copy elements>
<copying data>
<core batch>
<core part>
<cores value>
<correct device>
<correct dropout>
<correct format>
<correct gradients>
<correct name>
<correct predictions>
<correct rank>
<correct shape>
<correct weights>
<correlation average>
<correlation coefficient>
<correlation computation>
<correlation score>
<corresponding Tensor, corresponding tensor, corresponding tensors>
<corresponding bound>
<corresponding dimension, corresponding dimensions>
<corresponding evaluation>
<corresponding examples>
<corresponding shapes>
<corrupted data>
<corrupted state>
<cosine part>
<cost array>
<cost function>
<count samples>
<count sequence>
<counter epochs>
<counter function>
<counter parallel>
<counter variable>
<counting statistics>
<counts steps>
<coupling layer>
<covariance matrices, covariance matrix>
<covariance type>
<cpp>
<critical problem>
<critical step>
<cron>
<crop window>
<cropped tensor>
<cropping dimension>
<cross entropy loss>
<crossed column, crossed columns>
<crossed feature>
<cube function>
<cumprod>
<cumsum>
<cumulative computation>
<cumulative distribution>
<cumulative probability>
<curated list>
<curve type>
<curve values>
<custom layer>
<custom op>
<customizable number>
<dW, double weights>
<dask>
<data Tensors, data tensor>
<data access>
<data anyway>
<data axbc>
<data compatibility>
<data copy>
<data count>
<data directories>
<data distribution>
<data examples>
<data exploration>
<data fetcher>
<data frame>
<data generator>
<data history>
<data indefinitely>
<data later>
<data layouts>
<data linearly>
<data manageable>
<data meaning>
<data module>
<data moved, data movement>
<data normalizers>
<data outputs>
<data pointers>
<data quickly>
<data region>
<data retention>
<data retrieved>
<data samples>
<data scientists>
<data sequentially>
<data sparsity>
<data splits>
<data tracking>
<data transform>
<data transmission>
<data vectors>
<data version, data versions>
<data yielded>
<database type>
<dataflow graph>
<datapoint>
<debug tensor>
<decay computation>
<decay exponentially>
<decay factor>
<decay function>
<decay value>
<decision process>
<declaring tensor>
<decoder layer, decoder layers>
<decoder name>
<decoding iterations>
<decoding step>
<decreasing frequency>
<decreasing length>
<dedicated GPU>
<deep NLP>
<deep model, deep models>
<deeper layers>
<definitive list>
<defs, defun>
<degenerate dimension, degenerate dimensions>
<degree node, degree nodes>
<delayed weights>
<deletion operation>
<delimited data>
<delta extraction>
<demuxer>
<dense Matrix, dense matrix>
<dense approach>
<dense behavior>
<dense column, dense columns>
<dense component>
<dense elements>
<dense embedding>
<dense feature, dense features>
<dense flow>
<dense input>
<dense labels>
<dense level>
<dense multiplication>
<dense one>
<dense option>
<dense output>
<dense representation>
<dense results>
<dense shape>
<dense type>
<dense vector, dense vectors>
<densenet121>
<density function, df>
<density ratio>
<density unit>
<dependency graph>
<dependent values>
<deploy model, deploy models>
<deps>
<depth dimension, depth dimensions>
<depth operation>
<depth values>
<desactivate>
<descending list>
<described tensor>
<designated device>
<desired dimensions>
<desired label>
<desired normalization>
<desired seed>
<destination graph>
<detailed shape>
<detection model>
<detection threshold>
<determine number>
<deterministic evaluation>
<deterministic function>
<deterministic order, deterministic ordering>
<dev1>
<device capability>
<device data>
<device device>
<device field>
<device function>
<device names>
<device pair>
<device path>
<device scope>
<device side>
<device spec>
<device strings>
<device systems>
<device tag>
<device tensor>
<device type>
<device value, device values>
<df1>
<df2>
<diag>
<diagflat>
<diagonal covariance>
<diagonal elements>
<diagonal part>
<diagonal tensor>
<diagonal term>
<diagonal vector>
<dialog models>
<diffeomorphic>
<difference operation>
<difference quotient>
<difference step>
<differential equation, differential equations>
<differential function, differential functions, differentiated function>
<digamma>
<dilation arguments>
<dilation factor>
<dilation rate, dilation rates>
<dim Tensor>
<dim value>
<dim1>
<dim2>
<dimenional array>
<dimension length>
<dimension number>
<dimension tensor, dimensional Tensor, dimensional Tensors, dimensional tensor, dimensional tensors>
<dimension value>
<dimensional Normal>
<dimensional batch>
<dimensional case>
<dimensional data>
<dimensional distribution>
<dimensional feature>
<dimensional input>
<dimensional locations>
<dimensional loss>
<dimensional matrices>
<dimensional mismatch>
<dimensional outputs>
<dimensional padding>
<dimensional projections>
<dimensional shapes>
<dimensional space>
<dimth dimension>
<direct array>
<directed graph>
<disabling gradient>
<discrete distribution>
<discrete input>
<discrete numerical>
<discrete staircase>
<discrete states>
<discrete value>
<discretization count>
<discretization error>
<discretized>
<discriminator function>
<discriminator model>
<discriminator steps>
<disk location>
<displaying data>
<distance bound>
<distance loss>
<distance measure>
<distance weights>
<distinct models>
<distortion power>
<distributed category>
<distributed computation>
<distributed confidence>
<distributed devices>
<distributed environment>
<distributed execution>
<distributed function, distributed functions>
<distributed group, distributed groups>
<distributed inference>
<distributed inputs>
<distributed job>
<distributed manner>
<distributed mode, distributed model>
<distributed module>
<distributed network>
<distributed numbers>
<distributed one>
<distributed parameters>
<distributed performance>
<distributed points>
<distributed process, distributed processes>
<distributed processing>
<distributed sessions>
<distributed storage>
<distributed synchronization>
<distributed tasks>
<distributed variable>
<distribution functions>
<distribution learned>
<distribution models>
<distribution root>
<distribution type, distribution types>
<distribution vectors>
<divison>
<dmax>
<doc count>
<domain prediction>
<domain theorem>
<dominant part>
<dot product>
<double Tensor, double tensor, double tensors>
<double comparisons>
<double matrices>
<double scalar>
<double vector>
<doubling algorithm>
<downsample, downsampling>
<downscaling>
<drawn samples>
<drop control>
<drop probability>
<dropout probabilities, dropout probability>
<dropout rate>
<dropout technique>
<dropout value>
<dtype, dtypes>
<dummy data>
<dummy dimension>
<dummy tensors>
<dump data>
<dump directory>
<dump root>
<dumped tensor>
<duplicate data>
<duplicate models>
<dx1>
<dx2>
<dy1>
<dy2>
<dynamic Shape, dynamic shape, dynamic shapes>
<dynamic calculation>
<dynamic condition>
<dynamic decoding>
<dynamic graph>
<dynamicaly>
<dz>
<early experimentation>
<early gradients>
<early termination>
<edge effect>
<edge information>
<edge list>
<edge number>
<edge padding>
<edge values>
<effecive>
<eig>
<eigen decomposition>
<eigen value, eigen values>
<einsum>
<either Tensor>
<either device>
<either steps>
<elastic difference>
<ellipsis dimensions>
<embedding column, embedding columns>
<embedding data>
<embedding dimension, embedding dimensions>
<embedding parameters>
<embedding results>
<embedding tensors>
<embedding values>
<embedding variable>
<embedding vector, embedding vectors>
<embedding weights>
<empty Tensor, empty tensor>
<empty features>
<empty rows>
<empty values>
<encoded image>
<encoded labels>
<encoder layer, encoder layers>
<encoder model>
<encoder name>
<encoding vector>
<end product>
<end range>
<energies tensor>
<energy term>
<engineering function>
<enough rate>
<entire layer>
<entire matrix>
<entire path>
<entire sample>
<entire tensor>
<entire variable>
<entropy introduced>
<entropy term>
<entropy value>
<entry index>
<epoch count>
<epoch indices>
<eps, evaluation process>
<epsilon input>
<equal dimensions>
<equal number>
<equal rank>
<equal results>
<equal shape, equal shapes>
<equivalent function>
<equivalent integer>
<equivalent location>
<erf, erfinv>
<erfc>
<error batch>
<error calculation>
<error correction>
<error free>
<error log>
<error normalized>
<error points>
<error propagation>
<error rates>
<error score>
<error text>
<error trying>
<estimated values>
<estimated variance>
<evaluate call>
<evaluate function>
<evaluate model>
<evaluation approaches>
<evaluation batch>
<evaluation call>
<evaluation classes>
<evaluation data>
<evaluation function>
<evaluation graph>
<evaluation labels>
<evaluation loop>
<evaluation mode, evaluation model>
<evaluation networks>
<evaluation result, evaluation results>
<evaluation run>
<evaluation stages>
<evaluation statistics>
<evaluation step, evaluation steps>
<evaluation workers>
<event dimensions>
<event distribution>
<event shape>
<exact calculation>
<exact dimensions>
<exact inverse>
<exact loss>
<exact parameter>
<exact process>
<example data>
<example graph>
<example images>
<example inputs>
<example model>
<example number>
<example path>
<example sequence>
<example tensor>
<execute operation>
<executing training>
<execution engine>
<execution graph>
<execution guide>
<execution turned>
<exogenous features>
<expanded dimensions>
<expanded distribution>
<expanded shape>
<expanding columns>
<expected shape>
<expected sum>
<exponential decay>
<external errors>
<external list>
<external model>
<extra dimension>
<extra layer>
<extra step>
<extract data>
<f32>
<facial recognition>
<factor matrix>
<fail training>
<failed node>
<failure rate>
<fake batch>
<fake data>
<fake label>
<false models>
<fast convergence>
<fast reply>
<fast response>
<fast solution>
<faster training>
<fatal signal>
<fault tolerance, fault tolerant>
<fc1>
<feature array, feature arrays, features array, features arrays>
<feature cross>
<feature data>
<feature dimension, features dimension>
<feature group>
<feature indices>
<feature information>
<feature list>
<feature type>
<feature vector, feature vectors, features vector>
<features argument>
<features axis>
<featurized data>
<feed values>
<feedforward>
<feeding inputs>
<fetch list>
<ffmpeg>
<field number>
<fifth column>
<filename1>
<filepath>
<fill value>
<filled array>
<filled tensor>
<filter Tensor, filter tensor, filter tensors>
<filter algorithm>
<filter data>
<filter dimensions>
<filter element>
<filter step>
<filtered sequence>
<final accuracy>
<final activation>
<final binary>
<final calculation>
<final dimension>
<final divide>
<final entry>
<final epoch>
<final evaluation>
<final gradient>
<final layer>
<final loss>
<final model>
<final name>
<final network>
<final nodes>
<final ordering>
<final parameters>
<final prediction>
<final signal>
<final step>
<final variable>
<final window>
<finite amount>
<finite difference>
<fit function>
<fit operation>
<fitting data>
<five layers>
<fixed batch>
<fixed dimension>
<fixed feature>
<fixed graph>
<fixed grid>
<fixed memory>
<fixed order>
<fixed rank>
<fixed shape>
<fixed threshold>
<fixed time>
<fixed window>
<flat array>
<flat indices>
<flat list>
<flat vector, float vector>
<flattened arrays>
<flattened input>
<flattened representation>
<flattened sequence>
<flattened tensor>
<flattening order>
<flip mode>
<flipped image>
<float Tensor, float Tensors, float tensor, float tensors>
<float beta>
<float model>
<float multiplier>
<float number, float numbers, floating numbers>
<float scalar>
<float types, floating type>
<float16>
<floor function>
<flow Tensor>
<flow boundary>
<fmod>
<fnr>
<foldl, foldr>
<form matrices>
<form one>
<forward activations>
<forward algorithm>
<forward call>
<forward domain>
<forward examples>
<forward function, forward functions>
<forward graph>
<forward inference>
<forward layer>
<forward loop>
<forward model>
<forward network, forward networks>
<forward operation>
<forward step>
<forward transformation>
<forward variable, forward variables>
<fourth dimensions>
<fpr>
<frac>
<frame length>
<free functions>
<freq>
<frequency counting>
<frequency intensities>
<frequency spectrum>
<frequency value, frequent values>
<frequency weight, frequency weights>
<front optimization>
<froot>
<frozen graph>
<frozen layer, frozen layers>
<full Indexing>
<full batch>
<full cross>
<full data>
<full epoch>
<full features>
<full gradient>
<full graph>
<full grid>
<full input>
<full loss>
<full rank>
<full sequences>
<full statistics>
<full update>
<full variable>
<full window>
<fully connected layer>
<fully connected>
<function interface>
<functional layer, functional layers>
<functional transform>
<functor>
<fundamental flag>
<fundamental type>
<fundamental unit>
<gRPC>
<gain value>
<gamma Tensor>
<gamma parameter>
<gamma weight>
<garcon>
<gate bias>
<gate weights>
<gather values>
<gaussian functions>
<gbar>
<gemm>
<general function>
<general matrix>
<general operation>
<general point>
<general quantization>
<general solution>
<generalization performance>
<generalized contraction>
<generalized eigenvalues>
<generalized version>
<generate function>
<generating data>
<generating layer>
<generating time>
<generative chatbot>
<generator input, generator inputs>
<generator loss>
<generator output>
<generator queue>
<generator steps>
<generic Tensor>
<generic condition>
<generic data>
<generic distribution>
<generic regression>
<geoip>
<geqrf>
<gitter>
<global batch>
<global flow>
<global index>
<global list>
<global mean>
<global norm>
<global pooling>
<global rank>
<global ratio>
<glove model>
<glu>
<gnuplot>
<good accuracy>
<good approximation>
<good examples>
<good prediction>
<good start>
<gradient Methods, gradient methods>
<gradient accumulators>
<gradient behavior>
<gradient calculation, gradient calculations>
<gradient clipping, gradients clipping>
<gradient comparisons>
<gradient computated>
<gradient computed, gradients computed>
<gradient descent, gradient descents>
<gradient equal>
<gradient error>
<gradient flattening>
<gradient formula>
<gradient function>
<gradient implementation>
<gradient noise>
<gradient op, gradient operation>
<gradient outputs>
<gradient penalty>
<gradient square, gradient squared>
<gradient step>
<gradient tensor, gradient tensors, gradients tensor>
<gradient terms>
<gradient update, gradient updates, gradients updates>
<gradient values>
<gradient variable>
<gradients point>
<gram Model, gram model>
<gram matrix>
<gram product>
<graph computation>
<graph data>
<graph element, graph elements>
<graph function>
<graph mode, graph model>
<graph node, graph nodes>
<graph operation>
<graph proto>
<graph structure>
<graph vectors>
<gray image>
<great idea>
<great learning>
<greater length>
<grid point>
<ground truth>
<group path>
<groupname>
<groups layers>
<growing list>
<h5py>
<hacky>
<half line>
<half type>
<hand function>
<handling training>
<hanged process>
<hard error>
<hasher name>
<headed models>
<hermitian>
<hidden consists>
<hidden dimension>
<hidden distribution>
<hidden input, hidden inputs>
<hidden state, hidden states>
<hidden surface>
<hidden unit, hidden units>
<hidden weights>
<hierarchical order>
<high dimension, higher dimension>
<high padding>
<high score>
<higher Tensor>
<highest frequency>
<highest probability>
<highest results>
<hinge loss>
<histogram structure>
<historical state>
<holdout data>
<horizontal array>
<horizontal graph>
<hot Tensor, hot tensor>
<hot columns>
<hot encoding>
<hot feature>
<hot labels>
<hot matrix>
<hot representation>
<hot vector, hot vectors>
<hspmm>
<hue channel>
<huge gradients>
<huge network>
<human brain>
<human intuition>
<human labor>
<hundred steps>
<hyper parameter, hyper parameters>
<hyperthreading>
<hypothesis sequences>
<i1e>
<ideal amount>
<ideal value>
<identical behavior>
<identical length>
<identical name>
<identical rank>
<identical state>
<identity operator>
<identity tensor>
<identity values>
<idx>
<ifconfig>
<ifft, irfft>
<im2col>
<image activations>
<image batch>
<image bounds>
<image domain>
<image localization>
<image mode, image model>
<image projective>
<image randomly>
<image result, image results>
<imagenet>
<images total>
<img1>
<img2>
<immediate error>
<improved stability>
<inbound layer, inbound layers>
<inbound nodes>
<incoming weights>
<incomplete batch>
<inconsistent shapes>
<incorrect number>
<incorrect shapes>
<increasing elements>
<increment values>
<incremental behavior>
<independent input>
<independent layers>
<independent sample, independent samples>
<independent scaling>
<index element>
<index tensor>
<index vector>
<indexed values>
<indexing operation>
<indexing tensors>
<indicator column, indicator columns>
<indicator function>
<individual GPU>
<individual dimension, individual dimensions>
<individual examples>
<individual graph>
<individual layer>
<individual matrix>
<induced vector>
<inference features>
<inference mode>
<inference operation>
<inference phase>
<inference results>
<inference step>
<inferred shape, inferred shapes>
<inferred vector>
<infinity norm>
<infs>
<initial alignment>
<initial attentions>
<initial batch>
<initial bias>
<initial cell>
<initial coordination>
<initial encoding>
<initial entropy>
<initial entry>
<initial gradient, initial gradients>
<initial hidden>
<initial iterations>
<initial learning>
<initial matrix>
<initial model>
<initial module>
<initial network>
<initial observation>
<initial one, initial ones>
<initial shape>
<initial startup>
<initial states>
<initial step>
<initial variance>
<initial vector>
<initial weight, initial weights>
<initiate training>
<inits>
<ink data>
<inner dimension>
<inner layer>
<inner list>
<inner loop>
<inner model>
<inner sum>
<innermost dimension>
<innermost matrix>
<inp, inplace>
<input Examples, input example, input examples>
<input Input, input input>
<input Variables, input variable, input variables>
<input batch, input batches, inputs Batch, inputs batch>
<input channel, input channels>
<input constraints>
<input desired>
<input details>
<input dictionary>
<input dimension, input dimensions>
<input dropout>
<input embedding>
<input exception>
<input filled>
<input filters>
<input flag>
<input gate>
<input gradient>
<input index, input indices>
<input key, input keys>
<input labels>
<input list>
<input loss>
<input mean>
<input mini>
<input models>
<input multiple, input multiples>
<input names>
<input network>
<input operand>
<input pixels>
<input planes>
<input projection>
<input queues>
<input range>
<input rank, input ranks, inputs rank>
<input receivers>
<input records>
<input resolution>
<input row, input rows>
<input scalar>
<input schema>
<input section>
<input sentence, input sentences>
<input signal>
<input signature>
<input slice>
<input space>
<input spatial>
<input specs>
<input split>
<input statistics>
<input steps>
<input strategy>
<input stride>
<input summaries>
<input tags>
<input trying>
<input vector>
<input window>
<input words>
<inputs data>
<inputs flow>
<inputs tuple>
<inputs values>
<insertion indices>
<insignificant amount>
<installation loop>
<integer count>
<integer distributions>
<integer interval>
<integer label, integer labels>
<integer list>
<integer multiple>
<integer scalar>
<integer vector>
<integration better>
<integration process>
<integration result>
<intensity distribution>
<interactive node>
<intermediate layers>
<intermediate model>
<intermediate tensors>
<intermediate value>
<internal tensor>
<interop>
<interpolant>
<interpretting>
<interval indices>
<interval level>
<intrinsic conditioning>
<intrinsic parameters>
<inv>
<invalid graphs>
<invalid inputs>
<invalid shape>
<inverse domain>
<inverse functions>
<inverse gamma>
<inverse permutation>
<inverse scale>
<inversion process>
<inverted dropout>
<inverted index>
<invocation probability>
<iostream>
<iris directory>
<irregular intervals>
<isfinite>
<isinf>
<isnan>
<iterative training>
<iters>
<jitter saturation>
<jittered image>
<jobz>
<join examples>
<jth>
<junit>
<kernel elements>
<kernel function>
<kernel matrix>
<kernel points>
<kernel tensor>
<key tensors>
<keypoint>
<kmeans>
<known rank>
<known shape>
<known value>
<label binary>
<label classes, label classification>
<label column>
<label equality>
<label field>
<label format>
<label list>
<label name, label names>
<label numbers>
<label one>
<label problems>
<label regression>
<label smoothing>
<label strings>
<label vector, labels vector>
<label weight>
<labeled point>
<labels argument, labels arguments>
<labels array, labels arrays>
<labels dimension>
<labels distribution>
<labels input>
<labels nearest>
<labels source>
<large batch, large batches, larger batch>
<large corpora>
<large degree>
<large elements>
<large feature>
<large graphs>
<large images>
<large input, large inputs>
<large machine>
<large messages>
<large models>
<large networks, larger networks>
<large sequences>
<large tensor, large tensors, larger tensor>
<large text>
<larger array>
<larger checkpoint>
<larger clusters>
<larger indices>
<larger matrix>
<larger step>
<larger variable>
<larger variance>
<largest interval>
<latent state>
<latent values>
<latent vector>
<latter behavior>
<layer Input, layer input, layer inputs>
<layer batch>
<layer index>
<layer weights>
<layer1>
<lcc>
<ldflags>
<lead dimension>
<leading dimensions>
<leaf Tensors>
<leaf parameter, leaf parameters>
<leaf variable>
<leakyrelu>
<leaning model, learned models>
<learn classifier>
<learnable bias>
<learnable temperature>
<learnable weights>
<learned array>
<learned parameters>
<learning andreiliphd>
<learning behavior>
<learning capability>
<learning category>
<learning level>
<learning models>
<learning parameters>
<learning paulsteven>
<learning process>
<learning progress>
<learning rate decay>
<learning rate decrease>
<learning workflow>
<learning workloads>
<least rank>
<left Methods>
<left boundary>
<left data>
<left memory>
<left multiplication>
<left node>
<left value>
<leftmost dimensions>
<length multiple>
<length rank>
<length sequences>
<length training>
<lengths tensor>
<lerp>
<level abstraction>
<level intuitive>
<level normalization>
<level seed>
<level systems>
<libgomp>
<libjpeg>
<libsvm>
<life graph>
<likelihood loss>
<likelihood operation>
<line content>
<line tutorial>
<linear Independent>
<linear activation, linear activations>
<linear approximation>
<linear classifier>
<linear equations>
<linear estimator>
<linear interpolation>
<linear memory>
<linear network>
<linear outputs>
<linear policy>
<linear prediction, linear predictions>
<linear projection>
<linear region>
<linear scale>
<linear scaling>
<linear system>
<linear term>
<linear transformation, linear transformations>
<linear unit>
<linguistic data>
<link function>
<link line>
<little counterintuitive>
<little frustrated>
<lltm>
<load Parallelism>
<load function, load functions>
<load operation>
<load parameters>
<load process>
<load status>
<load steps>
<load weights, loading weights>
<loaded graph>
<loaded model>
<loading code>
<loading embedding>
<loading part>
<loading rows>
<loads data>
<loc Tensor>
<local Translation>
<local directory>
<local features>
<local graph>
<local rank>
<local seed>
<local servers>
<local statistics>
<local step, local steps>
<local task>
<local tensor>
<local training>
<locally connected>
<log data>
<log determinant>
<log directory>
<log elements>
<log factorial>
<log likelihood>
<log normalizing>
<log potentials>
<log sigmoid>
<log softmax>
<log space>
<log value>
<log1p>
<log1pexp>
<log2>
<logarithm function>
<logcosh>
<logdet>
<logdir>
<logging step>
<logical analogies>
<logical core>
<logical shape>
<logical sum>
<logistic loss>
<logit>
<logits Tensor, logits tensor>
<logits function>
<logits layer>
<logits shape>
<logspace>
<logsumexp>
<long matrix>
<longest sequence>
<looking curve>
<loop body>
<loop condition>
<loop counter>
<loop derivative>
<loop iterations>
<loop state>
<loop variable, loop variables>
<loss Methods>
<loss applied>
<loss calculated>
<loss comparisons>
<loss component>
<loss criterion>
<loss data>
<loss definition>
<loss element>
<loss fields>
<loss go>
<loss layer>
<loss module>
<loss problem>
<loss really>
<loss reduction>
<loss reported>
<loss score>
<loss shape>
<loss summaries>
<loss value, loss values>
<loss weights>
<losses dimension>
<low accuracy>
<low padding>
<low probabilities>
<low rank, lower rank>
<low values>
<low volume>
<lower boundary>
<lower frequency>
<lower latency>
<lower memory>
<lower performance>
<lower range>
<lower triangle>
<lower triangular>
<lucene>
<lxtGH>
<machine failures>
<machine training>
<magnitude values>
<main channel>
<main column>
<main diagonal>
<main graph>
<main group>
<manip>
<manipulating tensors>
<manual call>
<manual manipulation>
<manual variable>
<many Tensor, many tensor>
<many steps>
<map dimensions>
<map function>
<map node>
<map structure>
<map transformation>
<mapped indices>
<mapping tensor>
<maps Tensor>
<margin constant>
<mask array, mask arrays>
<mask tensor>
<masking layers>
<masking value>
<mass function>
<master GPU>
<master branch>
<master procedure>
<mat1>
<mat2>
<matching length>
<matching rank>
<matching shapes>
<matching type>
<mathematical components>
<matmul>
<matplotlib>
<matrix Tensor>
<matrix addition>
<matrix columns>
<matrix determinant>
<matrix equations>
<matrix exponential>
<matrix implied>
<matrix inverse>
<matrix logarithm>
<matrix manipulations>
<matrix matrix>
<matrix norm, matrix norms>
<matrix operation, matrix operations, matrix ops>
<matrix rank>
<matrix subtraction>
<matrix unchanged>
<max capacity>
<max depth>
<max element>
<max index>
<max mode>
<max norm, max norms>
<max scaling>
<max steps>
<max time>
<max values>
<maximal number>
<maximal values>
<maximum absolute>
<maximum count>
<maximum device>
<maximum factor>
<maximum feature>
<maximum index, maximum indices>
<maximum norm>
<maximum parallelism>
<maximum popularity>
<maximum rank>
<maximum scoring>
<maximum shape>
<maximum suppression>
<maximum vocabulary>
<maxpool2d>
<mean Tensor>
<mean estimate>
<mean function>
<mean intersection>
<mean magnitudes>
<mean number>
<mean operation>
<mean semantic>
<mean value, mean values>
<mean vector, mean vectors>
<mean weight>
<median value>
<mel>
<memcpy>
<memory length>
<mental model>
<merged result>
<merged value>
<meshgrid>
<metric function, metric functions>
<metric names>
<metric value>
<middle dimension>
<migration guide>
<miminum values>
<minibatch Tensor, minibatch tensor, minibatch tensors>
<minibatch dimension>
<minibatch elements>
<minibatch training>
<minimization problem>
<minimize loss>
<minimize true>
<minimum bounds>
<minimum length>
<minimum norm>
<minimum operation>
<minimum range>
<minimum rank>
<minimum threshold>
<minlength>
<minor dimension>
<mirrored values>
<mirrored variable>
<mixed Tensors>
<mixing probabilities>
<mixture model>
<mixture probabilities>
<mobile device, mobile devices>
<mobile systems>
<mode loss, model loss>
<mode model, model modes>
<mode value>
<model True>
<model accuracy>
<model architectures>
<model argument>
<model attempts>
<model based>
<model checkpoint, model checkpoints>
<model cloning>
<model code>
<model components>
<model configurations>
<model construction>
<model copy>
<model created>
<model creation>
<model debugging>
<model detects>
<model developers>
<model development>
<model estimator>
<model every>
<model export>
<model fails>
<model faster>
<model feeding>
<model formats>
<model forward>
<model frequently>
<model graph>
<model heads>
<model improves>
<model individually>
<model inferences>
<model information>
<model initialization>
<model involves>
<model knows>
<model learning, model learns>
<model let>
<model loaded>
<model locally>
<model longer>
<model metrics>
<model operates>
<model outside>
<model parallel>
<model parallelism>
<model path>
<model periodically>
<model predicts>
<model producers>
<model progressed, models progress>
<model protobufs>
<model pruning>
<model relies>
<model repeatedly>
<model replica, model replicas, model replication>
<model repository>
<model robust>
<model spec>
<model stands>
<model stop, model stops>
<model topology>
<model trainer>
<model types>
<model variable, model variables>
<model weights>
<model zoo>
<model1>
<model2>
<models mean>
<modulus operation>
<moment correlation>
<momentum rate>
<monitor data, monitoring data>
<monitor memory>
<monitor similarity>
<monitor training>
<monotonic function>
<morphological dilation>
<morphological features>
<move data, moving data>
<move nodes>
<move result>
<move tensors>
<moving average, moving averages>
<moving column>
<moving mean>
<moving statistics>
<moving variance>
<moving window>
<mtimes>
<multi array>
<multi core>
<multi dimensional>
<multi input>
<multi label>
<multi layer>
<multi nodes>
<multi word>
<multimodal distributions>
<multiple Examples, multiple examples>
<multiple Integer, multiple integer>
<multiple array>
<multiple authors>
<multiple axes>
<multiple batches>
<multiple checkpoints>
<multiple column>
<multiple consequent>
<multiple cores>
<multiple countries>
<multiple critical>
<multiple cycles>
<multiple dependencies>
<multiple devices>
<multiple domains>
<multiple edges>
<multiple encoder>
<multiple epochs>
<multiple evaluation>
<multiple features>
<multiple gradients>
<multiple graphs>
<multiple heads>
<multiple images>
<multiple independent>
<multiple indices>
<multiple iterations>
<multiple jobs>
<multiple machines>
<multiple measurements>
<multiple models, multiple modes>
<multiple numerical>
<multiple queue>
<multiple replicas>
<multiple retries>
<multiple series>
<multiple sessions>
<multiple shapes>
<multiple tensors>
<multiple topics>
<multiple towers>
<multiple transforms>
<multiple workers>
<multiplication algorithm>
<multiplication operation>
<multiplicative coefficient>
<multiplicative noise>
<multiplicative scaling>
<multiplying scale>
<multitask learning>
<multivariate distributions>
<multivariate series>
<mutiple values>
<mutual information>
<muxer>
<mvlgamma>
<mvn>
<name tensor>
<named function, named functions>
<named job>
<naming inputs>
<naming sequence>
<natural exponential>
<natural left>
<natural log>
<natural number>
<natural parameters>
<nd2>
<ndarray, ndarrays>
<ndf>
<ndims>
<ndv>
<near feature>
<nearest clusters>
<nearest neighbors>
<negative axis>
<negative correlation>
<negative count>
<negative determinant>
<negative edge>
<negative example>
<negative exists>
<negative factor>
<negative means>
<negative samples>
<negative scalar>
<negative section>
<negative sum>
<negative trials>
<negative words>
<neighbor points>
<neither name>
<nested combination>
<nested graph>
<nested parameter>
<nested range>
<nested sequences>
<nested tuple>
<network Input, network input>
<network configurations>
<network connectivity>
<network convergence>
<network criterion>
<network graph>
<network issues>
<network layers>
<network performing>
<network processing>
<network security>
<network state>
<network storage>
<network traffic>
<network weights>
<neural Tensor, neural tensor>
<neural architecture>
<neural layers>
<neural vocoder>
<neutral type>
<new axis>
<newb>
<next calculation>
<next cell>
<next channel>
<next checkpoint>
<next data>
<next image>
<next index>
<next layer>
<next nodes>
<next order>
<next states>
<ngpu>
<ngrams>
<nn module>
<nnnnn>
<nnz>
<node goes>
<node labels>
<node1>
<noise distribution>
<noise patterns>
<noisy example>
<non decreasing>
<non edge>
<non tensor>
<nonlinear models>
<nonlinearity function>
<norm constraints>
<norm degree>
<norm distance>
<norm problem>
<norm regularization>
<normal Variable, normal Variables>
<normal arrays>
<normal convolution>
<normal development>
<normal implementation>
<normal noise>
<normal numbers>
<normal options>
<normal pattern>
<normal scalar>
<normalization applied>
<normalization constant>
<normalization dimension>
<normalization issues>
<normalization methods>
<normalization part>
<normalization statistics>
<normalization strategies, normalization strategy>
<normalization values>
<normalization window>
<normalize data>
<normalized amplitude>
<normalized array>
<normalized copy>
<normalized image>
<normalized inputs>
<normalized model>
<normalized text>
<normalized vectors>
<normalized weights>
<normalizing constant>
<normalizing flow>
<normalizing labels>
<npairs loss>
<nrow>
<ntc>
<ntuple>
<nuclear norm>
<nullptr>
<num1>
<num2>
<numbers log>
<numel>
<numeric column, numerical columns>
<numeric differences>
<numerical data>
<numerical feature>
<numerical instabilities>
<numerical issues>
<numerical precision>
<numerical properties>
<numerical range, numerical ranges>
<numerical rank>
<numerical stability>
<numerical values>
<nvtx>
<oW, one word, ow>
<objective learning>
<occurrence count>
<occurring values>
<old GPU>
<old behavior>
<old graph>
<old inputs>
<old matrix>
<old models, older model>
<older checkpoint>
<one GPU>
<one Graph>
<one accordion>
<one batch>
<one caveat>
<one chart>
<one checkpoint>
<one computer>
<one contracting>
<one detail>
<one epoch>
<one feature>
<one fine>
<one forward>
<one gate>
<one go>
<one gradient>
<one ground>
<one head>
<one hot>
<one inbound>
<one job>
<one kernel>
<one label>
<one larger>
<one loads>
<one mapped>
<one metric>
<one minds>
<one minibatch>
<one minority>
<one network>
<one operand>
<one operating>
<one partition>
<one platform>
<one probability>
<one queue>
<one rank>
<one replica>
<one run>
<one scalar>
<one site>
<one storage>
<one subtle>
<one suggested>
<one tick>
<one title>
<one tower>
<one training>
<one transform>
<one values>
<one vector>
<one weight>
<onesided>
<online computation>
<online training>
<onto CPU>
<onto tensors>
<op type>
<opJ>
<open node>
<open problem>
<operand shape>
<operation Motivation>
<operation time>
<opposite similarity>
<optimal batches>
<optimal value>
<optimality>
<optimizable>
<optimization functions>
<optimization problem>
<optimization results>
<optimization routines>
<optimization runner>
<optimization step>
<optimization techniques>
<optional bias>
<optional cell>
<optional conditioning>
<optional float>
<optional inputs>
<optional randomisation, optional randomization>
<optional weight>
<orange line>
<orgqr, ormqr>
<original column, original columns>
<original device>
<original element>
<original matrix>
<original model>
<original parameters>
<original shape>
<original tensor>
<orthogonal columns>
<orthogonal decomposition>
<orthogonal matrix>
<orthogonal normalization>
<orthogonal rows>
<orthonormal>
<outcome matrices, outcome matrix>
<outcome vectors>
<outer dimension>
<outer list>
<outer operation>
<outermost axis>
<outlining number>
<output activation, output activations>
<output arguments, outputs argument>
<output batch>
<output classification>
<output column, output columns>
<output dependencies>
<output depth>
<output details>
<output diagonal>
<output dimension, output dimensionality, output dimensions>
<output dropout>
<output elements>
<output feature, output features>
<output fields>
<output filters>
<output functions>
<output histogram>
<output image>
<output index>
<output integers>
<output interval>
<output keys>
<output labels>
<output masking>
<output matrices, output matrix>
<output model>
<output names>
<output networks>
<output neurons>
<output node, output nodes>
<output predictions>
<output projection>
<output range>
<output rate>
<output result>
<output sentence>
<output sequence>
<output sides>
<output slice>
<output spatial>
<output split>
<output steps>
<output strategies, output strategy>
<output stride>
<output structure>
<output symbols>
<output training>
<output type>
<output units>
<output validation>
<output value>
<output variable>
<output vector, output vectors>
<output vertex, output vertices>
<output zeros>
<overall amount>
<overall batch>
<overall computation>
<overall data>
<overall model>
<overall variance>
<overlap values>
<overlapping parameter>
<overlaps tensor>
<overridden shape>
<p2p>
<packt>
<pacman>
<padded elements>
<padded tensor>
<padding algorithm>
<padding argument>
<padding arrays>
<padding dimension>
<padding mode, padding modes>
<padding number>
<padding type>
<padding value, padding values>
<pair mean>
<pam>
<pandas data>
<paper Learning>
<parallel inputs>
<parallel learning>
<parallel region>
<parallel threads>
<parallel version>
<parallelism model>
<parameter devices>
<parameter gradients>
<parameter graphs>
<parameter groups>
<parameter histograms>
<parameter information>
<parameter loc>
<parameter search>
<parameter shapes>
<parameter synchronization>
<parameter table>
<parameter tensor>
<parameter update, parameter updates>
<parameter upper>
<parameter vector>
<parameterized distribution>
<parameterized graph>
<parameterized model>
<parameters concentration>
<parameters values>
<parametric constraints>
<parent graph>
<partial derivative, partial derivatives>
<partial differential>
<partial execution>
<partial forward>
<partial inverse>
<partial list>
<partial run, partial runs>
<partial updates>
<participating device>
<particular cluster>
<particular devices>
<particular dimension>
<particular examples>
<particular layer>
<particular problem>
<particular shape>
<particular step>
<particular worker>
<partition function>
<partition number>
<partitioning information>
<partitioning strategy>
<pass shape>
<passing dimensions>
<penalize length>
<per device>
<per layer>
<per step>
<percentile value>
<perform dropout>
<perform operation>
<perform training>
<performance computing>
<performance distribution>
<performance results>
<performing computation>
<performing feature>
<performing model>
<performs function>
<permutation indices>
<permutation matrix>
<permutation operation>
<permutation results>
<permuted dimensions>
<persistable>
<persistence level>
<persistent state>
<persistent tensor>
<perturbations start>
<perturbed image>
<physical checkpoint>
<piecewise linear>
<pip3>
<pipeline optimization>
<pixel convolution>
<pixel flow>
<pixel images>
<pixel positions>
<place operation>
<place tensor>
<placeholder operation>
<placeholder tensor, placeholder tensors>
<plain mode>
<plane convolution>
<plot image>
<plotting statistics>
<point Tensor, points tensor>
<point addition>
<point collectives>
<point communcation, point communication>
<point comparison>
<point division>
<point error>
<point graph>
<point index, point indexes>
<point quantization>
<point scalar>
<point tree>
<policy gradient>
<policy network>
<polygamma>
<polygonal data>
<polynomial factors>
<polynomial order>
<pool2d>
<pooling dimensions>
<pooling filter>
<pooling modes>
<pooling operation>
<pooling regions>
<pooling sequence>
<pooling type>
<pooling window>
<poor cluster>
<poor result>
<popularity nodes>
<portion shape>
<positive cloud>
<positive correlation>
<positive count>
<positive definite>
<positive diagonals>
<positive elements>
<positive error>
<positive event>
<positive examples>
<positive function>
<positive images>
<positive labels>
<positive skew>
<positive weight, positive weights>
<possible number>
<post noise>
<post training>
<post update>
<postgres>
<potrf, potrs, pstrf>
<power function>
<powerful models>
<pprof>
<pre computed>
<pre process, pre processor>
<pre processing>
<pre train, pre training>
<pre update>
<preactivation values>
<preceding function>
<precision array>
<precision complex>
<precision data>
<precision format>
<precision function>
<precision matrix>
<precision recall>
<precompute>
<preconditioner, preconditioners>
<predicted classes>
<predicted label, predicted labels>
<predicted outputs>
<predicted probabilities, predicted probability>
<predicted vector>
<predicted word>
<prediction algorithm>
<prediction array>
<prediction batch>
<prediction call>
<prediction error, prediction errors>
<prediction graph>
<prediction head>
<prediction labels>
<prediction output>
<prediction part>
<prediction probabilities, prediction probability>
<prediction rule>
<prediction task>
<prediction time>
<predictions arguments>
<predictions dictionary>
<predictions negative>
<predictions parameter>
<predictions tensors>
<predictive performance>
<predictive power>
<preprint>
<prerelease>
<prespecified number>
<pretrainable layers>
<pretrained model, pretrained models>
<pretrained network>
<prevent numerical>
<previous approach>
<previous batch>
<previous epoch>
<previous image>
<previous layer, previous layers>
<previous steps>
<primal loss>
<primary distributions>
<primitive operation>
<prior distribution>
<prior probability>
<probabilistic interpretation>
<probabilistic model>
<probability density function>
<probability function, probability functions>
<probability histogram>
<probability matrix>
<probability predictions>
<probelm>
<proce, processable>
<process cluster>
<process termination>
<processed vector>
<processing batches>
<processing indices>
<processing path>
<processing queue>
<processing speed>
<processing steps>
<processing units>
<produce device>
<product name>
<product operation>
<profile step>
<profiling mode>
<projected factors>
<projected value>
<projection layer>
<projection matrices, projection matrix>
<propagation step>
<proper data>
<proper shape>
<proto file>
<protobuf>
<protos>
<pruned structure>
<pruning functions>
<pseudo function>
<pseudo inverse>
<ptrblck>
<pure function>
<pybind11>
<quad elements>
<quadratic equation>
<quantile, quantiles>
<quantized models>
<quantized training>
<quantized type>
<queue element>
<quickdraw>
<radius parameters>
<ragged matrix>
<ragged vector>
<randint>
<randn>
<random Shard, random shard>
<random accuracy>
<random batch, random batches>
<random behavior>
<random crop>
<random data>
<random factor>
<random gradients>
<random index>
<random inputs>
<random integers>
<random matrix>
<random noise>
<random one>
<random operations>
<random outcome>
<random perturbations>
<random port>
<random postfix>
<random projection, random projections>
<random records>
<random scales>
<random selection>
<random shape>
<random shuffling>
<random transformations>
<random variable, random variables>
<random variation>
<random vector>
<random walk, random walks>
<random weights>
<randomized algorithm>
<randomized transform>
<randomness seed>
<range inputs>
<rank Tensor, rank Tensors, rank tensors>
<rank approximation>
<rank array, rank arrays>
<rank loss>
<rank shape>
<rank-1>
<rank-2>
<rastered>
<rate parameter, rate parameters>
<rate signal>
<raw features>
<raw tensor>
<re2>
<read image>
<read tensor>
<real batch>
<real convolution>
<real distribution>
<real environment, real environments>
<real interval>
<real label, real labels>
<real matrix>
<real multi>
<real numbers>
<real outcomes>
<real person>
<real shape>
<real tasks>
<real type>
<real usage>
<real valued, real values>
<real word, real words>
<reasonable name>
<recall array>
<recall function>
<recall values>
<received tensor, receiver tensor>
<receiving process>
<recent checkpoint>
<recent model>
<recod>
<recognition network>
<recognition systems>
<recommended number>
<reconstructed data>
<reconstructed images>
<reconstruction distribution, reconstruction distributions>
<reconstruction error>
<reconstruction function>
<reconstruction probabilities, reconstruction probability>
<recurrent cell>
<recurrent weights>
<recursive structure>
<reduce batch>
<reduce index, reduced indices>
<reduce number>
<reduce product>
<reduce sum, reduced sum>
<reduce training>
<reduced Tensor, reduced tensor>
<reduced array>
<reduced dimensions, reducing dimensions>
<reduced memory>
<reduced values>
<reduction algo>
<reduction index>
<reduction mode>
<reduction occurs>
<reduction ratio>
<reduction results>
<redundant shapes>
<ref tensor>
<reference batch>
<regression algorithms>
<regression evaluation>
<regression head>
<regression labels>
<regression metric>
<regression model, regressive model>
<regression performance>
<regression problem, regression problems>
<regression score>
<regularization arguments>
<regularization factor>
<regularization loss, regularization losses>
<regularization multiplier>
<regularization parameters>
<regularization penalty>
<regularization term>
<reinforcement algorithm>
<rejection probabilities>
<rejection sampling>
<relationship column>
<relative accuracy>
<relative error>
<relative probability>
<relative similarity>
<release process>
<releases shape>
<reliability curve>
<reliability weights>
<remaining dimensions>
<remote case>
<remote development>
<remote devices>
<remote graph>
<remote task>
<removing examples>
<renorm>
<reparameterized sample>
<repartition data>
<repeat factor>
<repeat transformation>
<replaced Tensors>
<replica index>
<replica training>
<replicated mode>
<repmat>
<repo, repro>
<report device>
<reporting frequency>
<representing data>
<required parameter>
<residual layer, residual layers>
<residual network>
<residual parameters>
<residual sum>
<resnet18>
<resnet50>
<resnet>
<resolution model>
<respective shape>
<restore values>
<result Tensor, result tensor, resulting Tensor, resulting tensor>
<result nodes>
<result shape, resulting shape>
<resulting behavior>
<resulting dimensions>
<resulting directory>
<resulting operation>
<resulting transformation>
<resulting variable>
<resume training, resuming training>
<retain probability>
<returing>
<reverse flag>
<reverse transformation>
<reversible layers>
<reversing dimensions>
<right device>
<right examples>
<right node>
<right number>
<right value>
<rightmost dimension, rightmost dimensions>
<ring order>
<rint>
<robin balancing>
<rollout sampling>
<rotated tensor>
<rotating matrices>
<rotation angle>
<rough guide>
<round function>
<rounded numbers>
<rsqrt>
<rth>
<running Properties>
<running average, running averages>
<running example>
<running graphs>
<running mean>
<running models>
<running predict>
<running sample>
<running standard>
<running tally>
<running time>
<runtime shape>
<s32>
<sample approximation>
<sample batch>
<sample covariance>
<sample dimension, samples dimension>
<sample field>
<sample inputs>
<sample likelihood>
<sample one>
<sample outputs>
<sampled classes>
<sampling decoder>
<sampling distribution>
<sampling function>
<sampling probability>
<sampling randomly>
<sampling seed>
<sampling vectors>
<save model, save models, saved model>
<save multiple>
<save path>
<saveable>
<saved checkpoint>
<saved value>
<savefile>
<saver state>
<saving data>
<saving models>
<scalable models>
<scalar Variable, scalar variable>
<scalar addition>
<scalar angle>
<scalar denominator>
<scalar distributions>
<scalar division>
<scalar element>
<scalar loss>
<scalar min>
<scalar numeric>
<scalar operation, scalar operations>
<scalar outputs>
<scalar pad>
<scalar product>
<scalar scalar>
<scalar shape>
<scalar strings>
<scalar threshold>
<scalar type>
<scalar weight>
<scale argument, scale arguments>
<scale example>
<scale gradients>
<scale matrices, scale matrix>
<scale posterior>
<scale scale>
<scale shear>
<scale variable, scale variables>
<scatter function>
<scatter occurs>
<scatter operation>
<scatter phase>
<scatter plot>
<score calculation, score calculator>
<score examples>
<score values>
<scoring sequence>
<search state>
<second array>
<second aspect>
<second axis>
<second batch>
<second copy>
<second dimension, second dimensions>
<second distribution>
<second loop>
<second output>
<second tensors>
<second trick>
<second tuple>
<second vertex>
<seed entropy>
<segfaults>
<segmentation task, segmentation tasks>
<select function>
<select tensors, selected tensor>
<selected column>
<selected device>
<selected directory>
<self Tensor, self tensor>
<semantic information>
<semantic name>
<semantic relation>
<semantic segmentation>
<semantic similarity>
<sending state>
<sensitivity value>
<sent tensor>
<sentence labels>
<sentence length, sentence lengths>
<sentence tensor>
<sepal length>
<separable filters>
<separate GPU>
<sequence Layout>
<sequence alignment>
<sequence architectures>
<sequence batch>
<sequence columns>
<sequence condition>
<sequence dimension>
<sequence examples>
<sequence expansion>
<sequence inputs>
<sequence label, sequence labels>
<sequence one>
<sequence order>
<sequence step, sequence steps>
<sequence tasks>
<sequence tensor>
<sequence time>
<sequences left>
<sequential data>
<sequential features>
<sequential inference>
<sequential input>
<sequential manner>
<serialized tensor>
<serializers>
<series length>
<serving graph>
<sessionID>
<sgv1>
<sgv>
<shallow structure>
<shallow tree>
<shape Shape, shape shape>
<shape Tensor, shape tensor, shape tensors, shaped Tensor>
<shape arguments>
<shape changed>
<shape components>
<shape constraints>
<shape data>
<shape depends>
<shape dimension, shape dimensions>
<shape equal>
<shape format>
<shape information>
<shape invariant, shape invariants>
<shape one>
<shape samples>
<shape stride>
<shape value>
<shift Tensor>
<shift terms>
<shift values>
<shortcut function>
<shrinkage function>
<shrinkage parameter>
<shrinking variance>
<shuffling tensors>
<side inputs>
<side signal>
<sigmoid function>
<sigmoid transform>
<sign decay>
<sign operation>
<signal dimension>
<signal sequence>
<signature function>
<significant number>
<significant value>
<similar operation>
<similarity function>
<simple training>
<simplest network>
<simulated inputs>
<simulated quantization>
<single GPU, single gpu>
<single Gradient, single gradient>
<single Multinomial>
<single OS>
<single Tensor, single tensor>
<single allocation>
<single array>
<single batch>
<single bin, single binary>
<single box>
<single branch>
<single checkpoint>
<single column>
<single computer>
<single conditions>
<single cycle>
<single device>
<single dimension>
<single distribution>
<single draw>
<single equation>
<single feature, single features>
<single float>
<single flow>
<single forward>
<single functions>
<single graph>
<single histogram>
<single kernel>
<single labels>
<single length>
<single library>
<single machine>
<single matrix>
<single minibatch>
<single model>
<single number>
<single person>
<single prediction>
<single run>
<single sample>
<single scalar>
<single series>
<single spatial>
<single tower>
<single transform, single transformation>
<single transition>
<single tutorials>
<single unsafe>
<single vector>
<single version>
<single way>
<single words>
<singleton dimensions>
<singleton list>
<singular dimension>
<singular vectors>
<slack channel>
<slave node, slave nodes>
<slave status>
<slice dimensions>
<slice value>
<sliding blocks>
<sliding local>
<sliding window>
<slight bias>
<slogdet>
<small batch>
<small data>
<small deviation>
<small distributed>
<small effect>
<small layer>
<small multiple>
<small node>
<small operations>
<small sample>
<smallest power>
<snapshot tensor>
<snapshot value>
<sole metric>
<solution matrix>
<sorted vector>
<space model, space models>
<space operation>
<space vector, spaced vector>
<spadd>
<sparse arrays>
<sparse behavior>
<sparse column, sparse columns>
<sparse control>
<sparse copy>
<sparse data>
<sparse format>
<sparse indices>
<sparse input, sparse inputs>
<sparse labels>
<sparse matrix>
<sparse multiplication>
<sparse one, sparse ones>
<sparse representation>
<sparse segments>
<sparse type>
<sparse updates>
<sparse vector, sparse vectors>
<spatial data>
<spatial dimension, spatial dimensions>
<spatial dropout>
<spatial inputs>
<spatial problem>
<spatial shape>
<spatial transformation>
<spawn function>
<specificity value>
<spectral norm, spectral normalization>
<spline interpolation>
<split dimension>
<split images>
<split input>
<split line>
<splitting value>
<square crop>
<square identity>
<square image>
<square mean>
<square patch>
<square tensor, square tensors>
<square weights>
<square window>
<squared connectivity>
<squared distance>
<squared sum>
<squared values>
<squares problem, squares problems>
<squares solution>
<squeeze operation>
<srcdir>
<sspaddmm>
<sspmm>
<stable gradient>
<stable number>
<stable operator>
<stable order>
<stable results>
<stable state>
<stabler function>
<stacked Tensor>
<stacked form>
<stacked layers>
<staging inputs>
<standalone model>
<standard analysis>
<standard convolutions>
<standard decoder>
<standard element>
<standard gradient>
<standard graph>
<standard implicit>
<standard layers>
<standard models>
<standard parameter>
<standard process>
<standard random>
<standard score>
<standard services>
<standard task>
<standardized image>
<start state>
<start training>
<started process>
<starting nodes>
<starting threshold>
<state Tensor, state tensors>
<state map>
<state matrix>
<state memory>
<stated value>
<static batch>
<static number>
<static parameter>
<static value>
<statistic type>
<statistic value>
<statistical distributions>
<statistical models>
<statistical strength>
<stddev, stdev>
<steepest ascent>
<step Step, step step>
<step Tensor>
<step count>
<step counter>
<step mean>
<step number>
<step time>
<step variable>
<steps condition>
<stochastic approximation>
<stop flag>
<stop sign>
<stopping gradient>
<stopping hook>
<stopping signal>
<storage level>
<strange part>
<stratified batch>
<streaming value>
<stride array>
<stride integer>
<stride value, strides value>
<structural similarity>
<structure prediction>
<structured loss>
<structured noise>
<style loss>
<sub columns>
<sub tensors>
<subfolder, subfolders>
<subgraph, subgraphs>
<submatrices>
<subnode>
<subprojects>
<subsamplng value>
<subsequent layers>
<subsequent loss>
<subtraction operation>
<success probability>
<success rate>
<sufficient statistics>
<sum matrix>
<sum operation, sum operator>
<summary operation>
<summary sequence>
<summary type>
<summary value, summary values>
<summation index>
<supported device>
<supported distribution>
<surrogate functions>
<surrouding state>
<surrounding scope>
<survival function>
<svg1>
<symbolic function>
<symbolic shape>
<symmetric cropping>
<symmetric matrix>
<symmetric padding>
<symmetric window>
<sync training>
<synchronous mode>
<synchronous training>
<synthetic data>
<systemd>
<tad element>
<tag indices>
<tag sequence, tag sequences>
<tail decay>
<taking arguments>
<taking images>
<taking values>
<tan function>
<tapering window>
<tarball>
<task index, task indices>
<task loss>
<task number>
<task type>
<tax rate>
<tcas>
<teacher ratio>
<technical data>
<teh shape>
<temperature tensor>
<temperature variable>
<templated>
<temporal data>
<temporal dependency>
<temporal difference>
<temporal dimension>
<temporal order>
<temporal slice>
<temporal step>
<tensor constructor>
<tensor count>
<tensor details>
<tensor dimension, tensor dimensionality, tensor dimensions>
<tensor element, tensor elements>
<tensor graph>
<tensor label>
<tensor left>
<tensor length>
<tensor list>
<tensor pool>
<tensor proto>
<tensor range>
<tensor rank>
<tensor reduction>
<tensor state>
<tensor variable>
<tensors condition>
<tensors layer>
<terminal state>
<termination condition>
<termination tolerance>
<text corpora>
<text models>
<textual order>
<tfb>
<tfe>
<tflite function>
<tfprof>
<tfrecord format>
<tfrecord>
<th value>
<theoretical number>
<theta vector>
<third distribution>
<third image>
<third layer>
<threshold array, thresholds array>
<threshold decay>
<threshold step, threshold steps>
<thresholding>
<time axis>
<time difference>
<time dimension, time dimensions>
<time distributed>
<time domain>
<time equal>
<time frame>
<time gap>
<time grid>
<time improvement>
<time inference>
<time ordering>
<time reversal>
<time sequence, time sequences>
<time transformations>
<time update, tu>
<time window>
<timeseries>
<timestep>
<timing data>
<timly>
<tokenizers>
<top elements>
<top path>
<top prediction>
<top queue>
<topN>
<topic name>
<topological order, topological ordering>
<topological sort>
<total binary>
<total examples>
<total lengths>
<total lines>
<total memory>
<total power>
<total precision>
<total records>
<total shift>
<total system>
<total variation>
<total weight>
<tower call>
<tower context>
<tower device>
<tower function>
<tower model>
<trace operation>
<traced function>
<tracking list>
<tracking shapes>
<trailing dimension, trailing dimensions>
<train batches>
<train call>
<train data>
<train function>
<train loop>
<train phase>
<train steps>
<trained checkpoints>
<trained estimator>
<trained forever>
<trained parameters>
<trained values>
<trained variable, trained variables>
<trained weights>
<trained word>
<trainer function>
<trainer program>
<training Step, training step, training steps>
<training accuracy>
<training algorithm>
<training argument>
<training batch, training batches>
<training begins>
<training binaries, training binary>
<training checkpoint, training checkpoints>
<training cluster>
<training code>
<training compared>
<training condition>
<training convergence>
<training corpus>
<training cycles>
<training details>
<training easier>
<training example, training examples>
<training feature, training features>
<training flag>
<training generally>
<training graph>
<training hook, training hooks>
<training images>
<training input, training inputs>
<training job>
<training labels>
<training libraries>
<training logs>
<training looks>
<training master>
<training methods>
<training module>
<training network, training networks>
<training objective>
<training occurs>
<training operation, training ops>
<training pairs>
<training percentage>
<training performance>
<training pipeline, training pipelines>
<training procedure>
<training proceeds>
<training program, training programs>
<training progress, training progresses>
<training progression>
<training records>
<training result, training results>
<training round>
<training run>
<training script>
<training services>
<training session, training sessions>
<training specification>
<training speed>
<training started, training starts>
<training stops>
<training take>
<training time, training times>
<training updates>
<training version>
<training worker, training workers>
<transfer model, transfer models>
<transform data, transformed data>
<transform matrix>
<transform process>
<transform tensor, transformed tensor>
<transformation data>
<transformation parameters>
<transition matrix>
<transpose operation>
<transposed Tensor>
<transposed matrix>
<treebank>
<triangular band>
<triangular elements>
<triangular factorization>
<triangular part>
<triggered step>
<trim operation>
<trim values>
<triple elements>
<triplet loss>
<triu>
<trivial examples>
<trop>
<trtrs>
<true best>
<true body>
<true classes>
<true elements>
<true positions>
<true rank>
<true shuffling>
<truncated normal>
<truth data>
<truth labels>
<truth sequences>
<try training>
<ts1>
<tune learning>
<tuning procedure>
<tuple elements>
<tuple length>
<tuple shape>
<tuple structure>
<tuple value>
<tuple1>
<type Tensor, type tensor>
<type dimension>
<type distribution>
<uint16>
<uint32>
<unconstrained problem>
<unconstrained tensor>
<unconstrainted shape>
<undefined value>
<underlie tensors>
<underlying distribution>
<underlying tensor>
<understanding images>
<undirected graph>
<unevaluated Tensor>
<unfrozen layers>
<uniform images>
<unigram>
<uninformed state>
<unique graph>
<unique identity>
<unique step>
<unique values>
<unit Normal>
<unit Tensor>
<unit cube>
<unit interval>
<unit length>
<unit norm>
<unit simplex>
<unit variance>
<unitary matrix>
<unitary operator>
<units parameter>
<univariate distributions>
<unknown data>
<unknown dimensions>
<unknown mean>
<unknown rank>
<unknown shape>
<unknown tensor>
<unknown word>
<unpartitioned shape>
<unprojected>
<unraveling indices>
<unseen data>
<unseen sentence>
<unstructured noise>
<untar>
<untrained model>
<untyped>
<unzipped data>
<unzipped distribution>
<update equations>
<update gradient, updated gradient>
<update history>
<update mean>
<update messages>
<update parameters>
<update state, updates state>
<update step>
<update storage>
<update types>
<update vector, updated vectors>
<updated list>
<updated values>
<updaters>
<updates array>
<uplo>
<upper argument>
<upper incomplete>
<upper length>
<upper part>
<upper range>
<upper triangular>
<upscaling>
<usually channel>
<usually images>
<valid approach>
<valid checkpoint>
<valid device>
<valid dimension>
<valid message>
<valid one>
<valid part>
<valid sample>
<validate values>
<validation batch>
<validation data>
<validation images>
<validation input>
<validation loss, vl>
<validation performance>
<validation set, validation sets>
<validation score>
<validation split>
<validation step>
<value clipping>
<value data>
<value feature>
<value matrix, valued matrix>
<value tensors, values tensor>
<valued distribution>
<values Functions>
<values corrupted>
<values inputs>
<values left>
<vanishing gradients>
<variable batch>
<variable call>
<variable creator>
<variable depends>
<variable indices>
<variable map>
<variable mode>
<variable regularization>
<variable scope>
<variable shape>
<variable slice>
<variable update, variable updates>
<variable value>
<variable window>
<variance One>
<variance Tensor>
<variance estimate>
<variant type>
<variational autoencoder>
<various dimensions>
<various factors>
<various inputs>
<various values>
<varying order>
<varying parameter, varying parameters>
<varying probability>
<varying time>
<vast number>
<vector Tensor, vector tensor, vector tensors>
<vector counts>
<vector dimension>
<vector element, vector elements>
<vector length>
<vector multiplication>
<vector norm>
<vector one>
<vector operation, vector operations>
<vector pair>
<vector product>
<vector regardless>
<vector relative>
<vector shape>
<vector space, vector spaces>
<verification steps>
<vertex name>
<vertex sequence>
<vgg13>
<via list>
<view array>
<view layer>
<viewing distance>
<vin>
<visible distribution>
<visual example>
<visualization page>
<vocabulary list>
<vocabulary values>
<volumetric inputs>
<volumetric sampling>
<vraiment>
<w1>
<w2v>
<walk sequence>
<wargs>
<warning mean>
<wave data>
<wave length>
<wave samples>
<wavenet function>
<weight argument, weights argument>
<weight associated>
<weight column>
<weight constraint, weight constraints>
<weight distributions>
<weight files>
<weight initialisations>
<weight lookup>
<weight noise>
<weight ordering>
<weight quantization>
<weight regularization>
<weight rows>
<weight shapes>
<weight tensor, weight tensors, weights Tensors, weights tensor>
<weight values>
<weight variable>
<weight vector, weights vector>
<weighted average>
<weighted correlation>
<weighted graph>
<weighted infinity>
<weighted matrix>
<weighted random>
<weighted tensor>
<weighting function>
<weighting moving>
<weights array>
<weights connecting>
<weights data>
<weights list>
<weights trained>
<weird error>
<white list>
<white noise>
<window function>
<window length>
<window stride>
<word count>
<word embedding>
<word existence>
<word frequencies, word frequency>
<word index, word indices>
<word nodes>
<word shape>
<word similarity>
<word tensor>
<word vector, word vectors>
<worker device>
<worker job>
<worker machine>
<worker task>
<world data>
<wrong rank>
<wrong shape>
<x1>
<zero determinant>
<zero diagonal>
<zero dropout>
<zero entries>
<zero indices>
<zero masking>
<zero matrices, zero matrix>
<zero mean>
<zero one>
<zero tensor, zero tensors>
<zero weight>
<zeros left>
<zeros variable>
<zeta function>
<zoo model>