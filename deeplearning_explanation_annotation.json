[
    {
        "explanation": [
            "By default, both NCCL and Gloo backends will try to find the network interface to use for communication.", 
            "(Note that Gloo currently runs slower than NCCL for GPUs.).", 
            "Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don't change this setting.", 
            "gloo is good, but it cannot support gather and scatter in GPU, that 's why I chose cuda-aware MPI."
        ], 
        "concept": "gloo", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "As user you just have to provide your model Keras models into DL4J.", 
            "Keras models are made by connecting configurable building blocks together, with few restrictions."
        ], 
        "concept": "Keras model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A sparse gradient is represented by its indices, values and possibly empty or None shape."
        ], 
        "concept": "sparse gradient", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The learning phase flag is a bool tensor (0 = test, 1 = train) to be passed as input to any Keras function that uses a different behavior at train time and test time."
        ], 
        "concept": "learning phase", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The training script automatically separates the data set into these three categories, and the logging line above shows the accuracy of model when run on the validation set."
        ], 
        "concept": "training script", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "On GPU, if an out of bound index is found, the index is ignored."
        ], 
        "concept": "bound index", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Example: Max value of index is 150 and chars count is 3."
        ], 
        "concept": "max value", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "You can also specify a partitioner object to partition the primal weights during training (div partitioning strategy will be used)."
        ], 
        "concept": "partitioning strategy", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The training workflow usually proceeds as follows: Prepare training code with a few components: a."
        ], 
        "concept": "training code", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Notice, the how the inputs we set in the input section (nz, ngf, and nc) influence the generator architecture in code."
        ], 
        "concept": "input section", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Since only bilinear interpolation is currently supported, the last dimension of the warp tensor must be 2."
        ], 
        "concept": "Bilinear interpolation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other')."
        ], 
        "concept": "space model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Get LSTM forget gate bias initialization from Keras layer configuration.", 
            "Then LSTM layers are applied and the sum of the outputs of all LSTM steps is fed into a softmax layer to make a classification decision among the classes of drawings that we know.", 
            "Because of these long and short term dependencies, a LSTM is fitting for this task too.", 
            "Get whether LSTM layer should be unrolled (for truncated BPTT).", 
            "In addition, LSTM networks avoid vanishing and exploding gradients and are able to effectively capture long term dependencies due to its cell state, a feature not present in typical recurrent networks.", 
            "We suspect that a LSTM will be effective for this task, because of the temporal dependencies in the data.", 
            "We can use the hidden state to predict words in a language model, LSTM's in Pytorch\u00b6.", 
            "Although this name sounds scary, all the model is is a CRF but where an LSTM provides the features.", 
            "Does not support CuDNN (thus for GPUs, LSTM should be used in preference).", 
            "A LSTM is well suited for this type of problem due to the sequential nature of the data.", 
            "Note that other architectures (LSTM, etc) are usually more effective, especially for longer time series."
        ], 
        "concept": "lstm", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "PLEASE NOTE: CUDA constant memory is limited to 48KB per device."
        ], 
        "concept": "constant memory", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "True positives: correctly rejected.", 
            "Get the true positives count for the specified output."
        ], 
        "concept": "True positive", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history."
        ], 
        "concept": "Tensor operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The contracted dimensions of lhs and rhs must be of the same size."
        ], 
        "concept": "contracted dimensions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses."
        ], 
        "concept": "sigmoid transform", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "And the most popular model used so far is Google News model."
        ], 
        "concept": "popular model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The computed loss is saved as a parameter of the module."
        ], 
        "concept": "computed loss", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The sum matrix represents that row vector falling down the matrix from top to bottom, adding itself at each level."
        ], 
        "concept": "row vector", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Timestamp of when this tensor value was dumped.", 
            "Output slot index from which the tensor value was dumped."
        ], 
        "concept": "tensor values", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The problem I am facing right now is an exploding loss problem.", 
            "However, the exploding loss problem still cannot be alleviated."
        ], 
        "concept": "loss problem", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that Spark submit is a script that comes with a Spark distribution that users submit their job (in the form of a JAR file) to, in order to begin execution of their Spark job."
        ], 
        "concept": "Spark distribution", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Each example has 7 (pool2 height) * 7 (pool2 width) * 64 (pool2 channels) features, so we want the features dimension to have a value of 7 * 7 * 64 (3136 in total)."
        ], 
        "concept": "features dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Therefore, the scatter function should not be overly sensitive to reassociation."
        ], 
        "concept": "scatter function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension."
        ], 
        "concept": "temporal dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Update types are for instantiating various kinds of update types."
        ], 
        "concept": "update types", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The Exponential distribution is parameterized by an event rate parameter.", 
            "Construct Vector Exponential distribution supported on a subset of R^k."
        ], 
        "concept": "Exponential distributions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "More generally, the number of tensors is given by the product of the remaining dimensions, and the shape of the tensors is given by the size of the specified dimensions in the original shape."
        ], 
        "concept": "original shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Deterministic mode can have a performance impact, depending on your model."
        ], 
        "concept": "Deterministic mode", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Also they mention why a linear network won't be able to learn the representation."
        ], 
        "concept": "linear network", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The hidden state vector is then passed to the next time step, while the output vector is recorded."
        ], 
        "concept": "output vector", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "L2 loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
        ], 
        "concept": "L2 Loss", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Negative parameters will be replaced with 0."
        ], 
        "concept": "Negative parameters", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If None, the default metric functions are used; if {}, no metrics are used."
        ], 
        "concept": "metric functions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "L1 loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
        ], 
        "concept": "L1 loss", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Applicability: nd4j-cuda-xx, when multiple backends are on classpath Usage: A fallback for determining the local IP the parameter server, if other approaches fail to determine the Description: If set, only a single GPU will be used by ND4J, even if multiple GPUs are available in the system."
        ], 
        "concept": "single gpu", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Exploding gradients are problematic as they can 'mess up' the parameters of your network In the case of recurrent neural networks, adding some gradient normalization or gradient clipping may help."
        ], 
        "concept": "gradient norm", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This would make the learning behavior erratic, slow down the learning, and may not even lead to a usable result."
        ], 
        "concept": "learning behavior", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Specifies, if UNK word should be used instead of words that are absent in vocab.", 
            "This method allows you to specify, if UNK word should be used internally."
        ], 
        "concept": "unk", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If it does, move all files to one or the other as some Android devices will have problems with both present."
        ], 
        "concept": "Android device", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The model parameters are updated with the gradients averaged across all model replicas.", 
            "The model parameters are learned through the model training process described later."
        ], 
        "concept": "model parameters", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "When two compatible arrays are encountered, the result shape has the maximum among the two inputs at every dimension index."
        ], 
        "concept": "resulting shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "We're using netmasks for cases when Spark cluster is run on top of hadoop, or any other environment which doesn't assume Spark IP addresses announced.", 
            "However, if the Spark cluster is configured such that one or more of the workers cannot access the internet (or specifically, the NTP server), all retries can fail."
        ], 
        "concept": "Spark Cluster", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A similar operation is defined for columns."
        ], 
        "concept": "similar operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Global pooling layer can also handle mask arrays when dealing with variable length inputs.", 
            "Pooling layer #2 takes conv2 as input, producing pool2 as output."
        ], 
        "concept": "Pooling Layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor.", 
            "self tensor and the given tensor must be broadcastable.", 
            "If source is a tensor, self tensor will share the same storage and have the same size and strides as source.", 
            "Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]."
        ], 
        "concept": "self tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A conversion such as T=s32 to U=f32 will perform a normalizing int-to-float Note: The precise float-to-int and visa-versa conversions are currently unspecified, but may become additional arguments to the convert operation in the future."
        ], 
        "concept": "F32", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If less than or equal to 0, the entire matrix will be loaded into memory."
        ], 
        "concept": "entire matrix", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If mat1 is a \\((n \\times m)\\) tensor, mat2 is a Note."
        ], 
        "concept": "mat2", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic.", 
            "This inversion CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation).", 
            "Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model.", 
            "CBOW doesn't involve any pretraining.", 
            "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning."
        ], 
        "concept": "CBOW", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Get the true negatives count for the specified output."
        ], 
        "concept": "True negatives", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "By default, both NCCL and Gloo backends will try to find the network interface to use for communication.", 
            "(Note that Gloo currently runs slower than NCCL for GPUs.).", 
            "Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don't change this setting.", 
            "gloo is good, but it cannot support gather and scatter in GPU, that 's why I chose cuda-aware MPI."
        ], 
        "concept": "gloo", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A step counter might fall into this category."
        ], 
        "concept": "step counter", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Gitter is where you can request help and give feedback, but please do use this guide before asking questions we've answered below."
        ], 
        "concept": "Gitter", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The first Dense layer has 128 nodes (or neurons).", 
            "Masks should not be applied in all cases, depending on the network configuration - for example input Dense -> RNN The first dense layer should be masked (using the input mask) whereas the second shouldn't be, as it has valid data coming from the RNN layer below.", 
            "This allows different layers to handle masks differently - for example, bidirectional RNNs and normal RNNs operate differently with masks (the former sets activations to 0 outside of the data present region (and keeps the mask active for future layers like dense layers), whereas normal RNNs don't zero out the activations/errors )instead relying on backpropagated error arrays to handle the variable length case.", 
            "The final dense layer in a CNN contains a single node for each target class in the model (all the possible classes the model may predict), with a softmax activation function to generate a value between 0\u20131 for each node (the sum of all these softmax values is equal to 1)."
        ], 
        "concept": "Dense layers", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that the Laplace distribution can be thought of two exponential distributions spliced together \"back-to-back.\"."
        ], 
        "concept": "laplace distribution", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The model zoo also includes pretrained weights for different datasets that are downloaded automatically and checked for integrity using a checksum mechanism.", 
            "The model zoo comes with well-known image recognition configurations in the deep learning community."
        ], 
        "concept": "model zoo", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "One thing to note, even when the GPU kernel version of pad is used, it still needs its \"paddings\" input in CPU memory."
        ], 
        "concept": "GPU kernel", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "a triplet loss architecture is that a center loss layer stores its own parameters.", 
            "Center loss is similar to triplet loss except that it enforces intraclass consistency and doesn't require feed forward of multiple examples."
        ], 
        "concept": "triplet loss", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Created matrix can be lower- or upper-triangular."
        ], 
        "concept": "Created matrix", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "shape: Shape tuple, expected shape of the input (may include None for unchecked axes)."
        ], 
        "concept": "Shape tuple", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Models built and ready to go!"
        ], 
        "concept": "Models built", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If None, model checkpoints and summaries will not be written."
        ], 
        "concept": "model checkpoints", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Output array: Two input and one output arrays must have the same shape."
        ], 
        "concept": "outputs array", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value."
        ], 
        "concept": "backwards function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Loosely speaking, matrix multiplication is equal to the action of a Fourier multiplier: A u = IDFT3[ H DFT3[u] ]."
        ], 
        "concept": "matrix multiplication", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros."
        ], 
        "concept": "minlength", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If True, allow the final batch to be smaller if there are insufficient items left in the queue."
        ], 
        "concept": "Final batch", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Its first shape value is N, the minibatch size."
        ], 
        "concept": "shape value", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "As user you just have to provide your model Keras models into DL4J.", 
            "Keras models are made by connecting configurable building blocks together, with few restrictions."
        ], 
        "concept": "Keras model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A sparse gradient is represented by its indices, values and possibly empty or None shape."
        ], 
        "concept": "sparse gradient", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The learning phase flag is a bool tensor (0 = test, 1 = train) to be passed as input to any Keras function that uses a different behavior at train time and test time."
        ], 
        "concept": "learning phase", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The model definition and a pre-trained model can be found here.", 
            "The model definition is in the pytorch/examples repository we cloned previously, and with a few lines of python we can export it to ONNX."
        ], 
        "concept": "Model definition", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "JIT compilation can be turned on at the session level or manually for select operations.", 
            "JIT compilation for CPU operations must be done via the manual method documented below.", 
            "JIT compilation can also be turned on manually for one or more operators.", 
            "Once JIT has completed, code is likely to execute faster for all subsequent operations.", 
            "Currently JIT at the session level, which is what is used for the tutorial, only supports GPU.", 
            "Note: The name JIT for these components is a bit of a misnomer and comes from historical reasons."
        ], 
        "concept": "jit", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Since only bilinear interpolation is currently supported, the last dimension of the warp tensor must be 2."
        ], 
        "concept": "Bilinear interpolation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The DQN has several actions like translation and scaling."
        ], 
        "concept": "dqn", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other')."
        ], 
        "concept": "space model", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note: The SVHN dataset assigns the label 10 to the digit 0."
        ], 
        "concept": "SVHN", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A recurrent neural network is a network that maintains some kind of state.", 
            "Recurrent neural networks (RNN's) are used when the input is sequential in nature.", 
            "A Recurrent Neural Network, or RNN, is a network that operates on a sequence and uses its own output as input for subsequent steps.", 
            "RNN layers in DL4J can be combined with other layer types.", 
            "Typically RNN's are much more effective than regular feed forward neural networks for sequential data because they can keep track of dependencies in the data over multiple time steps.", 
            "Recurrent Neural Networks are useful for processing time series data or other sequentially fed data like video.", 
            "This allows different layers to handle masks differently - for example, bidirectional RNNs and normal RNNs operate differently with masks (the former sets activations to 0 outside of the data present region (and keeps the mask active for future layers like dense layers), whereas normal RNNs don't zero out the activations/errors )instead relying on backpropagated error arrays to handle the variable length case.", 
            "An RNN learns a string of characters.", 
            "RNN's can also be applied to situations where the input is sequential but the output isn't.", 
            "One RNN acts as an encoder, which encodes a variable length input sequence to a fixed-length context vector."
        ], 
        "concept": "recurrent neural network", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The image is then converted back to original image mode."
        ], 
        "concept": "image mode", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The load operation The session in which to restore the graph definition and variables."
        ], 
        "concept": "load operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "No hidden layer should be less than a quarter of the input layer's nodes.", 
            "Hidden layers can learn features from the input layer and it can send those features to be analyzed by our output layer to get the corresponding outputs.", 
            "In theory, this context vector (the final hidden layer of the RNN) will contain semantic information about the query sentence that is input to the bot."
        ], 
        "concept": "hidden layers", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "DDPG is a bit complicated to understand regarding the gradient update of the actor.", 
            "DDPG is a case of Deep Actor-Critic algorithm, so you have two gradients: one for the actor (the parameters leading to the action (mu)) and one for the critic (that estimates the value of a state-action (Q) \u2013 this is our case \u2013 , or sometimes the value of a state (V) ).", 
            "But DDPG is using sample batch."
        ], 
        "concept": "DDPG", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "When training started, the model did not know how to spell an English word, or that words were even a unit of text."
        ], 
        "concept": "training started", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If this is True then all subsequent layers in the model need to support masking or an exception will be raised."
        ], 
        "concept": "subsequent layers", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The contracted dimensions of lhs and rhs must be of the same size."
        ], 
        "concept": "contracted dimensions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Get LSTM forget gate bias initialization from Keras layer configuration.", 
            "Then LSTM layers are applied and the sum of the outputs of all LSTM steps is fed into a softmax layer to make a classification decision among the classes of drawings that we know.", 
            "Because of these long and short term dependencies, a LSTM is fitting for this task too.", 
            "Get whether LSTM layer should be unrolled (for truncated BPTT).", 
            "In addition, LSTM networks avoid vanishing and exploding gradients and are able to effectively capture long term dependencies due to its cell state, a feature not present in typical recurrent networks.", 
            "We suspect that a LSTM will be effective for this task, because of the temporal dependencies in the data.", 
            "We can use the hidden state to predict words in a language model, LSTM's in Pytorch\u00b6.", 
            "Although this name sounds scary, all the model is is a CRF but where an LSTM provides the features.", 
            "Does not support CuDNN (thus for GPUs, LSTM should be used in preference).", 
            "A LSTM is well suited for this type of problem due to the sequential nature of the data.", 
            "Note that other architectures (LSTM, etc) are usually more effective, especially for longer time series."
        ], 
        "concept": "lstm", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Each transform is length 8 (single transform) or shape (N, 8) (batched transforms)."
        ], 
        "concept": "single transformation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Mean Squared Error loss function where each the output is (optionally) weighted/scaled by a flags scalar value.", 
            "Mean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems)."
        ], 
        "concept": "Mean Squared Error", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Generates n samples or n batches of samples if the distribution parameters are batched.", 
            "When True distribution parameters are checked for validity despite possibly degrading runtime performance.", 
            "Distribution parameters are automatically broadcast in all functions; see examples for details."
        ], 
        "concept": "distribution parameter", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A tensor: The row factor matrix is initialized to this tensor, A numpy constant, \"random\": The rows are initialized using a normal distribution."
        ], 
        "concept": "factor matrix", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "TODO: handle axes arguments that alter merge behavior (requires changes to DL4J?).", 
            "\"\"\" # TODO allow (loc,scale) parameterization to allow independent constraints."
        ], 
        "concept": "TODO", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Layer name assigns layer string name."
        ], 
        "concept": "Layer name", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Compute model predictions given input data."
        ], 
        "concept": "Model prediction", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "State dropout is performed on the outgoing states of the cell."
        ], 
        "concept": "State dropout", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Therefore, the model now can learn four individual weights rather than just one; four weights creates a richer model than one weight."
        ], 
        "concept": "one weight", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Label template will be used for sentence labels generation."
        ], 
        "concept": "sentence labels", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If mat is a \\((n \\times m)\\) tensor, vec is a 1-D tensor of size m, then tensor must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n.", 
            "If mat is a \\((n \\times m)\\) tensor, vec is a 1-D tensor of Note.", 
            "A Vectorizer at its essence takes an input source and converts it to a matrix for neural network consumption."
        ], 
        "concept": "Vectorizer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The orange line represents training.", 
            "During training, summaries (the orange line) are recorded periodically as batches are processed, which is why it becomes a graph spanning x-axis range."
        ], 
        "concept": "orange line", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "True positives: correctly rejected.", 
            "Get the true positives count for the specified output."
        ], 
        "concept": "True positive", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "PLEASE NOTE: CUDA constant memory is limited to 48KB per device."
        ], 
        "concept": "constant memory", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The objective function is the function that your network is being trained to minimize (in which case it is often called a loss function or cost function).", 
            "You'd like to record how the learning rate varies over time, and how the objective function is changing."
        ], 
        "concept": "objective functions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension."
        ], 
        "concept": "temporal dimension", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This handler transform a tensor into itself if the source and destination graph are the same."
        ], 
        "concept": "destination graph", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Visual Studio doesn't support parallel custom task currently."
        ], 
        "concept": "Visual Studio", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A column reduction defines how a single column should be reduced."
        ], 
        "concept": "column reduction", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Although the MNIST data isn't time series in nature, we can interpret it as such since there are 784 inputs.", 
            "MNIST data set iterator - 60000 training digits, 10000 test digits, 10 classes."
        ], 
        "concept": "MNIST data", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The Exponential distribution is parameterized by an event rate parameter.", 
            "Construct Vector Exponential distribution supported on a subset of R^k."
        ], 
        "concept": "Exponential distributions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Seems like the network learnt something."
        ], 
        "concept": "network learnt", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Timestamp of when this tensor value was dumped.", 
            "Output slot index from which the tensor value was dumped."
        ], 
        "concept": "tensor values", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The problem I am facing right now is an exploding loss problem.", 
            "However, the exploding loss problem still cannot be alleviated."
        ], 
        "concept": "loss problem", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The computed loss is saved as a parameter of the module."
        ], 
        "concept": "computed loss", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This function applies a cosine decay function to a provided initial learning rate."
        ], 
        "concept": "decay function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "VGG11 performed best set at 1ms."
        ], 
        "concept": "vgg11", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Cycle schedule Starts at initial learning rate, then linearly increases learning rate until max learning rate is reached, at that point the learning rate is decreased back to initial learning rate.", 
            "If learning rate shrinks too much, the net's learning is no longer efficient.", 
            "There are infinite configurations of the learning rate and hidden layer size, since the learning rate space is continuous.", 
            "For eg, if a learning rate is specified this learning rate will apply to all unfrozen/trainable layers in the model.", 
            "If the score increases consistently, your learning rate is likely set too high.", 
            "Learning configs (like updaters, learning rate etc) specified with the layer here will be honored.", 
            "The learning rate is one of, if not the most important hyperparameter.", 
            "Also, the default learning rate is not optimal for all of the models, so to achieve maximum accuracy it would be necessary to tune for each model separately.", 
            "This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.", 
            "Specify the preprocessor for the added layers Usage example: specify a learning rate will set specified learning rate on all layers The specified layer and the layers preceding it will be \"frozen\" with parameters staying constant.", 
            "Add layers to the net Usage example: specify a learning rate will set specified learning rate on all layers Note this will also affect the layer that follows the layer specified, unless it is the output layer.", 
            "The issues mentioned above (learning rate, normalization, data shuffling) may contribute to this.", 
            "If not set, the loss, the learning rate, and the global norm of the gradients will be reported.", 
            "Learning rate schedules can be specified either based on the number of iterations, or the number of epochs that have elapsed.", 
            "If the score is flat or decreases very slowly (over a few hundred iterations) (a) your learning rate may be too low, or (b) you might be having difficulties with optimization.", 
            "The Learning rate curve Looks something like this: +-----------------------------------------+ | XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XXX | | XXX | | XXX | | | +-----------------------------------------+."
        ], 
        "concept": "learning rate", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The sum matrix represents that row vector falling down the matrix from top to bottom, adding itself at each level."
        ], 
        "concept": "row vector", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Spatial dropout: can only be applied to 4D (convolutional) activations."
        ], 
        "concept": "spatial dropout", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "And the most popular model used so far is Google News model."
        ], 
        "concept": "popular model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "NOTE: for Android development, adb shell is needed otherwise the following section of tutorial will not run."
        ], 
        "concept": "adb", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "While handwriting recognition has been attempted by different machine learning algorithms over the years, deep learning performs MNIST dataset.", 
            "If you are reading this, hopefully you can appreciate how effective some machine learning models are.", 
            "Many machine learning models are represented by composing layers.", 
            "Machine learning provides many algorithms to classify flowers statistically.", 
            "Machine learning techniques have a set of parameters that have to be chosen before any training can begin.", 
            "Some simple models can be described with a few lines of algebra, but complex machine learning models have a large number of parameters that are difficult to summarize.", 
            "When publishing research models and techniques, most machine learning practitioners share: code to create the model, and.", 
            "The reason you can't simply use your training data for evaluation is because machine learning methods are prone to overfitting (getting good at making predictions about the training set, but not performing well on larger datasets).", 
            "Overfitting is when a machine learning model performs worse on new data than on their training data.", 
            "Typical machine learning, of course, has one hidden layer, and those shallow nets are called Perceptrons.", 
            "Machine learning models, however, need to have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!)."
        ], 
        "concept": "Machine Learning", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The op accepts, for example, input types (float, double, float) and in that case the output type would also be (float, double, float).", 
            "If the output type was qint8 ([-128, 127]), the operation will additionally subtract each value by 128 prior to casting, so that the range of values aligns with the range of qint8."
        ], 
        "concept": "output type", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "More generally, the number of tensors is given by the product of the remaining dimensions, and the shape of the tensors is given by the size of the specified dimensions in the original shape."
        ], 
        "concept": "original shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses."
        ], 
        "concept": "sigmoid transform", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The probability mass function (pmf) and cumulative distribution function (cdf) are."
        ], 
        "concept": "cumulative distribution", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The training workflow usually proceeds as follows: Prepare training code with a few components: a."
        ], 
        "concept": "training code", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Example: Max value of index is 150 and chars count is 3."
        ], 
        "concept": "max value", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Another way to think about this is that the output dimension equals the number of weights of the linear model; the larger this dimension, the larger the \"degrees of freedom\" of the model.", 
            "However, after a certain threshold, higher output dimensions increase the accuracy by very little, while making training take more time."
        ], 
        "concept": "output dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "An optimization result represents the results of an optimization run, including the canditate configuration, the trained model, the score for that model, and index of the model."
        ], 
        "concept": "optimization results", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "GPU operations are called in an asynchronously way, so that they are not blocking your CPU.", 
            "By default, GPU operations are asynchronous."
        ], 
        "concept": "GPU operations", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Examples: x is a batch matrix with compatible shape for matmul if."
        ], 
        "concept": "batch matrix", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The prespecified number of workers (in this case 2) will then train its own model using its data."
        ], 
        "concept": "prespecified number", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "You can also specify a partitioner object to partition the primal weights during training (div partitioning strategy will be used)."
        ], 
        "concept": "partitioning strategy", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "On GPU, if an out of bound index is found, the index is ignored."
        ], 
        "concept": "bound index", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "After I trained this model for a few hours, the average training speed for epoch 10 was slow down to 40s.", 
            "However, I noticed that the training speed gets slow down slowly at each batch and memory usage on GPU also increases.", 
            "Currently, the memory usage would not increase but the training speed still gets slower batch-batch."
        ], 
        "concept": "training speed", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Each example has 7 (pool2 height) * 7 (pool2 width) * 64 (pool2 channels) features, so we want the features dimension to have a value of 7 * 7 * 64 (3136 in total)."
        ], 
        "concept": "features dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad."
        ], 
        "concept": "Korrine", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Therefore, the scatter function should not be overly sensitive to reassociation."
        ], 
        "concept": "scatter function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history."
        ], 
        "concept": "Tensor operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The training script automatically separates the data set into these three categories, and the logging line above shows the accuracy of model when run on the validation set."
        ], 
        "concept": "training script", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Update types are for instantiating various kinds of update types."
        ], 
        "concept": "update types", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Notice, the how the inputs we set in the input section (nz, ngf, and nc) influence the generator architecture in code."
        ], 
        "concept": "input section", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that Spark submit is a script that comes with a Spark distribution that users submit their job (in the form of a JAR file) to, in order to begin execution of their Spark job."
        ], 
        "concept": "Spark distribution", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Deterministic mode can have a performance impact, depending on your model."
        ], 
        "concept": "Deterministic mode", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Because Inception networks are large, we will use the Deeplearning4j model zoo to help build our Zeppelin, make sure you add the deeplearning4j-zoo artifact to the Spark interpreter."
        ], 
        "concept": "Inception networks", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A Mixture is defined by a Categorical (cat, representing the mixture probabilities) and a list of Distribution objects all having matching dtype, batch shape, event shape, and continuity properties (the components)."
        ], 
        "concept": "mixture probabilities", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Trajectory clustering can be a difficult problem to solve when your data isn't quite \"even\"."
        ], 
        "concept": "Trajectory clustering", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "So, although raw data can be numerical or categorical, an ML model represents all features as numbers.", 
            "ML models generally represent categorical values as simple vectors in which a 1 represents the presence of a value and a 0 represents the absence of a value."
        ], 
        "concept": "ML model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "None, control outputs are enabled."
        ], 
        "concept": "control outputs", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The resulting operation takes no inputs."
        ], 
        "concept": "resulting operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "alpha: Confidence interval level desired."
        ], 
        "concept": "interval level", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Cosine similarity Note that you need to initialize a scaling constant equal to the norm2 of the vector."
        ], 
        "concept": "Cosine similarity", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Negative shift values will shift elements in the opposite direction."
        ], 
        "concept": "shift values", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "TFGAN is a lightweight library for training and evaluating GANs."
        ], 
        "concept": "TFGAN", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Internally, SQSS has a queue for the input examples."
        ], 
        "concept": "SQSS", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Also, each tensor in the tensor list needs to reside on a different GPU.", 
            "Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called.", 
            "Append to lists in loops (tensor list ops are automatically created): Nested control flow."
        ], 
        "concept": "tensor list", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Defines, if adaptive gradients should be created during vocabulary mastering.", 
            "This method defines whether adaptive gradients should be used or not.", 
            "This method defines if Adaptive Gradients should be used in calculations.", 
            "This method specifies, if adaptive gradients should be used during model training."
        ], 
        "concept": "adaptive gradients", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "When two compatible arrays are encountered, the result shape has the maximum among the two inputs at every dimension index."
        ], 
        "concept": "resulting shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The model parameters are updated with the gradients averaged across all model replicas.", 
            "The model parameters are learned through the model training process described later."
        ], 
        "concept": "model parameters", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A similar operation is defined for columns."
        ], 
        "concept": "similar operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The total variation is the sum of the absolute differences for neighboring pixel-values in the input images."
        ], 
        "concept": "total variation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "We're using netmasks for cases when Spark cluster is run on top of hadoop, or any other environment which doesn't assume Spark IP addresses announced.", 
            "However, if the Spark cluster is configured such that one or more of the workers cannot access the internet (or specifically, the NTP server), all retries can fail."
        ], 
        "concept": "Spark Cluster", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "weight decay should not be used when learning \\(a\\) for good performance.", 
            "note:: weight decay should not be used when learning :math:`a` for good performance."
        ], 
        "concept": "weight decay", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "VSMs have a long, rich history in NLP, but all methods depend in some way or Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning.", 
            "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other')."
        ], 
        "concept": "VSMs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Our next order of business is to create a vocabulary and load query/response sentence pairs into memory."
        ], 
        "concept": "next order", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Consequently, the parameters (post noise) should be cleared after each training iteration."
        ], 
        "concept": "post noise", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The mixture model is defined by a Categorical distribution (the mixture) and a python list of Distribution objects."
        ], 
        "concept": "mixture model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "During execution any unknown shape dimensions are determined dynamically, see Tensor Shapes for more details."
        ], 
        "concept": "Tensor Shapes", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Suppose our input examples consist of different words from a limited palette of only 81 words."
        ], 
        "concept": "input examples", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "So, you'll have to spend WAY more time building model + cooccurrence statistics will be shifted due to the absense of sentence boundaries."
        ], 
        "concept": "CoOccurrence", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The output tensor has shape.", 
            "Other output tensor can be duplicated as well.", 
            "When all the input tensors are finished, the output tensor is passed along in the graph.", 
            "Note that a tensor can be both an inside tensor and an output tensor if it is consumed by operations both outside and inside of ops.", 
            "The output tensor is 1-D of size steps.", 
            "All this output tensor knows is its data and shape.", 
            "For instance, some output tensor can be omitted.", 
            "Output tensor has one more dimension than input tensor, the first dimension indicates the partition.", 
            "Like the input, the resulting output tensors have a batch dimension.", 
            "The output tensors can also be remapped.", 
            "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1.", 
            "Currently, only 4-D output tensors (batched image-like tensors) are supported.", 
            "keepdim (bool): whether the output tensors have :attr:`dim` retained or not.", 
            "If a directory is passed to --outdir option, the outputs will be saved as npy files named after output tensor keys under the given directory."
        ], 
        "concept": "Output Tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A series of nodes can be ungrouped so that the nodes in the series do not Selection can also be helpful in understanding high-degree nodes."
        ], 
        "concept": "degree node", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Global pooling layer can also handle mask arrays when dealing with variable length inputs.", 
            "Pooling layer #2 takes conv2 as input, producing pool2 as output."
        ], 
        "concept": "Pooling Layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor.", 
            "self tensor and the given tensor must be broadcastable.", 
            "If source is a tensor, self tensor will share the same storage and have the same size and strides as source.", 
            "Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]."
        ], 
        "concept": "self tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A conversion such as T=s32 to U=f32 will perform a normalizing int-to-float Note: The precise float-to-int and visa-versa conversions are currently unspecified, but may become additional arguments to the convert operation in the future."
        ], 
        "concept": "F32", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.", 
            "The KL divergence, found in the variational autoencoder loss, is an example."
        ], 
        "concept": "kl divergence", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The reparameterized sample therefore becomes differentiable."
        ], 
        "concept": "reparameterized sample", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The capacity argument controls the how long the prefetching is allowed to grow the queues."
        ], 
        "concept": "Prefetching", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The poisson distribution is defined over the integers."
        ], 
        "concept": "Poisson distribution", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "SVD decomposiiton of a matrix The decomposition is such that: A = U x S x VT L will be the same dimensions as A.", 
            "Analogously, the SVD on GPU uses the MAGMA routine gesdd as well."
        ], 
        "concept": "svd", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Specifies, if UNK word should be used instead of words that are absent in vocab.", 
            "This method allows you to specify, if UNK word should be used internally."
        ], 
        "concept": "unk", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "My guess is that tensorflow may not cache the intermediate feature maps in the graphdef mode, but pytorch may do.", 
            "Once setup is completed, Tensorflow can interact with S3 in a variety of ways.", 
            "sets module: Tensorflow set operations.", 
            "You can specify a continuous feature like so: Although, as a single real number, a continuous feature can often be input directly into the model, Tensorflow offers useful transformations for this sort of column as well."
        ], 
        "concept": "Tensorflow", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If None, the default metric functions are used; if {}, no metrics are used."
        ], 
        "concept": "metric functions", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Negative parameters will be replaced with 0."
        ], 
        "concept": "Negative parameters", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "L2 loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
        ], 
        "concept": "L2 Loss", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "L1 loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
        ], 
        "concept": "L1 loss", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The resulting variable can be used for example for evaluating functions at all locations on a grid."
        ], 
        "concept": "resulting variable", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Applicability: nd4j-cuda-xx, when multiple backends are on classpath Usage: A fallback for determining the local IP the parameter server, if other approaches fail to determine the Description: If set, only a single GPU will be used by ND4J, even if multiple GPUs are available in the system."
        ], 
        "concept": "single gpu", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If it does, move all files to one or the other as some Android devices will have problems with both present."
        ], 
        "concept": "Android device", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Exploding gradients are problematic as they can 'mess up' the parameters of your network In the case of recurrent neural networks, adding some gradient normalization or gradient clipping may help."
        ], 
        "concept": "gradient norm", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This would make the learning behavior erratic, slow down the learning, and may not even lead to a usable result."
        ], 
        "concept": "learning behavior", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A gradient op represents a jacobian operation."
        ], 
        "concept": "gradient operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "However, the cluster centers may be retrieved by the latest checkpoint saved during training."
        ], 
        "concept": "cluster center", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that leaf parameters are parameters that do not have any nested parameter spaces."
        ], 
        "concept": "nested parameter", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The hidden state vector is then passed to the next time step, while the output vector is recorded."
        ], 
        "concept": "output vector", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that sgv1 is modified in place."
        ], 
        "concept": "sgv1", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Blas buffer util for interopping with the underlying buffers and the given ndarrays."
        ], 
        "concept": "Blas", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "loss: The loss reported."
        ], 
        "concept": "loss reported", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Also they mention why a linear network won't be able to learn the representation."
        ], 
        "concept": "linear network", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Each column is plotted separately; only numerical and categorical columns are plotted.", 
            "Indicator columns and embedding columns never work on features directly, but instead take categorical columns as input."
        ], 
        "concept": "Categorical column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A step counter might fall into this category."
        ], 
        "concept": "step counter", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Its first shape value is N, the minibatch size."
        ], 
        "concept": "shape value", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The forward transformation creates samples, the inverse is useful for computing probabilities.", 
            "If X is a scalar then the forward transformation is: scale * X + shift where * denotes the scalar product."
        ], 
        "concept": "forward transformation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This operation applies Max operation to specific inputs through given indices Expected arguments: input: array to be updated indices: array containing indexes for first dimension of input updates: array containing elements to be interfered with input."
        ], 
        "concept": "Max operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If True, allow the final batch to be smaller if there are insufficient items left in the queue."
        ], 
        "concept": "Final batch", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Momentum and Adagrad use variables to accumulate updates.", 
            "Adagrad keeps a history of gradients being passed in."
        ], 
        "concept": "adagrad", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Loads LFW faces data transform.", 
            "Thus, LFW images are scaled to 28 pixels x 28 pixels."
        ], 
        "concept": "LFW", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If None (default), static shape information for batch sizes is omitted."
        ], 
        "concept": "static Shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The GPU version is still a bit slower than the CPU version, but not by much."
        ], 
        "concept": "GPU version", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros."
        ], 
        "concept": "minlength", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Loosely speaking, matrix multiplication is equal to the action of a Fourier multiplier: A u = IDFT3[ H DFT3[u] ]."
        ], 
        "concept": "matrix multiplication", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Get the true negatives count for the specified output."
        ], 
        "concept": "True negatives", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "RNN layers in DL4J can be combined with other layer types."
        ], 
        "concept": "layer types", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Checkpoint model file must exist."
        ], 
        "concept": "model file", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "param labels Each triple in the list specifies the shape, array order and type of values for the labels arrays."
        ], 
        "concept": "labels arrays", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic.", 
            "This inversion CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation).", 
            "Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model.", 
            "CBOW doesn't involve any pretraining.", 
            "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning."
        ], 
        "concept": "CBOW", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Weight parameter keys given the layer configuration."
        ], 
        "concept": "weight parameters", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "shape: Shape tuple, expected shape of the input (may include None for unchecked axes)."
        ], 
        "concept": "Shape tuple", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The running estimates are Note."
        ], 
        "concept": "running estimates", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The model definition and a pre-trained model can be found here.", 
            "The model definition is in the pytorch/examples repository we cloned previously, and with a few lines of python we can export it to ONNX."
        ], 
        "concept": "Model definition", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Created matrix can be lower- or upper-triangular."
        ], 
        "concept": "Created matrix", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that sgv is modified in place."
        ], 
        "concept": "sgv", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output."
        ], 
        "concept": "ellipsis dimensions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Similarly, evaluation metrics used for regression differ from classification.", 
            "Evaluation metrics are an essential part of training a model."
        ], 
        "concept": "evaluation metrics", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Passthrough = feed forward the input mask (if/when necessary) but don't actually apply it."
        ], 
        "concept": "Passthrough", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Determining if the input pipeline is the bottleneck can be complicated.", 
            "If the input pipeline is shared between training and validation, restoring the checkpoint during validation may override the validation input pipeline.", 
            "To make the image processing pipeline easier to explain, assume that the input pipeline is targeting 8 GPUs with a batch size of 256 (32 per GPU).", 
            "This hook saves the state of the iterators in the Graph so that when training is resumed the input pipeline continues from where it left off.", 
            "Before the model starts running all the stages, the input pipeline stages are warmed up to prime the staging buffers in between with one set of data.", 
            "If the difference in examples per second for the full model and the trivial model is minimal then the input pipeline is likely a bottleneck.", 
            "Example of checkpointing the training pipeline: This hook should be used if the input pipeline state needs to be saved separate from the model checkpoint.", 
            "If GPU utilization is not approaching 80-100%, then the input pipeline may be the bottleneck.", 
            "The input pipeline checkpoint may be large, if there are large shuffle or prefetch buffers for instance, and may bloat the checkpoint size."
        ], 
        "concept": "Input Pipeline", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "a triplet loss architecture is that a center loss layer stores its own parameters.", 
            "Center loss is similar to triplet loss except that it enforces intraclass consistency and doesn't require feed forward of multiple examples."
        ], 
        "concept": "triplet loss", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value."
        ], 
        "concept": "backwards function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This error or loss score will eventually converge to a value close to zero."
        ], 
        "concept": "loss score", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "One thing to note, even when the GPU kernel version of pad is used, it still needs its \"paddings\" input in CPU memory."
        ], 
        "concept": "GPU kernel", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If no global Keras session exists at this point: we will create a new global session."
        ], 
        "concept": "Keras session", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Output array: Two input and one output arrays must have the same shape."
        ], 
        "concept": "outputs array", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The model zoo also includes pretrained weights for different datasets that are downloaded automatically and checked for integrity using a checksum mechanism.", 
            "The model zoo comes with well-known image recognition configurations in the deep learning community."
        ], 
        "concept": "model zoo", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that the Laplace distribution can be thought of two exponential distributions spliced together \"back-to-back.\"."
        ], 
        "concept": "laplace distribution", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "From the bijective, nonzero differentiability of g, the inverse function theorem implies g^{-1} is differentiable in the image of g."
        ], 
        "concept": "inverse function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Gitter is where you can request help and give feedback, but please do use this guide before asking questions we've answered below."
        ], 
        "concept": "Gitter", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The first Dense layer has 128 nodes (or neurons).", 
            "Masks should not be applied in all cases, depending on the network configuration - for example input Dense -> RNN The first dense layer should be masked (using the input mask) whereas the second shouldn't be, as it has valid data coming from the RNN layer below.", 
            "This allows different layers to handle masks differently - for example, bidirectional RNNs and normal RNNs operate differently with masks (the former sets activations to 0 outside of the data present region (and keeps the mask active for future layers like dense layers), whereas normal RNNs don't zero out the activations/errors )instead relying on backpropagated error arrays to handle the variable length case.", 
            "The final dense layer in a CNN contains a single node for each target class in the model (all the possible classes the model may predict), with a softmax activation function to generate a value between 0\u20131 for each node (the sum of all these softmax values is equal to 1)."
        ], 
        "concept": "Dense layers", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If mat1 is a \\((n \\times m)\\) tensor, mat2 is a Note."
        ], 
        "concept": "mat2", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If None, model checkpoints and summaries will not be written."
        ], 
        "concept": "model checkpoints", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If less than or equal to 0, the entire matrix will be loaded into memory."
        ], 
        "concept": "entire matrix", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Models built and ready to go!"
        ], 
        "concept": "Models built", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If i dont use this snippet then the model works in multi-gpu.", 
            "By keeping the validation set separate, you can ensure that the model works with data it's never seen before.", 
            "Let's get rid of these two assumptions, so our model works with any 2d single channel image."
        ], 
        "concept": "model works", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.", 
            "For instance, a beam search in sequence-to-sequence translation is a loop over the (varying) sequence length of inputs."
        ], 
        "concept": "beam search", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "State dropout is performed on the outgoing states of the cell."
        ], 
        "concept": "State dropout", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Marine Automatic Identification System (AIS) is an open system for marine broadcasting of positions.", 
            "AIS data, coordinates can be reported at irregular intervals over time.", 
            "Furthermore, AIS data for 1 year is over 100GB compressed."
        ], 
        "concept": "AIS", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "No hidden layer should be less than a quarter of the input layer's nodes.", 
            "Hidden layers can learn features from the input layer and it can send those features to be analyzed by our output layer to get the corresponding outputs.", 
            "In theory, this context vector (the final hidden layer of the RNN) will contain semantic information about the query sentence that is input to the bot."
        ], 
        "concept": "hidden layers", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This method defines, if ETL time per iteration should be reported together with other data.", 
            "In our dl4j-examples repo, we don't make the ETL asynchronous, because the point of examples is to keep them simple."
        ], 
        "concept": "ETL", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "JIT compilation can be turned on at the session level or manually for select operations.", 
            "JIT compilation for CPU operations must be done via the manual method documented below.", 
            "JIT compilation can also be turned on manually for one or more operators.", 
            "Once JIT has completed, code is likely to execute faster for all subsequent operations.", 
            "Currently JIT at the session level, which is what is used for the tutorial, only supports GPU.", 
            "Note: The name JIT for these components is a bit of a misnomer and comes from historical reasons."
        ], 
        "concept": "jit", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Therefore, the model now can learn four individual weights rather than just one; four weights creates a richer model than one weight."
        ], 
        "concept": "one weight", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Layer name assigns layer string name."
        ], 
        "concept": "Layer name", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This handler transform a tensor into itself if the source and destination graph are the same."
        ], 
        "concept": "destination graph", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Compute model predictions given input data."
        ], 
        "concept": "Model prediction", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This makes moving averages move faster.", 
            "The moving averages are computed using exponential decay."
        ], 
        "concept": "moving average", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "TODO: handle axes arguments that alter merge behavior (requires changes to DL4J?).", 
            "\"\"\" # TODO allow (loc,scale) parameterization to allow independent constraints."
        ], 
        "concept": "TODO", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A bernoulli trial is a mechanism for detecting the probability of a given event occurring k times in n independent trials."
        ], 
        "concept": "Bernoulli trials", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A tensor: The row factor matrix is initialized to this tensor, A numpy constant, \"random\": The rows are initialized using a normal distribution."
        ], 
        "concept": "factor matrix", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The objective function is the function that your network is being trained to minimize (in which case it is often called a loss function or cost function).", 
            "You'd like to record how the learning rate varies over time, and how the objective function is changing."
        ], 
        "concept": "objective functions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Generates n samples or n batches of samples if the distribution parameters are batched.", 
            "When True distribution parameters are checked for validity despite possibly degrading runtime performance.", 
            "Distribution parameters are automatically broadcast in all functions; see examples for details."
        ], 
        "concept": "distribution parameter", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Mean Squared Error loss function where each the output is (optionally) weighted/scaled by a flags scalar value.", 
            "Mean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems)."
        ], 
        "concept": "Mean Squared Error", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Label template will be used for sentence labels generation."
        ], 
        "concept": "sentence labels", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Each transform is length 8 (single transform) or shape (N, 8) (batched transforms)."
        ], 
        "concept": "single transformation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The image is then converted back to original image mode."
        ], 
        "concept": "image mode", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The load operation The session in which to restore the graph definition and variables."
        ], 
        "concept": "load operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A recurrent neural network is a network that maintains some kind of state.", 
            "Recurrent neural networks (RNN's) are used when the input is sequential in nature.", 
            "A Recurrent Neural Network, or RNN, is a network that operates on a sequence and uses its own output as input for subsequent steps.", 
            "RNN layers in DL4J can be combined with other layer types.", 
            "Typically RNN's are much more effective than regular feed forward neural networks for sequential data because they can keep track of dependencies in the data over multiple time steps.", 
            "Recurrent Neural Networks are useful for processing time series data or other sequentially fed data like video.", 
            "This allows different layers to handle masks differently - for example, bidirectional RNNs and normal RNNs operate differently with masks (the former sets activations to 0 outside of the data present region (and keeps the mask active for future layers like dense layers), whereas normal RNNs don't zero out the activations/errors )instead relying on backpropagated error arrays to handle the variable length case.", 
            "An RNN learns a string of characters.", 
            "RNN's can also be applied to situations where the input is sequential but the output isn't.", 
            "One RNN acts as an encoder, which encodes a variable length input sequence to a fixed-length context vector."
        ], 
        "concept": "recurrent neural network", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The orange line represents training.", 
            "During training, summaries (the orange line) are recorded periodically as batches are processed, which is why it becomes a graph spanning x-axis range."
        ], 
        "concept": "orange line", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If this is True then all subsequent layers in the model need to support masking or an exception will be raised."
        ], 
        "concept": "subsequent layers", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "DDPG is a bit complicated to understand regarding the gradient update of the actor.", 
            "DDPG is a case of Deep Actor-Critic algorithm, so you have two gradients: one for the actor (the parameters leading to the action (mu)) and one for the critic (that estimates the value of a state-action (Q) \u2013 this is our case \u2013 , or sometimes the value of a state (V) ).", 
            "But DDPG is using sample batch."
        ], 
        "concept": "DDPG", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "When training started, the model did not know how to spell an English word, or that words were even a unit of text."
        ], 
        "concept": "training started", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If mat is a \\((n \\times m)\\) tensor, vec is a 1-D tensor of size m, then tensor must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n.", 
            "If mat is a \\((n \\times m)\\) tensor, vec is a 1-D tensor of Note.", 
            "A Vectorizer at its essence takes an input source and converts it to a matrix for neural network consumption."
        ], 
        "concept": "Vectorizer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The DQN has several actions like translation and scaling."
        ], 
        "concept": "dqn", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note: The SVHN dataset assigns the label 10 to the digit 0."
        ], 
        "concept": "SVHN", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The input tags and values must have the same shape."
        ], 
        "concept": "input tags", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Printing will switch to scientific notation on a per element basis - when abs value is greater than or equal to 10000 If the number of elements in the array is greater than 1000 (by default) only the first and last three elements in a dimension are included."
        ], 
        "concept": "abs value", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If not provided, the true class distribution is estimated live in a streaming fashion.", 
            "That is, soft classes are not allowed, and the labels vector must provide a single specific index for the true class for each row of logits (each minibatch entry)."
        ], 
        "concept": "True class", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Here's the relevant code: The units parameter defines the number of output neurons in a given layer.", 
            "Therefore, the full set of layers A logit output layer connected to the top hidden layer When defining an output layer, the units parameter specifies the number of outputs."
        ], 
        "concept": "units parameter", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Seems like the network learnt something."
        ], 
        "concept": "network learnt", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The prespecified number of workers (in this case 2) will then train its own model using its data."
        ], 
        "concept": "prespecified number", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "An error will be raised if the shape of a loop variable after an iteration is determined to be more general than or incompatible with its shape invariant."
        ], 
        "concept": "shape invariant", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The graph visualization can help you understand and debug them."
        ], 
        "concept": "Graph Visualization", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Although the MNIST data isn't time series in nature, we can interpret it as such since there are 784 inputs.", 
            "MNIST data set iterator - 60000 training digits, 10000 test digits, 10 classes."
        ], 
        "concept": "MNIST data", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Hidden layers can learn features from the input layer and it can send those features to be analyzed by our output layer to get the corresponding outputs.", 
            "The input layer is only a set of inputs values fed into the network."
        ], 
        "concept": "input layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The probability mass function (pmf) and cumulative distribution function (cdf) are."
        ], 
        "concept": "cumulative distribution", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A column reduction defines how a single column should be reduced."
        ], 
        "concept": "column reduction", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Device memory will be used for cache (if current backend support such differentiation).", 
            "DEVICE memory will probably have the same size, but won't be accounted in this value."
        ], 
        "concept": "device memory", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A Mixture is defined by a Categorical (cat, representing the mixture probabilities) and a list of Distribution objects all having matching dtype, batch shape, event shape, and continuity properties (the components)."
        ], 
        "concept": "mixture probabilities", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Nested tuple shapes are not supported."
        ], 
        "concept": "Nested tuple", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Examples: x is a batch matrix with compatible shape for matmul if."
        ], 
        "concept": "batch matrix", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "GPU operations are called in an asynchronously way, so that they are not blocking your CPU.", 
            "By default, GPU operations are asynchronous."
        ], 
        "concept": "GPU operations", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The training workflow usually proceeds as follows: Prepare training code with a few components: a."
        ], 
        "concept": "training workflow", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "An optimization result represents the results of an optimization run, including the canditate configuration, the trained model, the score for that model, and index of the model."
        ], 
        "concept": "optimization results", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "While handwriting recognition has been attempted by different machine learning algorithms over the years, deep learning performs MNIST dataset.", 
            "If you are reading this, hopefully you can appreciate how effective some machine learning models are.", 
            "Many machine learning models are represented by composing layers.", 
            "Machine learning provides many algorithms to classify flowers statistically.", 
            "Machine learning techniques have a set of parameters that have to be chosen before any training can begin.", 
            "Some simple models can be described with a few lines of algebra, but complex machine learning models have a large number of parameters that are difficult to summarize.", 
            "When publishing research models and techniques, most machine learning practitioners share: code to create the model, and.", 
            "The reason you can't simply use your training data for evaluation is because machine learning methods are prone to overfitting (getting good at making predictions about the training set, but not performing well on larger datasets).", 
            "Overfitting is when a machine learning model performs worse on new data than on their training data.", 
            "Typical machine learning, of course, has one hidden layer, and those shallow nets are called Perceptrons.", 
            "Machine learning models, however, need to have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!)."
        ], 
        "concept": "Machine Learning", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Because Inception networks are large, we will use the Deeplearning4j model zoo to help build our Zeppelin, make sure you add the deeplearning4j-zoo artifact to the Spark interpreter."
        ], 
        "concept": "Inception networks", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The MKL is optimized for NCHW and Intel is working to get near performance parity when using NHWC.", 
            "However, MKL is liked with by default (when available) so setting this option explicitly is not usually required."
        ], 
        "concept": "mkl", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "NOTE: for Android development, adb shell is needed otherwise the following section of tutorial will not run."
        ], 
        "concept": "adb", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Tensor along dimension is a powerful technique, but can be a little hard to understand at first."
        ], 
        "concept": "tensor along", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Raises: Exception if covariance type is unknown."
        ], 
        "concept": "covariance type", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "One use case is that a special unknown word token is used as ID 0."
        ], 
        "concept": "unknown word", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Activation Layer Used to apply activation on input and corresponding derivative on epsilon."
        ], 
        "concept": "apply activation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Another way to think about this is that the output dimension equals the number of weights of the linear model; the larger this dimension, the larger the \"degrees of freedom\" of the model.", 
            "However, after a certain threshold, higher output dimensions increase the accuracy by very little, while making training take more time."
        ], 
        "concept": "output dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Autograd is reverse automatic differentiation system.", 
            "Recall that Function s are what autograd uses to compute the results and gradients, and encode the operation history.", 
            "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.", 
            "Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU.", 
            "Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors."
        ], 
        "concept": "Autograd", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Visual Studio doesn't support parallel custom task currently."
        ], 
        "concept": "Visual Studio", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Spatial dropout: can only be applied to 4D (convolutional) activations."
        ], 
        "concept": "spatial dropout", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "For many corpora, average sentence length is six words.", 
            "A:If average sentence contains 6 words, and window size is 5, maximum theoretical number of 10 skipgram rounds will be achieved on 0 words."
        ], 
        "concept": "average sentence", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Trajectory clustering can be a difficult problem to solve when your data isn't quite \"even\"."
        ], 
        "concept": "Trajectory clustering", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Since shape inference is an optional feature, and the shapes of tensors may vary dynamically, shape functions must be robust to incomplete shape information for any of the inputs.", 
            "Unknown shape: has an unknown number of dimensions, and an unknown If a tensor is produced by an operation of type \"Foo\", its shape may be inferred if there is a registered shape function for \"Foo\"."
        ], 
        "concept": "shape functions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Here's the impact of our L2 regularization penalty: As you can see, the L2 regularized model has become much more resistant to overfitting than the baseline model, even though both models have the same number of parameters."
        ], 
        "concept": "baseline model", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The training argument takes a boolean specifying whether or not the model is currently being run in training mode; dropout will only be performed if training is True.", 
            "Example 3: Training models with weights merge on GPU (recommended for NV-link)."
        ], 
        "concept": "training model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The input name should be based on a tensorflow type or onnx type, not the nd4j name."
        ], 
        "concept": "input names", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "After I trained this model for a few hours, the average training speed for epoch 10 was slow down to 40s.", 
            "However, I noticed that the training speed gets slow down slowly at each batch and memory usage on GPU also increases.", 
            "Currently, the memory usage would not increase but the training speed still gets slower batch-batch."
        ], 
        "concept": "training speed", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Consequently, an array of all 1s (or, indeed any array of equal values) will result in the same performance as no cost array; non- Evaluation.", 
            "A cost array can be used to bias the multi class predictions towards or away from certain classes."
        ], 
        "concept": "cost array", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A larger variance (99%) will result in a higher order feature set."
        ], 
        "concept": "larger variance", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "partitioner: Partitioner to be passed to the Checkpointable API."
        ], 
        "concept": "Partitioner", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Elementwise scalar multiplication looks like this: And produces this: Subtraction and division follow a similar pattern: If you perform all these operations on your initial 2 x 2 matrix, you should end up with this matrix: Elementwise vector operations."
        ], 
        "concept": "vector operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Recall that the CRF computes a conditional probability.", 
            "The LSTM tagger above is typically sufficient for part-of-speech tagging, but a sequence model like the CRF is really essential for strong performance on NER."
        ], 
        "concept": "CRFs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A fused RNN cell represents the entire RNN expanded over the time dimension.", 
            "RNN cell composed sequentially of multiple simple cells.", 
            "An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs."
        ], 
        "concept": "rnn cell", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The top element of the column vector combines with the top elements of each column in the matrix, and so forth.", 
            "The two top elements are then multiplied by each other, as are the bottom two, and the two products are added to consolidate in a single scalar."
        ], 
        "concept": "top elements", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The op accepts, for example, input types (float, double, float) and in that case the output type would also be (float, double, float).", 
            "If the output type was qint8 ([-128, 127]), the operation will additionally subtract each value by 128 prior to casting, so that the range of values aligns with the range of qint8."
        ], 
        "concept": "output type", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This function applies a cosine decay function to a provided initial learning rate."
        ], 
        "concept": "decay function", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "VGG11 performed best set at 1ms."
        ], 
        "concept": "vgg11", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad."
        ], 
        "concept": "Korrine", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Cycle schedule Starts at initial learning rate, then linearly increases learning rate until max learning rate is reached, at that point the learning rate is decreased back to initial learning rate.", 
            "If learning rate shrinks too much, the net's learning is no longer efficient.", 
            "There are infinite configurations of the learning rate and hidden layer size, since the learning rate space is continuous.", 
            "For eg, if a learning rate is specified this learning rate will apply to all unfrozen/trainable layers in the model.", 
            "If the score increases consistently, your learning rate is likely set too high.", 
            "Learning configs (like updaters, learning rate etc) specified with the layer here will be honored.", 
            "The learning rate is one of, if not the most important hyperparameter.", 
            "Also, the default learning rate is not optimal for all of the models, so to achieve maximum accuracy it would be necessary to tune for each model separately.", 
            "This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.", 
            "Specify the preprocessor for the added layers Usage example: specify a learning rate will set specified learning rate on all layers The specified layer and the layers preceding it will be \"frozen\" with parameters staying constant.", 
            "Add layers to the net Usage example: specify a learning rate will set specified learning rate on all layers Note this will also affect the layer that follows the layer specified, unless it is the output layer.", 
            "The issues mentioned above (learning rate, normalization, data shuffling) may contribute to this.", 
            "If not set, the loss, the learning rate, and the global norm of the gradients will be reported.", 
            "Learning rate schedules can be specified either based on the number of iterations, or the number of epochs that have elapsed.", 
            "If the score is flat or decreases very slowly (over a few hundred iterations) (a) your learning rate may be too low, or (b) you might be having difficulties with optimization.", 
            "The Learning rate curve Looks something like this: +-----------------------------------------+ | XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XX | | XX XXX | | XXX | | XXX | | | +-----------------------------------------+."
        ], 
        "concept": "learning rate", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The output tensor has shape.", 
            "Other output tensor can be duplicated as well.", 
            "When all the input tensors are finished, the output tensor is passed along in the graph.", 
            "Note that a tensor can be both an inside tensor and an output tensor if it is consumed by operations both outside and inside of ops.", 
            "The output tensor is 1-D of size steps.", 
            "All this output tensor knows is its data and shape.", 
            "For instance, some output tensor can be omitted.", 
            "Output tensor has one more dimension than input tensor, the first dimension indicates the partition.", 
            "Like the input, the resulting output tensors have a batch dimension.", 
            "The output tensors can also be remapped.", 
            "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1.", 
            "Currently, only 4-D output tensors (batched image-like tensors) are supported.", 
            "keepdim (bool): whether the output tensors have :attr:`dim` retained or not.", 
            "If a directory is passed to --outdir option, the outputs will be saved as npy files named after output tensor keys under the given directory."
        ], 
        "concept": "Output Tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Indicates how a distributed variable will be aggregated."
        ], 
        "concept": "distributed variable", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Negative shift values will shift elements in the opposite direction."
        ], 
        "concept": "shift values", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened."
        ], 
        "concept": "trailing dimension", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "if a decimal output value is required."
        ], 
        "concept": "output value", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Defines, if adaptive gradients should be created during vocabulary mastering.", 
            "This method defines whether adaptive gradients should be used or not.", 
            "This method defines if Adaptive Gradients should be used in calculations.", 
            "This method specifies, if adaptive gradients should be used during model training."
        ], 
        "concept": "adaptive gradients", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Cosine similarity Note that you need to initialize a scaling constant equal to the norm2 of the vector."
        ], 
        "concept": "Cosine similarity", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "weight decay should not be used when learning \\(a\\) for good performance.", 
            "note:: weight decay should not be used when learning :math:`a` for good performance."
        ], 
        "concept": "weight decay", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.", 
            "The KL divergence, found in the variational autoencoder loss, is an example."
        ], 
        "concept": "kl divergence", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The features dictionary maps feature column names to Tensors containing the corresponding feature data, and labels is a Tensor containing the batch's label data.", 
            "Label data will be converted to a one-hot representation automatically."
        ], 
        "concept": "Label data", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The total variation is the sum of the absolute differences for neighboring pixel-values in the input images."
        ], 
        "concept": "total variation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "During execution any unknown shape dimensions are determined dynamically, see Tensor Shapes for more details."
        ], 
        "concept": "Tensor Shapes", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "So, although raw data can be numerical or categorical, an ML model represents all features as numbers.", 
            "ML models generally represent categorical values as simple vectors in which a 1 represents the presence of a value and a 0 represents the absence of a value."
        ], 
        "concept": "ML model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The poisson distribution is defined over the integers."
        ], 
        "concept": "Poisson distribution", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Suppose our input examples consist of different words from a limited palette of only 81 words."
        ], 
        "concept": "input examples", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "TFGAN is a lightweight library for training and evaluating GANs."
        ], 
        "concept": "TFGAN", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The mixture model is defined by a Categorical distribution (the mixture) and a python list of Distribution objects."
        ], 
        "concept": "mixture model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "From the bijective, nonzero differentiability of g, the inverse function theorem implies g^{-1} is differentiable in the image of g."
        ], 
        "concept": "inverse function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Naively employing asynchronous updates of model parameters leads to sub-optimal training performance because an individual model replica might be trained on a stale copy of the model parameters."
        ], 
        "concept": "model replicas", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output."
        ], 
        "concept": "ellipsis dimensions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Consequently, the parameters (post noise) should be cleared after each training iteration."
        ], 
        "concept": "post noise", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If None (default), static shape information for batch sizes is omitted."
        ], 
        "concept": "static Shape", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Momentum and Adagrad use variables to accumulate updates.", 
            "Adagrad keeps a history of gradients being passed in."
        ], 
        "concept": "adagrad", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Internally, SQSS has a queue for the input examples."
        ], 
        "concept": "SQSS", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "VSMs have a long, rich history in NLP, but all methods depend in some way or Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning.", 
            "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other')."
        ], 
        "concept": "VSMs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Similarly, evaluation metrics used for regression differ from classification.", 
            "Evaluation metrics are an essential part of training a model."
        ], 
        "concept": "evaluation metrics", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "My guess is that tensorflow may not cache the intermediate feature maps in the graphdef mode, but pytorch may do.", 
            "Once setup is completed, Tensorflow can interact with S3 in a variety of ways.", 
            "sets module: Tensorflow set operations.", 
            "You can specify a continuous feature like so: Although, as a single real number, a continuous feature can often be input directly into the model, Tensorflow offers useful transformations for this sort of column as well."
        ], 
        "concept": "Tensorflow", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.", 
            "For instance, a beam search in sequence-to-sequence translation is a loop over the (varying) sequence length of inputs."
        ], 
        "concept": "beam search", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "SVD decomposiiton of a matrix The decomposition is such that: A = U x S x VT L will be the same dimensions as A.", 
            "Analogously, the SVD on GPU uses the MAGMA routine gesdd as well."
        ], 
        "concept": "svd", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The resulting operation takes no inputs."
        ], 
        "concept": "resulting operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A bernoulli trial is a mechanism for detecting the probability of a given event occurring k times in n independent trials."
        ], 
        "concept": "Bernoulli trials", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The capacity argument controls the how long the prefetching is allowed to grow the queues."
        ], 
        "concept": "Prefetching", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "alpha: Confidence interval level desired."
        ], 
        "concept": "interval level", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This method defines, if ETL time per iteration should be reported together with other data.", 
            "In our dl4j-examples repo, we don't make the ETL asynchronous, because the point of examples is to keep them simple."
        ], 
        "concept": "ETL", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "None, control outputs are enabled."
        ], 
        "concept": "control outputs", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Device memory will be used for cache (if current backend support such differentiation).", 
            "DEVICE memory will probably have the same size, but won't be accounted in this value."
        ], 
        "concept": "device memory", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The network will be trained similarly to the network trained tutorial 15.", 
            "Training statistics may include things like per-epoch run times, These statistics are primarily used for debugging and optimization, in order to gain some insight into what aspects of network training are taking the most time."
        ], 
        "concept": "network trained", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Also, each tensor in the tensor list needs to reside on a different GPU.", 
            "Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called.", 
            "Append to lists in loops (tensor list ops are automatically created): Nested control flow."
        ], 
        "concept": "tensor list", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Here's the impact of our L2 regularization penalty: As you can see, the L2 regularized model has become much more resistant to overfitting than the baseline model, even though both models have the same number of parameters."
        ], 
        "concept": "baseline model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "By convention, dimensions are listed in increasing order of dimension number.", 
            "The lowest dimension number in dimensions is the slowest varying dimension (most major) in the loop nest which collapses these dimension, and the highest dimension number is fastest varying (most minor).", 
            "Example with contracting dimension numbers: Associated batch dimension numbers from the 'lhs' and 'rhs' must have the same dimension number, must be listed in the same order in both arrays, must have the same dimension sizes, and must be ordered before contracting and non-contracting/non-batch dimension numbers."
        ], 
        "concept": "dimension number", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "For many corpora, average sentence length is six words.", 
            "A:If average sentence contains 6 words, and window size is 5, maximum theoretical number of 10 skipgram rounds will be achieved on 0 words."
        ], 
        "concept": "average sentence", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If input shapes have rank-R, then output shape will have rank-(R+1).", 
            "Note that for convolutional models, input shape information follows the NCHW convention."
        ], 
        "concept": "Input shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Sequence models are central to NLP: they are models where there is some sort of dependence through time between your Model for part-of-speech tagging.", 
            "The LSTM tagger above is typically sufficient for part-of-speech tagging, but a sequence model like the CRF is really essential for strong performance on NER."
        ], 
        "concept": "sequence model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "I is Win + Sin where + is element-wise In summary, the scatter operation can be defined as follows."
        ], 
        "concept": "scatter operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A series of nodes can be ungrouped so that the nodes in the series do not Selection can also be helpful in understanding high-degree nodes."
        ], 
        "concept": "degree node", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Sequence models are central to NLP: they are models where there is some sort of dependence through time between your Model for part-of-speech tagging.", 
            "The LSTM tagger above is typically sufficient for part-of-speech tagging, but a sequence model like the CRF is really essential for strong performance on NER."
        ], 
        "concept": "sequence model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "I is Win + Sin where + is element-wise In summary, the scatter operation can be defined as follows."
        ], 
        "concept": "scatter operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The optimization algorithm is how updates are made, given the gradient."
        ], 
        "concept": "optimization algorithms", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened."
        ], 
        "concept": "trailing dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The true rank of an array is the number of dimensions which have a size greater than 1."
        ], 
        "concept": "true rank", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Indicates how a distributed variable will be aggregated."
        ], 
        "concept": "distributed variable", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Get the false positives count for the specified output."
        ], 
        "concept": "false positives", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If no global Keras session exists at this point: we will create a new global session."
        ], 
        "concept": "Keras session", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The forward transformation creates samples, the inverse is useful for computing probabilities.", 
            "If X is a scalar then the forward transformation is: scale * X + shift where * denotes the scalar product."
        ], 
        "concept": "forward transformation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This operation applies Max operation to specific inputs through given indices Expected arguments: input: array to be updated indices: array containing indexes for first dimension of input updates: array containing elements to be interfered with input."
        ], 
        "concept": "Max operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The gradient computation of this operation will only take advantage of sparsity in the input gradient when that gradient comes from a Relu.", 
            "If gradients are computed in that context, then the gradient computation is recorded as well.", 
            "Note: The gradient computation on GPU is faster for large matrices but not for large batch dimensions when the submatrices are small."
        ], 
        "concept": "Gradient Computation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "By comparing these the AUC is generated, with some discretization error."
        ], 
        "concept": "AUC", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The feature indices represented as a dense tensor."
        ], 
        "concept": "feature indices", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This data should have the same distribution as the real-world data you want to make predictions about with your model."
        ], 
        "concept": "world data", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Autoencoders are neural networks for unsupervised learning.", 
            "The autoencoder here has been tuned to converge with an average reconstruction error of approximately 2% when trained for 35+ epochs.", 
            "In deep learning, an autoencoder is a neural network that \"attempts\" to reconstruct its input.", 
            "In practice, autoencoders are often applied to data denoising and dimensionality reduction.", 
            "This is because our seq2seq autoencoder uses multiple inputs/outputs.", 
            "Autoencoders are also useful for data visualization when the raw input data has high dimensionality and cannot easily be plotted.", 
            "MADE: Masked Autoencoder for Distribution Estimation."
        ], 
        "concept": "autoencoder", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that sgv is modified in place."
        ], 
        "concept": "sgv", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "In a multi-headed model, each head is represented by an entry in this dict.", 
            "Single-headed models only need to specify one entry in this dictionary."
        ], 
        "concept": "headed model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Sequence classification is one common use of masking."
        ], 
        "concept": "sequence classification", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Suppose tensor length is n, there are d devices and g gather shards."
        ], 
        "concept": "tensor length", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "RNN layers in DL4J can be combined with other layer types."
        ], 
        "concept": "layer types", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Checkpoint model file must exist."
        ], 
        "concept": "model file", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Weight parameter keys given the layer configuration."
        ], 
        "concept": "weight parameters", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The running estimates are Note."
        ], 
        "concept": "running estimates", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If the user-defined function passed into the map transformation changes the size of the elements, then the ordering of the map transformation and the transformations that buffer elements affects the memory usage."
        ], 
        "concept": "map transformation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The gradient computation of this operation will only take advantage of sparsity in the input gradient when that gradient comes from a Relu.", 
            "If gradients are computed in that context, then the gradient computation is recorded as well.", 
            "Note: The gradient computation on GPU is faster for large matrices but not for large batch dimensions when the submatrices are small."
        ], 
        "concept": "Gradient Computation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Default: 1e-2 inplace: can optionally do the operation in-place."
        ], 
        "concept": "inplace", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This error or loss score will eventually converge to a value close to zero."
        ], 
        "concept": "loss score", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "On the other hand, the validation loss will be identical whether we shuffle the validation set or not."
        ], 
        "concept": "VL", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "loss: The loss reported."
        ], 
        "concept": "loss reported", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that sgv1 is modified in place."
        ], 
        "concept": "sgv1", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Determining if the input pipeline is the bottleneck can be complicated.", 
            "If the input pipeline is shared between training and validation, restoring the checkpoint during validation may override the validation input pipeline.", 
            "To make the image processing pipeline easier to explain, assume that the input pipeline is targeting 8 GPUs with a batch size of 256 (32 per GPU).", 
            "This hook saves the state of the iterators in the Graph so that when training is resumed the input pipeline continues from where it left off.", 
            "Before the model starts running all the stages, the input pipeline stages are warmed up to prime the staging buffers in between with one set of data.", 
            "If the difference in examples per second for the full model and the trivial model is minimal then the input pipeline is likely a bottleneck.", 
            "Example of checkpointing the training pipeline: This hook should be used if the input pipeline state needs to be saved separate from the model checkpoint.", 
            "If GPU utilization is not approaching 80-100%, then the input pipeline may be the bottleneck.", 
            "The input pipeline checkpoint may be large, if there are large shuffle or prefetch buffers for instance, and may bloat the checkpoint size."
        ], 
        "concept": "Input Pipeline", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model)."
        ], 
        "concept": "Predictive models", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Passthrough = feed forward the input mask (if/when necessary) but don't actually apply it."
        ], 
        "concept": "Passthrough", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Loads LFW faces data transform.", 
            "Thus, LFW images are scaled to 28 pixels x 28 pixels."
        ], 
        "concept": "LFW", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The GPU version is still a bit slower than the CPU version, but not by much."
        ], 
        "concept": "GPU version", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "However, the cluster centers may be retrieved by the latest checkpoint saved during training."
        ], 
        "concept": "cluster center", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If the repeat transformation is applied before the shuffle transformation, then the epoch boundaries are blurred."
        ], 
        "concept": "repeat transformation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A gradient op represents a jacobian operation."
        ], 
        "concept": "gradient operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This attention energies tensor is the same size as the encoder output, and the two are ultimately multiplied, resulting in a weighted tensor whose largest values represent the most important parts of the query sentence at a particular time-step of decoding."
        ], 
        "concept": "energies tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "I knew training percentage is correct, but couldn't figure out why it's wrong for validation and test."
        ], 
        "concept": "training percentage", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The Google model may take as much as 10G of RAM, and the JVM only launches with 256 MB of RAM, so you have to adjust your heap space."
        ], 
        "concept": "Google model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Vector aggregations are saved only by Shards started aggregation process."
        ], 
        "concept": "aggregation process", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The resulting dimensions are: (batch, sequence, embedding)."
        ], 
        "concept": "resulting dimensions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Autoencoders are neural networks for unsupervised learning.", 
            "The autoencoder here has been tuned to converge with an average reconstruction error of approximately 2% when trained for 35+ epochs.", 
            "In deep learning, an autoencoder is a neural network that \"attempts\" to reconstruct its input.", 
            "In practice, autoencoders are often applied to data denoising and dimensionality reduction.", 
            "This is because our seq2seq autoencoder uses multiple inputs/outputs.", 
            "Autoencoders are also useful for data visualization when the raw input data has high dimensionality and cannot easily be plotted.", 
            "MADE: Masked Autoencoder for Distribution Estimation."
        ], 
        "concept": "autoencoder", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The training argument takes a boolean specifying whether or not the model is currently being run in training mode; dropout will only be performed if training is True.", 
            "Example 3: Training models with weights merge on GPU (recommended for NV-link)."
        ], 
        "concept": "training model", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Nested tuple shapes are not supported."
        ], 
        "concept": "Nested tuple", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Complex128 elements must be written as two consecutive DOUBLE values, real component first."
        ], 
        "concept": "Complex128", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The training workflow usually proceeds as follows: Prepare training code with a few components: a."
        ], 
        "concept": "training workflow", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The Google model may take as much as 10G of RAM, and the JVM only launches with 256 MB of RAM, so you have to adjust your heap space."
        ], 
        "concept": "Google model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The maximum absolute difference allowed."
        ], 
        "concept": "maximum absolute", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "I knew training percentage is correct, but couldn't figure out why it's wrong for validation and test."
        ], 
        "concept": "training percentage", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Complex128 elements must be written as two consecutive DOUBLE values, real component first."
        ], 
        "concept": "Complex128", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Since shape inference is an optional feature, and the shapes of tensors may vary dynamically, shape functions must be robust to incomplete shape information for any of the inputs.", 
            "Unknown shape: has an unknown number of dimensions, and an unknown If a tensor is produced by an operation of type \"Foo\", its shape may be inferred if there is a registered shape function for \"Foo\"."
        ], 
        "concept": "shape functions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The maximum absolute difference allowed."
        ], 
        "concept": "maximum absolute", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "One use case is that a special unknown word token is used as ID 0."
        ], 
        "concept": "unknown word", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Autograd is reverse automatic differentiation system.", 
            "Recall that Function s are what autograd uses to compute the results and gradients, and encode the operation history.", 
            "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.", 
            "Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU.", 
            "Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors."
        ], 
        "concept": "Autograd", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The final dimension contains the indices of top-k labels.", 
            "The final dimension contains the top k predicted class indices.", 
            "The final dimension contains the logit values for each class."
        ], 
        "concept": "final dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Default: 1e-2 inplace: can optionally do the operation in-place."
        ], 
        "concept": "inplace", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The MKL is optimized for NCHW and Intel is working to get near performance parity when using NHWC.", 
            "However, MKL is liked with by default (when available) so setting this option explicitly is not usually required."
        ], 
        "concept": "mkl", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Sequence classification is one common use of masking."
        ], 
        "concept": "sequence classification", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The input tags and values must have the same shape."
        ], 
        "concept": "input tags", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The graph visualization can help you understand and debug them."
        ], 
        "concept": "Graph Visualization", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Activation Layer Used to apply activation on input and corresponding derivative on epsilon."
        ], 
        "concept": "apply activation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Marine Automatic Identification System (AIS) is an open system for marine broadcasting of positions.", 
            "AIS data, coordinates can be reported at irregular intervals over time.", 
            "Furthermore, AIS data for 1 year is over 100GB compressed."
        ], 
        "concept": "AIS", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Raises: Exception if covariance type is unknown."
        ], 
        "concept": "covariance type", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "However, if a function with data-dependent if statements and loops is traced, only the operations called along the execution route taken by the example input will be recorded."
        ], 
        "concept": "example inputs", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The input name should be based on a tensorflow type or onnx type, not the nd4j name."
        ], 
        "concept": "input names", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A larger variance (99%) will result in a higher order feature set."
        ], 
        "concept": "larger variance", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If the user-defined function passed into the map transformation changes the size of the elements, then the ordering of the map transformation and the transformations that buffer elements affects the memory usage."
        ], 
        "concept": "map transformation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "partitioner: Partitioner to be passed to the Checkpointable API."
        ], 
        "concept": "Partitioner", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Vector aggregations are saved only by Shards started aggregation process."
        ], 
        "concept": "aggregation process", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The inferred shape of a tensor is used to provide shape information without having to launch the graph in a session.", 
            "These inferred shapes might have known or unknown rank."
        ], 
        "concept": "inferred shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The matrix columns represent the prediction labels and the rows represent the real labels."
        ], 
        "concept": "matrix column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Hidden layers can learn features from the input layer and it can send those features to be analyzed by our output layer to get the corresponding outputs.", 
            "The input layer is only a set of inputs values fed into the network."
        ], 
        "concept": "input layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Dataflow is a common programming model for parallel computing."
        ], 
        "concept": "Dataflow", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that an alias input may itself be an alias."
        ], 
        "concept": "alias input", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer."
        ], 
        "concept": "regularization losses", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Logistic regression is one in which the dependant variable is categorical rather than continuous - meaning that it can predict only a limited number of classes or categories, like a switch you flip on or off.", 
            "We will train a logistic regression model that, given an individual's information, outputs a number between 0 and 1\u2014this can be interpreted as the probability that the individual has an annual income of over 50,000 dollars."
        ], 
        "concept": "logistic regression", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If mat1 is a \\((n \\times m)\\) tensor, mat2 is a Note."
        ], 
        "concept": "mat1", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This feature cross enables the model to train on pricing conditions related to each individual section, which is a much stronger signal than latitude and longitude alone."
        ], 
        "concept": "feature cross", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A fused RNN cell represents the entire RNN expanded over the time dimension.", 
            "RNN cell composed sequentially of multiple simple cells.", 
            "An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs."
        ], 
        "concept": "rnn cell", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The only difference is that after pooling regions are generated, a mean operation is performed instead of a max operation in each pooling region."
        ], 
        "concept": "mean operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This parameter is only used when Distribution Strategy is used with estimator or keras."
        ], 
        "concept": "Distribution strategy", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The output shape is the same as the input shape.", 
            "In terms of this constant, the output shape is.", 
            "If input shapes have rank-R, then output shape will have rank-(R+1).", 
            "If indices is a scalar the output shape will be a vector of length depth.", 
            "The parameters \\(\\mu\\) and \\(\\sigma\\), and output shape have to have a floating point elemental type.", 
            "Output shape is [1, height, width, channels]."
        ], 
        "concept": "output shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If the computation time takes longer than the copy and aggregation, the copy itself becomes essentially free."
        ], 
        "concept": "computation time", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The Evaluation class is used for evaluation.", 
            "In DL4J the Evaluation Class and variants of the Evaluation Class are available to evaluate your model's performance.", 
            "An Evaluation class has more built-in methods if you need to extract a Under Curve (AUC)."
        ], 
        "concept": "Evaluation Class", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Determines whether all sampled classes in a batch are unique."
        ], 
        "concept": "sampled class", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Fused batch norm combines the multiple operations needed to do batch normalization into a single kernel.", 
            "If present, then the batch normalization uses weighted mean and variance.", 
            "Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and This layer uses statistics computed from input data in both training and evaluation modes.", 
            "Because the Batch Normalization is done over the `C` dimension, computing statistics on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.", 
            "Batch norm is an expensive process that for some models makes up a large percentage of the operation time.", 
            "In many cases, the inference graph will be different from the training graph: for example, techniques like dropout and batch normalization use different operations in each case."
        ], 
        "concept": "Batch normalization", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "So, when multiple indices in updates refer to the same index in operand, the corresponding value in output will be non-deterministic.", 
            "If indices is rank 1 (a vector), then for each position i, out[indices[i], ...] = updates[i, ...] Note that if multiple indices refer to the same location, the output at those locations is undefined - different updates may occur in different orders.", 
            "If indices is rank 1 (a vector), then for each position i, out[indices[i], ...] -= updates[i, ...] Note that if multiple indices refer to the same location, the contributions from each is handled correctly."
        ], 
        "concept": "multiple indices", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A column reduction defines how a single column should be reduced."
        ], 
        "concept": "single column", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The output shape is the same as the input shape.", 
            "In terms of this constant, the output shape is.", 
            "If input shapes have rank-R, then output shape will have rank-(R+1).", 
            "If indices is a scalar the output shape will be a vector of length depth.", 
            "The parameters \\(\\mu\\) and \\(\\sigma\\), and output shape have to have a floating point elemental type.", 
            "Output shape is [1, height, width, channels]."
        ], 
        "concept": "output shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Recall that the CRF computes a conditional probability.", 
            "The LSTM tagger above is typically sufficient for part-of-speech tagging, but a sequence model like the CRF is really essential for strong performance on NER."
        ], 
        "concept": "CRFs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Getting Dense Word Embeddings An Example: N-Gram Language Modeling Exercise: Computing Word Embeddings: Continuous Bag-of-Words."
        ], 
        "concept": "Dense Word", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device."
        ], 
        "concept": "single device", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Getting Dense Word Embeddings An Example: N-Gram Language Modeling Exercise: Computing Word Embeddings: Continuous Bag-of-Words."
        ], 
        "concept": "Dense Word", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "However, if the range of the data is limited, floating-point addition is close enough to being associative for most practical uses."
        ], 
        "concept": "point addition", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Exploding gradients are problematic as they can 'mess up' the parameters of your network In the case of recurrent neural networks, adding some gradient normalization or gradient clipping may help."
        ], 
        "concept": "Exploding gradients", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Currently random search and grid search are supported."
        ], 
        "concept": "Random Search", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If no initializer is provided, the output structure and dtypes of fn are assumed to be the same as its input; and in this case, the first argument of fn must match the structure of elems."
        ], 
        "concept": "output structure", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "While the GPU is busy your multiple workers can load a new batch of images instead of a large file.", 
            "If multiple workers or threads all execute count in parallel, there is no guarantee that access to the variable v is atomic at any point within any thread's calculation of count."
        ], 
        "concept": "multiple worker", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "An example sequence of events, leading to an outdated workspace pointer: Workspace W is opened (iteration 1)."
        ], 
        "concept": "example sequence", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Dataflow is a common programming model for parallel computing."
        ], 
        "concept": "Dataflow", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The resulting behavior may be undefined."
        ], 
        "concept": "resulting behavior", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "TF Lite allows mobile developers to do inference efficiently on mobile devices."
        ], 
        "concept": "mobile devices", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The only difference is that after pooling regions are generated, a mean operation is performed instead of a max operation in each pooling region."
        ], 
        "concept": "mean operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Small gradients means it is hard to learn.", 
            "During backproping, small gradients might underflow in the reduced numerical range, causing a model to converge at suboptimal level."
        ], 
        "concept": "Small gradients", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This parameter is only used when Distribution Strategy is used with estimator or keras."
        ], 
        "concept": "Distribution strategy", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Currently random search and grid search are supported."
        ], 
        "concept": "Random Search", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Torch Script currently does not support mutating tensors in place, so any tensor indexing can only appear on the right-hand size of an expression."
        ], 
        "concept": "Tensor index", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "While the GPU is busy your multiple workers can load a new batch of images instead of a large file.", 
            "If multiple workers or threads all execute count in parallel, there is no guarantee that access to the variable v is atomic at any point within any thread's calculation of count."
        ], 
        "concept": "multiple worker", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Separating out the auxiliary nodes typically doesn't remove critical information since these nodes are usually related to bookkeeping functions."
        ], 
        "concept": "auxiliary nodes", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer."
        ], 
        "concept": "regularization losses", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note, LARS scaling is currently only enabled for dense tensors."
        ], 
        "concept": "LARS", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Fused batch norm combines the multiple operations needed to do batch normalization into a single kernel.", 
            "If present, then the batch normalization uses weighted mean and variance.", 
            "Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and This layer uses statistics computed from input data in both training and evaluation modes.", 
            "Because the Batch Normalization is done over the `C` dimension, computing statistics on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.", 
            "Batch norm is an expensive process that for some models makes up a large percentage of the operation time.", 
            "In many cases, the inference graph will be different from the training graph: for example, techniques like dropout and batch normalization use different operations in each case."
        ], 
        "concept": "Batch normalization", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Output column name is the same as the input column name.", 
            "Suppose the possible String values were {\"a\",\"b\",\"c\",\"d\"} and the String column value to be converted contained the String \"a,c\", then the 4 output columns would have values [\"true\",\"false\",\"true\",\"false\"]."
        ], 
        "concept": "output column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted."
        ], 
        "concept": "sufficient statistic", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Small gradients means it is hard to learn.", 
            "During backproping, small gradients might underflow in the reduced numerical range, causing a model to converge at suboptimal level."
        ], 
        "concept": "Small gradients", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Separating out the auxiliary nodes typically doesn't remove critical information since these nodes are usually related to bookkeeping functions."
        ], 
        "concept": "auxiliary nodes", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The trained model can then be used as an application resource."
        ], 
        "concept": "trained models", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The probability density function (pdf) is: Where scale = sigma is the standard deviation of the underlying normal distribution.", 
            "The probability density function (pdf) is, Examples."
        ], 
        "concept": "probability density function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Gradients are pushed to them and the chief worker will wait until enough gradients are collected and then average them before applying to variables."
        ], 
        "concept": "chief worker", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "stddev: The standard deviation of the Gaussian kernel to be approximated.", 
            "The standard deviation is, if you want, the tradeoff between exploration and exploitation: the smaller the std, the less exploration."
        ], 
        "concept": "standard deviations", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "In contrast, Real NVP can compute both forward and inverse computations in parallel."
        ], 
        "concept": "nvprof", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Google Cloud TPU Documentation \u2014Set up and run a Google Cloud TPU.", 
            "Google Cloud provides no setup required, pre-configured virtual machines to help you build your deep learning projects."
        ], 
        "concept": "Google Cloud", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Matrix transpose operation: If input has shape [a,b] output has shape [b,a]."
        ], 
        "concept": "Matrix transpose", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Multiple shifts along multiple axes may be specified."
        ], 
        "concept": "multiple axes", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Converts JavaRDD labeled points to JavaRDD datasets."
        ], 
        "concept": "JavaRDD", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The graph shows the average error is about \\$2,500 dollars."
        ], 
        "concept": "average error", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The probability density function (pdf) is: Where scale = sigma is the standard deviation of the underlying normal distribution.", 
            "The probability density function (pdf) is, Examples."
        ], 
        "concept": "probability density function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The sizes in these dimensions must match, but tensordot will deal with broadcasted dimensions."
        ], 
        "concept": "Tensordot", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The classes Tensor must provide string labels, not integer class IDs."
        ], 
        "concept": "classes tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This class defines input desired for any given node/operation within graph."
        ], 
        "concept": "input desired", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "However this is not the desired behaviour, since the missing loss component should be treated as unknown rather than zero."
        ], 
        "concept": "loss component", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "We include an example in the NLP section since word similarity visualization is a common use."
        ], 
        "concept": "word similarity", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "I think tfrecord format is good way to save data chunks, and avoids reading lots of small files, which is especially slow on hdfs."
        ], 
        "concept": "TFRecord format", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "MNIST dataset, please resize the images from the dataset to 32x32.", 
            "The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here: Fashion MNIST is intended as a drop-in replacement for the classic MNIST dataset\u2014often used as the \"Hello, World\" of machine learning programs for computer vision.", 
            "MNIST is the \"Hello World\" of deep learning.", 
            "The MNIST digits are transformed into a flat 1D array of length 784 (MNIST images are 28x28 pixels, which equals 784 when you lay them end to end)."
        ], 
        "concept": "MNIST", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "MNIST dataset, please resize the images from the dataset to 32x32.", 
            "The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here: Fashion MNIST is intended as a drop-in replacement for the classic MNIST dataset\u2014often used as the \"Hello, World\" of machine learning programs for computer vision.", 
            "MNIST is the \"Hello World\" of deep learning.", 
            "The MNIST digits are transformed into a flat 1D array of length 784 (MNIST images are 28x28 pixels, which equals 784 when you lay them end to end)."
        ], 
        "concept": "MNIST", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The loss scale is not updated for the lifetime of the class.", 
            "Loss scale manager uses an exponential update strategy.", 
            "Loss scale managers with a different strategy should subclass this class."
        ], 
        "concept": "loss scale", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Premade Estimators are designed to get results out of the box."
        ], 
        "concept": "Premade", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note, LARS scaling is currently only enabled for dense tensors."
        ], 
        "concept": "LARS", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Google Cloud TPU Documentation \u2014Set up and run a Google Cloud TPU.", 
            "Google Cloud provides no setup required, pre-configured virtual machines to help you build your deep learning projects."
        ], 
        "concept": "Google Cloud", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Measures the loss given an input tensor x and a labels tensor y containing values (1 or -1).", 
            "labels tensor are directly broadcasted to all the TPU cores since the partition dims is None."
        ], 
        "concept": "labels tensors", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If None, a learnable temperature is created."
        ], 
        "concept": "learnable temperature", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The gradient calculation and application are delegated to an underlying optimizer."
        ], 
        "concept": "gradient calculation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "However this is not the desired behaviour, since the missing loss component should be treated as unknown rather than zero."
        ], 
        "concept": "loss component", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Run a k nearest neighbors search on a NEW data point.", 
            "To do so, you can select points in multiple After clicking on a point, its nearest neighbors are also selected."
        ], 
        "concept": "nearest neighbor", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The low padding is applied in the direction of lower indices while the high padding is applied in the direction of higher indices."
        ], 
        "concept": "low padding", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This class defines input desired for any given node/operation within graph."
        ], 
        "concept": "input desired", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If None, a learnable temperature is created."
        ], 
        "concept": "learnable temperature", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Separable convolutions split a regular convolution operation into two simpler operations, which are usually computationally more efficient.", 
            "Convolution is the code for applying the convolution operator."
        ], 
        "concept": "convolution operations", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "I think tfrecord format is good way to save data chunks, and avoids reading lots of small files, which is especially slow on hdfs."
        ], 
        "concept": "TFRecord format", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The supported device names are \"/device:CPU:0\" (or \"/cpu:0\") for the CPU device, and \"/device:GPU:i\" (or \"/gpu:i\") for the ith GPU device."
        ], 
        "concept": "supported device", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The output elements will be resorted to preserve the sort order along increasing dimension number.", 
            "In the figure, the element of value 9 is selected by both of the top windows (blue and red) and the binary addition scatter function produces the output element of value 8 (2 + 6).", 
            "The parameters and output element type have to be a boolean type, an integral type or a floating point types, and the types have to be consistent."
        ], 
        "concept": "output element", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "We include an example in the NLP section since word similarity visualization is a common use."
        ], 
        "concept": "word similarity", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The GPUs have to wait for that CPU to finish.", 
            "GPU operations are called in an asynchronously way, so that they are not blocking your CPU.", 
            "In a workstation with multiple GPU cards, each GPU will have similar speed and contain enough memory to run an entire CIFAR-10 model.", 
            "Note, a GPU with CUDA is not critical for this tutorial as a CPU will not take much time.", 
            "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default.", 
            "Below are some other Check if a GPU is underutilized by running nvidia-smi -l 2.", 
            "When GPU RAM is less than CPU RAM, you need to monitor how much RAM is being used off-heap.", 
            "On the server there are 8 GPU installed and they are all the same type 1080Ti.", 
            "As GPUs and other hardware accelerators get faster, preprocessing of data can be a bottleneck.", 
            "If no GPUs are available, then the model is going to be placed on the CPU.", 
            "When each GPU completes the computation, gradients are going to be reduced (added) onto the master GPU.", 
            "By default, GPU operations are asynchronous.", 
            "If you do so, your GPU will run out of RAM when trying to run jobs.", 
            "GPUs are run in their default Boost.", 
            "In this design, each GPU on the server has its own copy of each variable.", 
            "GPUs and a single aggregated gradient is sent to the parameter server.", 
            "The GPUs are synchronized in operation.", 
            "While the process is running, the GPU has still 80% memory blocked and pytorch is using this space.", 
            "This setup requires that all GPUs share the model parameters.", 
            "Update model parameters synchronously by waiting for all GPUs to finish Here is a diagram of this model: Note that each GPU computes inference as well as the gradients for a unique batch of data.", 
            "If JavaCPP or your GPU throw an out-of-memory error (OOM), or even if your compute slows down due to GPU memory being limited, then you may want to either decrease batch size or increase the amount of off-heap memory that JavaCPP is allowed to allocate, if that's possible.", 
            "This utilty and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend.", 
            "Get device (GPU, etc) current bytes - may be null if no compute devices are present in the system.", 
            "While the GPU is busy your multiple workers can load a new batch of images instead of a large file.", 
            "After the GPU finishes it can just get the new batch, which is probably already waiting for it.", 
            "These are used at each layer for doing things like normalization and NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST).", 
            "For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won't be enough for modern deep learning.", 
            "If GPU utilization is not approaching 80-100%, then the input pipeline may be the bottleneck.", 
            "Note that values set here on the layer will be applied to all relevant layers - unless the value is overridden Note: values set by this method will be applied to all applicable layers in the network, unless a different Note: values set by this method will be applied to all applicable layers in the network, unless a different NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST)."
        ], 
        "concept": "GPU", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Root value is left -- right is unused."
        ], 
        "concept": "Root value", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The resulting perturbed image, \\(x'\\), is then misclassified by the target network as a \"gibbon\" when it is still clearly a \"panda\"."
        ], 
        "concept": "perturbed image", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Init has been internally called."
        ], 
        "concept": "inits", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The training process will then continue in this way until the model is fully trained.", 
            "This way you can use a trained model without having to retrain it, or pick-up training where you left of\u2014in case the training process was interrupted.", 
            "The training process itself can take several hours, so make sure you have a machine available for that long.", 
            "If True, performs intermediate model checkpoints and exports during the training process, rather than only once model training is complete.", 
            "If your training process uses workspaces, we recommend that you disable (or reduce the frequency of) periodic GC calls.", 
            "This is very convenient as my training process uses standard disposable cloud workers that should not store anything of a value on their local drives!"
        ], 
        "concept": "Training processes", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A feature column can be either one of the raw inputs in the original features dict (a base feature column), or any new columns created using transformations defined over one or multiple base columns (a derived feature columns).", 
            "Therefore, the code to create the feature column is: Feature columns can be far more sophisticated than those we're showing here.", 
            "The third feature column also specifies a lambda the program will invoke to scale the raw data: # Define three numeric feature columns.", 
            "As long as all feature columns are unweighted sparse columns this computes the prediction of a linear model which stores all weights in a single variable.", 
            "An Estimator expects a vector of numeric inputs, and feature columns describe how the model should convert each feature.", 
            "Each feature column needs a different kind of operation during this conversion.", 
            "A feature column is an object describing how the model should use raw input data from the features dictionary.", 
            "A feature column is an abstract concept of any raw or derived variable that can be used to predict the target label.", 
            "Feature columns are very rich, Estimators can use, allowing easy experimentation.", 
            "Feature Columns, handle a variety of input data types without changes to the model.", 
            "Feature columns can have internal state, like layers, so they often need to be initialized."
        ], 
        "concept": "features columns", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "My GPU memory isn't freed properly\u00b6.", 
            "the cifar10 training example with -opencl, the GPU memory usage grows at a rate of 200MB per second (looking at nvidia-smi) until the lack of memory crashes the program.", 
            "If JavaCPP or your GPU throw an out-of-memory error (OOM), or even if your compute slows down due to GPU memory being limited, then you may want to either decrease batch size or increase the amount of off-heap memory that JavaCPP is allowed to allocate, if that's possible.", 
            "I found that after I passed the output of A to B, the GPU memory increases 4 GB.", 
            "If your GPU memory isn't freed even after Python quits, it is very likely that some Python subprocesses are still alive.", 
            "These are used at each layer for doing things like normalization and NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST).", 
            "Note that values set here on the layer will be applied to all relevant layers - unless the value is overridden Note: values set by this method will be applied to all applicable layers in the network, unless a different Note: values set by this method will be applied to all applicable layers in the network, unless a different NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST)."
        ], 
        "concept": "gpu memory", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A Sequential module simply performs function composition."
        ], 
        "concept": "Sequential module", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Model Requirements: - Model must be a sequential model or functional model."
        ], 
        "concept": "functional model", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Elementwise scalar multiplication can be represented several ways.", 
            "Elementwise scalar multiplication looks like this: And produces this: Subtraction and division follow a similar pattern: If you perform all these operations on your initial 2 x 2 matrix, you should end up with this matrix: Elementwise vector operations."
        ], 
        "concept": "scalar multiplication", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If None (the default), the most recent checkpoint found within the model directory is chosen."
        ], 
        "concept": "recent checkpoint", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "An unknown Dimension is compatible with all other Dimensions."
        ], 
        "concept": "unknown dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Shapes of fixed rank but variable size are allowed by setting any shape dimension to None.", 
            "All shape dimensions must be fully defined."
        ], 
        "concept": "shape dimension", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Training is the stage of machine learning when the model is gradually optimized, or the model learns the dataset.", 
            "The model learns to associate images and labels.", 
            "That is, the model learns the best way to map your input numeric categorical values to the embeddings vector value in order to solve your problem."
        ], 
        "concept": "model learning", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The optional bid value for this cluster's spot instances Uses the on-demand market if empty."
        ], 
        "concept": "bid value", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "One main column can have several sub columns."
        ], 
        "concept": "main column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Separable convolutions split a regular convolution operation into two simpler operations, which are usually computationally more efficient.", 
            "Convolution is the code for applying the convolution operator."
        ], 
        "concept": "convolution operations", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "At least one of {shape, ndim} must be specified."
        ], 
        "concept": "NDim", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A single dimension may be -1, in which case it's inferred from the remaining dimensions and the number of elements in input."
        ], 
        "concept": "single dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Also, the final activation becomes a Sigmoid, which squashes values into a range between 0 and 1."
        ], 
        "concept": "final activation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "By default, the losses are averaged over each loss element in the batch."
        ], 
        "concept": "loss element", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Multidimensional indices include a int64 index for each dimension."
        ], 
        "concept": "Multidimensional indices", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The only difference is that the shortcut function versions create and run the layer in a single call."
        ], 
        "concept": "shortcut function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The lower triangular part of the matrix is defined as the elements on and below the diagonal."
        ], 
        "concept": "lower triangular", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Remember that the input sentences were heavily filtered.", 
            "The input sentence is evaluated Computation Graph: Forward input through encoder model."
        ], 
        "concept": "input sentences", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "By permitting a richer palette of numbers for every cell, an embedding column contains far fewer cells than an indicator column.", 
            "Indicator columns and embedding columns never work on features directly, but instead take categorical columns as input.", 
            "Instead of representing the data as a one-hot vector of many dimensions, an embedding column represents that data as a lower-dimensional, ordinary vector in which each cell can contain any number, not just 0 or 1."
        ], 
        "concept": "embedding columns", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state)."
        ], 
        "concept": "initial input", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Vectorization is the first problem many data scientists will have to solve to start training their algorithms on data."
        ], 
        "concept": "Vectorization", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block."
        ], 
        "concept": "Convolution kernel", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This is because larger epsilons mean we take a larger step in the direction that will maximize the loss."
        ], 
        "concept": "larger step", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that the resulting sequence may be of length 0, if the input sequence is less than or equal to N.", 
            "In addition, once an input sequence element is attended to at a given output timestep, elements occurring before it cannot be attended to at subsequent output timesteps.", 
            "Monotonic attention implies that the input sequence is processed in an explicitly left-to-right manner when generating the output sequence.", 
            "It is recommended to use truncated BPTT when your input sequences are long (typically, more than a few hundred time steps)."
        ], 
        "concept": "Input Sequences", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "How ccache is used to speed up build times."
        ], 
        "concept": "ccache", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "HDF5 is really sensitive about the order its resources are deallocated in."
        ], 
        "concept": "hdf5", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The sum operation operates over all the elements.", 
            "The sum operation still operates over all the elements, and divides by n.", 
            "This operator is similar to the unsorted segment sum operator found here."
        ], 
        "concept": "sum operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A matrix that ordered its elements by row would look like this: Elementwise scalar operations."
        ], 
        "concept": "scalar operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "When training started, the model did not know how to spell an English word, or that words were even a unit of text."
        ], 
        "concept": "English word", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Check that the convolution mode is consistent with the padding specification."
        ], 
        "concept": "convolution mode", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Yet Not a single tutorials works as expected : 1."
        ], 
        "concept": "single tutorials", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Multilayer Network to tweak for transfer learning."
        ], 
        "concept": "Multilayer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "As mentioned, the model under attack is the same MNIST model from pytorch/examples/mnist."
        ], 
        "concept": "MNIST model", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Mean Absolute Error loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
        ], 
        "concept": "Mean absolute error", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "CPU backend supports multiple CPU ISAs.", 
            "CPU info will not be logged.", 
            "CPU backend it might be ignored, depending on Aggregate.", 
            "If this cannot be loaded, the CPU (nd4j-native) backend will be loaded second.", 
            "While the accelerator is performing training step N, the CPU is preparing the data for step N+1.", 
            "On the other hand, higher values can increase contention if CPU is scarce.", 
            "This function may be used when CPU time is scarce and inputs are trusted or unimportant."
        ], 
        "concept": "CPU", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "expected inputs are 3-D, 4-D or 5-D in shape.", 
            "Expected inputs are spatial (4 dimensional)."
        ], 
        "concept": "Expected inputs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Singleton evaluation hrunner class for performing evaluation on Spark."
        ], 
        "concept": "Perform evaluation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Check backprop gradients for a pretrain layer NOTE: gradient checking pretrain layers can be difficult...", 
            "See the guide: Testing > Gradient checking."
        ], 
        "concept": "Gradient checking", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Network outputs are for output layers only.", 
            "the network output doesn't have to be in any specific range.", 
            "Each network output is assumed to be a separate/independent binary class, with probability 0 to 1 independent of all other outputs."
        ], 
        "concept": "network outputs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Frozen layer freezes parameters of the layer it wraps, but allows the backpropagation to continue.", 
            "During the forward pass the frozen layer behaves as the layer within it would during test regardless of the training/test mode the network is in."
        ], 
        "concept": "Frozen layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "A common regression metric is Mean Absolute Error (MAE)."
        ], 
        "concept": "regression metric", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The binary select function is used to select an element from each window by applying it across each window, and it is called with the property that the first parameter's index vector is lexicographically less than the second parameter's index vector."
        ], 
        "concept": "select function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Target sparsity/dense level, when threshold step will happen."
        ], 
        "concept": "threshold steps", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "If they don't, and the first process does not terminate, the process termination will go unnoticed."
        ], 
        "concept": "process termination", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Recall, the final layer of a CNN model, which is often times an FC layer, has the same number of nodes as the number of output Imagenet, they all have output layers of size 1000, one node for each class."
        ], 
        "concept": "final layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Get the gradient tensors that this object is aware of."
        ], 
        "concept": "gradient tensor", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Hence, the inversion process can get ambiguous."
        ], 
        "concept": "inversion process", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "the entire layer graph is retrievable from that layer, recursively."
        ], 
        "concept": "entire layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "For batch of matrices, each individual matrix is raised to the power n."
        ], 
        "concept": "individual matrix", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Language modeling is key to many interesting problems such as speech recognition, machine translation, or image captioning.", 
            "My model is not a language model but it deals with many-to-many interaction.", 
            "Our language model might do OK on this sentence, but wouldn't it be much We have seen mathematician and physicist in the same role in a sentence."
        ], 
        "concept": "Language Modeling", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Automatic variable lifting makes it possible to compile these APIs without extra effort, at the cost of introducing a discrepancy between the semantics of executing Python functions and their corresponding compiled functions."
        ], 
        "concept": "Automatic variable", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The gradient calculation and application are delegated to an underlying optimizer."
        ], 
        "concept": "gradient calculation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default."
        ], 
        "concept": "one GPU", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The Gamma distribution is defined over positive real numbers using parameters concentration (aka \"alpha\") and rate (aka \"beta\").", 
            "Here's the code we'll use: Gamma Distribution."
        ], 
        "concept": "gamma distribution", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Fractional average pooling is similar to Fractional max pooling in the pooling region generation step."
        ], 
        "concept": "average pooling", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Numpy equivalent is tensor[mask].", 
            "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations.", 
            "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays.", 
            "# Numpy uses type 'Object' when the int overflows long, but we don't # have a similar concept.", 
            "Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients."
        ], 
        "concept": "numpy", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "See the guide: Graph Editor (contrib) > Module: reroute."
        ], 
        "concept": "Contrib", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The connection weights and biases are managed by the layer object."
        ], 
        "concept": "Connection Weight", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Now that the network configuration is set up and instantiated along with our MNIST test/train iterators, training takes just a few lines of code.", 
            "Note 1: The network configuration may be incomplete, but the inputs have been added to the layer already."
        ], 
        "concept": "Network Configuration", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The condition tensor must be a scalar if x and y are scalar.", 
            "The condition tensor acts as a mask that chooses, based on the value at each element, whether the corresponding element / row in the output should be taken from x (if true) or y (if false)."
        ], 
        "concept": "condition tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Consequently, the output activations size is equal to the input size.", 
            "In the CONCAT case, the output activations size (dimension 1) is 2x larger than the standard RNN's activations array.", 
            "In all cases except CONCAT, the output activations size is the same size as the standard RNN that is being wrapped by this layer.", 
            "- Per output masking: Where each output activation value is present or not - mask shape [n,c,h,w] (same as output)."
        ], 
        "concept": "output activations", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that ffmpeg is free to select the \"best\" audio track from an mp4."
        ], 
        "concept": "FFmpeg", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "For a three-training-worker distributed configuration, each training worker is likely to go through the whole epoch independently."
        ], 
        "concept": "training worker", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Multi-label classification handles the case where each example may have zero or more associated labels, from a discrete set."
        ], 
        "concept": "label classes", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "To ensure that training continues without diverging it is necessary that the restored node resumes training with a copy of the model identical to that on the other nodes at the current point."
        ], 
        "concept": "training continues", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Each queue element is a tuple of one or more tensors, where each tuple component has a static dtype, and may have a static shape."
        ], 
        "concept": "queue element", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "In practice though, performing feature crosses still adds significant value to the learning capability of your models."
        ], 
        "concept": "performing feature", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "The graph structure is like assembly code: inspecting it can convey some useful information, but it does not contain all of the useful context that source code conveys."
        ], 
        "concept": "graph structure", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "May have shape [B1, ..., Bb, r], b >= 0, and characterizes b-batches of r x r None, an identity matrix is used inside the perturbation."
        ], 
        "concept": "Identity matrix", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "As ROC metrics are only defined for binary classification, this treats the multi-class output as a set of 'one-vs-all' binary classification problems.", 
            "ROC has 2 modes of operation: (a) Thresholded (less memory) Thresholded Is an approximate method, that (for large datasets) may use significantly less memory than exact.."
        ], 
        "concept": "ROC", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "As argmax is being done here, labels and predictions type can be different."
        ], 
        "concept": "argmax", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Spatial transformer networks are a generalization of differentiable attention to any spatial transformation.", 
            "Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model."
        ], 
        "concept": "Spatial transformer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Otherwise, gradient updates are applied asynchronous.", 
            "Collect gradients updates to apply them with the last tower.", 
            "If the argument is supplied, gradient updates will be synchronous.", 
            "If left as None, gradient updates will be asynchronous."
        ], 
        "concept": "gradient updates", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Conditional reduction: apply the reduces on a specified column, where the reduction occurs *only* on those Beware, the output will be huge!"
        ], 
        "concept": "reduction occurs", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "This tradeoff is usually worth it, and (as long as the truncated BPTT lengths are set appropriately), truncated BPTT works well in practice.", 
            "Truncated backpropagation through time (BPTT) was developed in order to reduce the computational complexity of each parameter update in a recurrent neural network."
        ], 
        "concept": "bptt", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "higher rank input), computes the top k entries in each row (resp."
        ], 
        "concept": "Higher ranks", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "By default, computations involving variables that require gradients will keep history."
        ], 
        "concept": "computations involving", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Note that more complex networks and problems may never yield an optimal score."
        ], 
        "concept": "complex networks", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "WorkerID: A unique identifier for workers, within a session For example, single machine training (with 1 listener) would have 1 session ID, 1 type ID, 1 worker ID, and multiple timestamps."
        ], 
        "concept": "single machine", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert1"
    }, 
    {
        "explanation": [
            "Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block."
        ], 
        "concept": "Convolution kernel", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Init has been internally called."
        ], 
        "concept": "inits", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The GPUs have to wait for that CPU to finish.", 
            "GPU operations are called in an asynchronously way, so that they are not blocking your CPU.", 
            "In a workstation with multiple GPU cards, each GPU will have similar speed and contain enough memory to run an entire CIFAR-10 model.", 
            "Note, a GPU with CUDA is not critical for this tutorial as a CPU will not take much time.", 
            "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default.", 
            "Below are some other Check if a GPU is underutilized by running nvidia-smi -l 2.", 
            "When GPU RAM is less than CPU RAM, you need to monitor how much RAM is being used off-heap.", 
            "On the server there are 8 GPU installed and they are all the same type 1080Ti.", 
            "As GPUs and other hardware accelerators get faster, preprocessing of data can be a bottleneck.", 
            "If no GPUs are available, then the model is going to be placed on the CPU.", 
            "When each GPU completes the computation, gradients are going to be reduced (added) onto the master GPU.", 
            "By default, GPU operations are asynchronous.", 
            "If you do so, your GPU will run out of RAM when trying to run jobs.", 
            "GPUs are run in their default Boost.", 
            "In this design, each GPU on the server has its own copy of each variable.", 
            "GPUs and a single aggregated gradient is sent to the parameter server.", 
            "The GPUs are synchronized in operation.", 
            "While the process is running, the GPU has still 80% memory blocked and pytorch is using this space.", 
            "This setup requires that all GPUs share the model parameters.", 
            "Update model parameters synchronously by waiting for all GPUs to finish Here is a diagram of this model: Note that each GPU computes inference as well as the gradients for a unique batch of data.", 
            "If JavaCPP or your GPU throw an out-of-memory error (OOM), or even if your compute slows down due to GPU memory being limited, then you may want to either decrease batch size or increase the amount of off-heap memory that JavaCPP is allowed to allocate, if that's possible.", 
            "This utilty and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend.", 
            "Get device (GPU, etc) current bytes - may be null if no compute devices are present in the system.", 
            "While the GPU is busy your multiple workers can load a new batch of images instead of a large file.", 
            "After the GPU finishes it can just get the new batch, which is probably already waiting for it.", 
            "These are used at each layer for doing things like normalization and NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST).", 
            "For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won't be enough for modern deep learning.", 
            "If GPU utilization is not approaching 80-100%, then the input pipeline may be the bottleneck.", 
            "Note that values set here on the layer will be applied to all relevant layers - unless the value is overridden Note: values set by this method will be applied to all applicable layers in the network, unless a different Note: values set by this method will be applied to all applicable layers in the network, unless a different NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST)."
        ], 
        "concept": "GPU", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Vectorization is the first problem many data scientists will have to solve to start training their algorithms on data."
        ], 
        "concept": "Vectorization", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "By permitting a richer palette of numbers for every cell, an embedding column contains far fewer cells than an indicator column.", 
            "Indicator columns and embedding columns never work on features directly, but instead take categorical columns as input.", 
            "Instead of representing the data as a one-hot vector of many dimensions, an embedding column represents that data as a lower-dimensional, ordinary vector in which each cell can contain any number, not just 0 or 1."
        ], 
        "concept": "embedding columns", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The training process will then continue in this way until the model is fully trained.", 
            "This way you can use a trained model without having to retrain it, or pick-up training where you left of\u2014in case the training process was interrupted.", 
            "The training process itself can take several hours, so make sure you have a machine available for that long.", 
            "If True, performs intermediate model checkpoints and exports during the training process, rather than only once model training is complete.", 
            "If your training process uses workspaces, we recommend that you disable (or reduce the frequency of) periodic GC calls.", 
            "This is very convenient as my training process uses standard disposable cloud workers that should not store anything of a value on their local drives!"
        ], 
        "concept": "Training processes", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Training is the stage of machine learning when the model is gradually optimized, or the model learns the dataset.", 
            "The model learns to associate images and labels.", 
            "That is, the model learns the best way to map your input numeric categorical values to the embeddings vector value in order to solve your problem."
        ], 
        "concept": "model learning", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Remember that the input sentences were heavily filtered.", 
            "The input sentence is evaluated Computation Graph: Forward input through encoder model."
        ], 
        "concept": "input sentences", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "HDF5 is really sensitive about the order its resources are deallocated in."
        ], 
        "concept": "hdf5", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The sum operation operates over all the elements.", 
            "The sum operation still operates over all the elements, and divides by n.", 
            "This operator is similar to the unsorted segment sum operator found here."
        ], 
        "concept": "sum operation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "the entire layer graph is retrievable from that layer, recursively."
        ], 
        "concept": "entire layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Multilayer Network to tweak for transfer learning."
        ], 
        "concept": "Multilayer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Mean Absolute Error loss function where each the output is (optionally) weighted/scaled by a flags scalar value."
        ], 
        "concept": "Mean absolute error", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Frozen layer freezes parameters of the layer it wraps, but allows the backpropagation to continue.", 
            "During the forward pass the frozen layer behaves as the layer within it would during test regardless of the training/test mode the network is in."
        ], 
        "concept": "Frozen layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Network outputs are for output layers only.", 
            "the network output doesn't have to be in any specific range.", 
            "Each network output is assumed to be a separate/independent binary class, with probability 0 to 1 independent of all other outputs."
        ], 
        "concept": "network outputs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "CPU backend supports multiple CPU ISAs.", 
            "CPU info will not be logged.", 
            "CPU backend it might be ignored, depending on Aggregate.", 
            "If this cannot be loaded, the CPU (nd4j-native) backend will be loaded second.", 
            "While the accelerator is performing training step N, the CPU is preparing the data for step N+1.", 
            "On the other hand, higher values can increase contention if CPU is scarce.", 
            "This function may be used when CPU time is scarce and inputs are trusted or unimportant."
        ], 
        "concept": "CPU", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default."
        ], 
        "concept": "one GPU", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Check that the convolution mode is consistent with the padding specification."
        ], 
        "concept": "convolution mode", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "As ROC metrics are only defined for binary classification, this treats the multi-class output as a set of 'one-vs-all' binary classification problems.", 
            "ROC has 2 modes of operation: (a) Thresholded (less memory) Thresholded Is an approximate method, that (for large datasets) may use significantly less memory than exact.."
        ], 
        "concept": "ROC", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "As argmax is being done here, labels and predictions type can be different."
        ], 
        "concept": "argmax", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that ffmpeg is free to select the \"best\" audio track from an mp4."
        ], 
        "concept": "FFmpeg", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Each queue element is a tuple of one or more tensors, where each tuple component has a static dtype, and may have a static shape."
        ], 
        "concept": "queue element", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Now that the network configuration is set up and instantiated along with our MNIST test/train iterators, training takes just a few lines of code.", 
            "Note 1: The network configuration may be incomplete, but the inputs have been added to the layer already."
        ], 
        "concept": "Network Configuration", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "See the guide: Graph Editor (contrib) > Module: reroute."
        ], 
        "concept": "Contrib", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Numpy equivalent is tensor[mask].", 
            "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations.", 
            "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays.", 
            "# Numpy uses type 'Object' when the int overflows long, but we don't # have a similar concept.", 
            "Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients."
        ], 
        "concept": "numpy", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "For a three-training-worker distributed configuration, each training worker is likely to go through the whole epoch independently."
        ], 
        "concept": "training worker", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The condition tensor must be a scalar if x and y are scalar.", 
            "The condition tensor acts as a mask that chooses, based on the value at each element, whether the corresponding element / row in the output should be taken from x (if true) or y (if false)."
        ], 
        "concept": "condition tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Otherwise, gradient updates are applied asynchronous.", 
            "Collect gradients updates to apply them with the last tower.", 
            "If the argument is supplied, gradient updates will be synchronous.", 
            "If left as None, gradient updates will be asynchronous."
        ], 
        "concept": "gradient updates", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "WorkerID: A unique identifier for workers, within a session For example, single machine training (with 1 listener) would have 1 session ID, 1 type ID, 1 worker ID, and multiple timestamps."
        ], 
        "concept": "single machine", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Consequently, the output activations size is equal to the input size.", 
            "In the CONCAT case, the output activations size (dimension 1) is 2x larger than the standard RNN's activations array.", 
            "In all cases except CONCAT, the output activations size is the same size as the standard RNN that is being wrapped by this layer.", 
            "- Per output masking: Where each output activation value is present or not - mask shape [n,c,h,w] (same as output)."
        ], 
        "concept": "output activations", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "May have shape [B1, ..., Bb, r], b >= 0, and characterizes b-batches of r x r None, an identity matrix is used inside the perturbation."
        ], 
        "concept": "Identity matrix", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The graph structure is like assembly code: inspecting it can convey some useful information, but it does not contain all of the useful context that source code conveys."
        ], 
        "concept": "graph structure", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Spatial transformer networks are a generalization of differentiable attention to any spatial transformation.", 
            "Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model."
        ], 
        "concept": "Spatial transformer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Printing will switch to scientific notation on a per element basis - when abs value is greater than or equal to 10000 If the number of elements in the array is greater than 1000 (by default) only the first and last three elements in a dimension are included."
        ], 
        "concept": "abs value", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If not provided, the true class distribution is estimated live in a streaming fashion.", 
            "That is, soft classes are not allowed, and the labels vector must provide a single specific index for the true class for each row of logits (each minibatch entry)."
        ], 
        "concept": "True class", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Here's the relevant code: The units parameter defines the number of output neurons in a given layer.", 
            "Therefore, the full set of layers A logit output layer connected to the top hidden layer When defining an output layer, the units parameter specifies the number of outputs."
        ], 
        "concept": "units parameter", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This makes moving averages move faster.", 
            "The moving averages are computed using exponential decay."
        ], 
        "concept": "moving average", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If i dont use this snippet then the model works in multi-gpu.", 
            "By keeping the validation set separate, you can ensure that the model works with data it's never seen before.", 
            "Let's get rid of these two assumptions, so our model works with any 2d single channel image."
        ], 
        "concept": "model works", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "param labels Each triple in the list specifies the shape, array order and type of values for the labels arrays."
        ], 
        "concept": "labels arrays", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Our next order of business is to create a vocabulary and load query/response sentence pairs into memory."
        ], 
        "concept": "next order", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Blas buffer util for interopping with the underlying buffers and the given ndarrays."
        ], 
        "concept": "Blas", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Each column is plotted separately; only numerical and categorical columns are plotted.", 
            "Indicator columns and embedding columns never work on features directly, but instead take categorical columns as input."
        ], 
        "concept": "Categorical column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The resulting variable can be used for example for evaluating functions at all locations on a grid."
        ], 
        "concept": "resulting variable", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The reparameterized sample therefore becomes differentiable."
        ], 
        "concept": "reparameterized sample", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "So, you'll have to spend WAY more time building model + cooccurrence statistics will be shifted due to the absense of sentence boundaries."
        ], 
        "concept": "CoOccurrence", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that leaf parameters are parameters that do not have any nested parameter spaces."
        ], 
        "concept": "nested parameter", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The top element of the column vector combines with the top elements of each column in the matrix, and so forth.", 
            "The two top elements are then multiplied by each other, as are the bottom two, and the two products are added to consolidate in a single scalar."
        ], 
        "concept": "top elements", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Consequently, an array of all 1s (or, indeed any array of equal values) will result in the same performance as no cost array; non- Evaluation.", 
            "A cost array can be used to bias the multi class predictions towards or away from certain classes."
        ], 
        "concept": "cost array", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Tensor along dimension is a powerful technique, but can be a little hard to understand at first."
        ], 
        "concept": "tensor along", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "An error will be raised if the shape of a loop variable after an iteration is determined to be more general than or incompatible with its shape invariant."
        ], 
        "concept": "shape invariant", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Elementwise scalar multiplication looks like this: And produces this: Subtraction and division follow a similar pattern: If you perform all these operations on your initial 2 x 2 matrix, you should end up with this matrix: Elementwise vector operations."
        ], 
        "concept": "vector operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The features dictionary maps feature column names to Tensors containing the corresponding feature data, and labels is a Tensor containing the batch's label data.", 
            "Label data will be converted to a one-hot representation automatically."
        ], 
        "concept": "Label data", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "if a decimal output value is required."
        ], 
        "concept": "output value", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If input shapes have rank-R, then output shape will have rank-(R+1).", 
            "Note that for convolutional models, input shape information follows the NCHW convention."
        ], 
        "concept": "Input shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "By convention, dimensions are listed in increasing order of dimension number.", 
            "The lowest dimension number in dimensions is the slowest varying dimension (most major) in the loop nest which collapses these dimension, and the highest dimension number is fastest varying (most minor).", 
            "Example with contracting dimension numbers: Associated batch dimension numbers from the 'lhs' and 'rhs' must have the same dimension number, must be listed in the same order in both arrays, must have the same dimension sizes, and must be ordered before contracting and non-contracting/non-batch dimension numbers."
        ], 
        "concept": "dimension number", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Naively employing asynchronous updates of model parameters leads to sub-optimal training performance because an individual model replica might be trained on a stale copy of the model parameters."
        ], 
        "concept": "model replicas", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The network will be trained similarly to the network trained tutorial 15.", 
            "Training statistics may include things like per-epoch run times, These statistics are primarily used for debugging and optimization, in order to gain some insight into what aspects of network training are taking the most time."
        ], 
        "concept": "network trained", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The optimization algorithm is how updates are made, given the gradient."
        ], 
        "concept": "optimization algorithms", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "By comparing these the AUC is generated, with some discretization error."
        ], 
        "concept": "AUC", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "On the other hand, the validation loss will be identical whether we shuffle the validation set or not."
        ], 
        "concept": "VL", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This data should have the same distribution as the real-world data you want to make predictions about with your model."
        ], 
        "concept": "world data", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The feature indices represented as a dense tensor."
        ], 
        "concept": "feature indices", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Get the false positives count for the specified output."
        ], 
        "concept": "false positives", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model)."
        ], 
        "concept": "Predictive models", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If the repeat transformation is applied before the shuffle transformation, then the epoch boundaries are blurred."
        ], 
        "concept": "repeat transformation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This attention energies tensor is the same size as the encoder output, and the two are ultimately multiplied, resulting in a weighted tensor whose largest values represent the most important parts of the query sentence at a particular time-step of decoding."
        ], 
        "concept": "energies tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "In a multi-headed model, each head is represented by an entry in this dict.", 
            "Single-headed models only need to specify one entry in this dictionary."
        ], 
        "concept": "headed model", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Suppose tensor length is n, there are d devices and g gather shards."
        ], 
        "concept": "tensor length", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The true rank of an array is the number of dimensions which have a size greater than 1."
        ], 
        "concept": "true rank", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The inferred shape of a tensor is used to provide shape information without having to launch the graph in a session.", 
            "These inferred shapes might have known or unknown rank."
        ], 
        "concept": "inferred shape", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The matrix columns represent the prediction labels and the rows represent the real labels."
        ], 
        "concept": "matrix column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The resulting dimensions are: (batch, sequence, embedding)."
        ], 
        "concept": "resulting dimensions", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The final dimension contains the indices of top-k labels.", 
            "The final dimension contains the top k predicted class indices.", 
            "The final dimension contains the logit values for each class."
        ], 
        "concept": "final dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "However, if a function with data-dependent if statements and loops is traced, only the operations called along the execution route taken by the example input will be recorded."
        ], 
        "concept": "example inputs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If mat1 is a \\((n \\times m)\\) tensor, mat2 is a Note."
        ], 
        "concept": "mat1", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Logistic regression is one in which the dependant variable is categorical rather than continuous - meaning that it can predict only a limited number of classes or categories, like a switch you flip on or off.", 
            "We will train a logistic regression model that, given an individual's information, outputs a number between 0 and 1\u2014this can be interpreted as the probability that the individual has an annual income of over 50,000 dollars."
        ], 
        "concept": "logistic regression", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that an alias input may itself be an alias."
        ], 
        "concept": "alias input", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "An example sequence of events, leading to an outdated workspace pointer: Workspace W is opened (iteration 1)."
        ], 
        "concept": "example sequence", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The resulting behavior may be undefined."
        ], 
        "concept": "resulting behavior", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Torch Script currently does not support mutating tensors in place, so any tensor indexing can only appear on the right-hand size of an expression."
        ], 
        "concept": "Tensor index", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "TF Lite allows mobile developers to do inference efficiently on mobile devices."
        ], 
        "concept": "mobile devices", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Exploding gradients are problematic as they can 'mess up' the parameters of your network In the case of recurrent neural networks, adding some gradient normalization or gradient clipping may help."
        ], 
        "concept": "Exploding gradients", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If no initializer is provided, the output structure and dtypes of fn are assumed to be the same as its input; and in this case, the first argument of fn must match the structure of elems."
        ], 
        "concept": "output structure", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device."
        ], 
        "concept": "single device", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The Evaluation class is used for evaluation.", 
            "In DL4J the Evaluation Class and variants of the Evaluation Class are available to evaluate your model's performance.", 
            "An Evaluation class has more built-in methods if you need to extract a Under Curve (AUC)."
        ], 
        "concept": "Evaluation Class", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Determines whether all sampled classes in a batch are unique."
        ], 
        "concept": "sampled class", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A column reduction defines how a single column should be reduced."
        ], 
        "concept": "single column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "So, when multiple indices in updates refer to the same index in operand, the corresponding value in output will be non-deterministic.", 
            "If indices is rank 1 (a vector), then for each position i, out[indices[i], ...] = updates[i, ...] Note that if multiple indices refer to the same location, the output at those locations is undefined - different updates may occur in different orders.", 
            "If indices is rank 1 (a vector), then for each position i, out[indices[i], ...] -= updates[i, ...] Note that if multiple indices refer to the same location, the contributions from each is handled correctly."
        ], 
        "concept": "multiple indices", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "However, if the range of the data is limited, floating-point addition is close enough to being associative for most practical uses."
        ], 
        "concept": "point addition", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If the computation time takes longer than the copy and aggregation, the copy itself becomes essentially free."
        ], 
        "concept": "computation time", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This feature cross enables the model to train on pricing conditions related to each individual section, which is a much stronger signal than latitude and longitude alone."
        ], 
        "concept": "feature cross", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Output column name is the same as the input column name.", 
            "Suppose the possible String values were {\"a\",\"b\",\"c\",\"d\"} and the String column value to be converted contained the String \"a,c\", then the 4 output columns would have values [\"true\",\"false\",\"true\",\"false\"]."
        ], 
        "concept": "output column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The trained model can then be used as an application resource."
        ], 
        "concept": "trained models", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Gradients are pushed to them and the chief worker will wait until enough gradients are collected and then average them before applying to variables."
        ], 
        "concept": "chief worker", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Multiple shifts along multiple axes may be specified."
        ], 
        "concept": "multiple axes", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The classes Tensor must provide string labels, not integer class IDs."
        ], 
        "concept": "classes tensor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The sizes in these dimensions must match, but tensordot will deal with broadcasted dimensions."
        ], 
        "concept": "Tensordot", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The graph shows the average error is about \\$2,500 dollars."
        ], 
        "concept": "average error", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "In contrast, Real NVP can compute both forward and inverse computations in parallel."
        ], 
        "concept": "nvprof", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Premade Estimators are designed to get results out of the box."
        ], 
        "concept": "Premade", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The loss scale is not updated for the lifetime of the class.", 
            "Loss scale manager uses an exponential update strategy.", 
            "Loss scale managers with a different strategy should subclass this class."
        ], 
        "concept": "loss scale", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Converts JavaRDD labeled points to JavaRDD datasets."
        ], 
        "concept": "JavaRDD", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Matrix transpose operation: If input has shape [a,b] output has shape [b,a]."
        ], 
        "concept": "Matrix transpose", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted."
        ], 
        "concept": "sufficient statistic", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "stddev: The standard deviation of the Gaussian kernel to be approximated.", 
            "The standard deviation is, if you want, the tradeoff between exploration and exploitation: the smaller the std, the less exploration."
        ], 
        "concept": "standard deviations", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Run a k nearest neighbors search on a NEW data point.", 
            "To do so, you can select points in multiple After clicking on a point, its nearest neighbors are also selected."
        ], 
        "concept": "nearest neighbor", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The low padding is applied in the direction of lower indices while the high padding is applied in the direction of higher indices."
        ], 
        "concept": "low padding", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Measures the loss given an input tensor x and a labels tensor y containing values (1 or -1).", 
            "labels tensor are directly broadcasted to all the TPU cores since the partition dims is None."
        ], 
        "concept": "labels tensors", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The supported device names are \"/device:CPU:0\" (or \"/cpu:0\") for the CPU device, and \"/device:GPU:i\" (or \"/gpu:i\") for the ith GPU device."
        ], 
        "concept": "supported device", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The output elements will be resorted to preserve the sort order along increasing dimension number.", 
            "In the figure, the element of value 9 is selected by both of the top windows (blue and red) and the binary addition scatter function produces the output element of value 8 (2 + 6).", 
            "The parameters and output element type have to be a boolean type, an integral type or a floating point types, and the types have to be consistent."
        ], 
        "concept": "output element", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "My GPU memory isn't freed properly\u00b6.", 
            "the cifar10 training example with -opencl, the GPU memory usage grows at a rate of 200MB per second (looking at nvidia-smi) until the lack of memory crashes the program.", 
            "If JavaCPP or your GPU throw an out-of-memory error (OOM), or even if your compute slows down due to GPU memory being limited, then you may want to either decrease batch size or increase the amount of off-heap memory that JavaCPP is allowed to allocate, if that's possible.", 
            "I found that after I passed the output of A to B, the GPU memory increases 4 GB.", 
            "If your GPU memory isn't freed even after Python quits, it is very likely that some Python subprocesses are still alive.", 
            "These are used at each layer for doing things like normalization and NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST).", 
            "Note that values set here on the layer will be applied to all relevant layers - unless the value is overridden Note: values set by this method will be applied to all applicable layers in the network, unless a different Note: values set by this method will be applied to all applicable layers in the network, unless a different NONE: cache disabled (default value) DEVICE: GPU memory will be used (on CPU backends effect will be the same as for HOST)."
        ], 
        "concept": "gpu memory", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Elementwise scalar multiplication can be represented several ways.", 
            "Elementwise scalar multiplication looks like this: And produces this: Subtraction and division follow a similar pattern: If you perform all these operations on your initial 2 x 2 matrix, you should end up with this matrix: Elementwise vector operations."
        ], 
        "concept": "scalar multiplication", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Root value is left -- right is unused."
        ], 
        "concept": "Root value", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A Sequential module simply performs function composition."
        ], 
        "concept": "Sequential module", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "At least one of {shape, ndim} must be specified."
        ], 
        "concept": "NDim", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state)."
        ], 
        "concept": "initial input", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Also, the final activation becomes a Sigmoid, which squashes values into a range between 0 and 1."
        ], 
        "concept": "final activation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A single dimension may be -1, in which case it's inferred from the remaining dimensions and the number of elements in input."
        ], 
        "concept": "single dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A feature column can be either one of the raw inputs in the original features dict (a base feature column), or any new columns created using transformations defined over one or multiple base columns (a derived feature columns).", 
            "Therefore, the code to create the feature column is: Feature columns can be far more sophisticated than those we're showing here.", 
            "The third feature column also specifies a lambda the program will invoke to scale the raw data: # Define three numeric feature columns.", 
            "As long as all feature columns are unweighted sparse columns this computes the prediction of a linear model which stores all weights in a single variable.", 
            "An Estimator expects a vector of numeric inputs, and feature columns describe how the model should convert each feature.", 
            "Each feature column needs a different kind of operation during this conversion.", 
            "A feature column is an object describing how the model should use raw input data from the features dictionary.", 
            "A feature column is an abstract concept of any raw or derived variable that can be used to predict the target label.", 
            "Feature columns are very rich, Estimators can use, allowing easy experimentation.", 
            "Feature Columns, handle a variety of input data types without changes to the model.", 
            "Feature columns can have internal state, like layers, so they often need to be initialized."
        ], 
        "concept": "features columns", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This is because larger epsilons mean we take a larger step in the direction that will maximize the loss."
        ], 
        "concept": "larger step", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Multidimensional indices include a int64 index for each dimension."
        ], 
        "concept": "Multidimensional indices", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Shapes of fixed rank but variable size are allowed by setting any shape dimension to None.", 
            "All shape dimensions must be fully defined."
        ], 
        "concept": "shape dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "One main column can have several sub columns."
        ], 
        "concept": "main column", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "By default, the losses are averaged over each loss element in the batch."
        ], 
        "concept": "loss element", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The only difference is that the shortcut function versions create and run the layer in a single call."
        ], 
        "concept": "shortcut function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The lower triangular part of the matrix is defined as the elements on and below the diagonal."
        ], 
        "concept": "lower triangular", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The optional bid value for this cluster's spot instances Uses the on-demand market if empty."
        ], 
        "concept": "bid value", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that the resulting sequence may be of length 0, if the input sequence is less than or equal to N.", 
            "In addition, once an input sequence element is attended to at a given output timestep, elements occurring before it cannot be attended to at subsequent output timesteps.", 
            "Monotonic attention implies that the input sequence is processed in an explicitly left-to-right manner when generating the output sequence.", 
            "It is recommended to use truncated BPTT when your input sequences are long (typically, more than a few hundred time steps)."
        ], 
        "concept": "Input Sequences", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "An unknown Dimension is compatible with all other Dimensions."
        ], 
        "concept": "unknown dimension", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The resulting perturbed image, \\(x'\\), is then misclassified by the target network as a \"gibbon\" when it is still clearly a \"panda\"."
        ], 
        "concept": "perturbed image", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Model Requirements: - Model must be a sequential model or functional model."
        ], 
        "concept": "functional model", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If None (the default), the most recent checkpoint found within the model directory is chosen."
        ], 
        "concept": "recent checkpoint", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Yet Not a single tutorials works as expected : 1."
        ], 
        "concept": "single tutorials", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "When training started, the model did not know how to spell an English word, or that words were even a unit of text."
        ], 
        "concept": "English word", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "How ccache is used to speed up build times."
        ], 
        "concept": "ccache", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A matrix that ordered its elements by row would look like this: Elementwise scalar operations."
        ], 
        "concept": "scalar operation", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Language modeling is key to many interesting problems such as speech recognition, machine translation, or image captioning.", 
            "My model is not a language model but it deals with many-to-many interaction.", 
            "Our language model might do OK on this sentence, but wouldn't it be much We have seen mathematician and physicist in the same role in a sentence."
        ], 
        "concept": "Language Modeling", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "For batch of matrices, each individual matrix is raised to the power n."
        ], 
        "concept": "individual matrix", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Get the gradient tensors that this object is aware of."
        ], 
        "concept": "gradient tensor", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "As mentioned, the model under attack is the same MNIST model from pytorch/examples/mnist."
        ], 
        "concept": "MNIST model", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "expected inputs are 3-D, 4-D or 5-D in shape.", 
            "Expected inputs are spatial (4 dimensional)."
        ], 
        "concept": "Expected inputs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Hence, the inversion process can get ambiguous."
        ], 
        "concept": "inversion process", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Recall, the final layer of a CNN model, which is often times an FC layer, has the same number of nodes as the number of output Imagenet, they all have output layers of size 1000, one node for each class."
        ], 
        "concept": "final layer", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Fractional average pooling is similar to Fractional max pooling in the pooling region generation step."
        ], 
        "concept": "average pooling", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The Gamma distribution is defined over positive real numbers using parameters concentration (aka \"alpha\") and rate (aka \"beta\").", 
            "Here's the code we'll use: Gamma Distribution."
        ], 
        "concept": "gamma distribution", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Automatic variable lifting makes it possible to compile these APIs without extra effort, at the cost of introducing a discrepancy between the semantics of executing Python functions and their corresponding compiled functions."
        ], 
        "concept": "Automatic variable", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "If they don't, and the first process does not terminate, the process termination will go unnoticed."
        ], 
        "concept": "process termination", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Target sparsity/dense level, when threshold step will happen."
        ], 
        "concept": "threshold steps", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "A common regression metric is Mean Absolute Error (MAE)."
        ], 
        "concept": "regression metric", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The binary select function is used to select an element from each window by applying it across each window, and it is called with the property that the first parameter's index vector is lexicographically less than the second parameter's index vector."
        ], 
        "concept": "select function", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Singleton evaluation hrunner class for performing evaluation on Spark."
        ], 
        "concept": "Perform evaluation", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Check backprop gradients for a pretrain layer NOTE: gradient checking pretrain layers can be difficult...", 
            "See the guide: Testing > Gradient checking."
        ], 
        "concept": "Gradient checking", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Note that more complex networks and problems may never yield an optimal score."
        ], 
        "concept": "complex networks", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "By default, computations involving variables that require gradients will keep history."
        ], 
        "concept": "computations involving", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "higher rank input), computes the top k entries in each row (resp."
        ], 
        "concept": "Higher ranks", 
        "score": -1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "This tradeoff is usually worth it, and (as long as the truncated BPTT lengths are set appropriately), truncated BPTT works well in practice.", 
            "Truncated backpropagation through time (BPTT) was developed in order to reduce the computational complexity of each parameter update in a recurrent neural network."
        ], 
        "concept": "bptt", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Conditional reduction: apply the reduces on a specified column, where the reduction occurs *only* on those Beware, the output will be huge!"
        ], 
        "concept": "reduction occurs", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "In practice though, performing feature crosses still adds significant value to the learning capability of your models."
        ], 
        "concept": "performing feature", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "To ensure that training continues without diverging it is necessary that the restored node resumes training with a copy of the model identical to that on the other nodes at the current point."
        ], 
        "concept": "training continues", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "Multi-label classification handles the case where each example may have zero or more associated labels, from a discrete set."
        ], 
        "concept": "label classes", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }, 
    {
        "explanation": [
            "The connection weights and biases are managed by the layer object."
        ], 
        "concept": "Connection Weight", 
        "score": 1, 
        "domain": "deeplearning", 
        "user": "expert2"
    }
]