[
    {
        "concept": "ZNode",
        "explanation": [
            "[root znode] is the path of the zookeeper znode, under which the edit log information will be stored.",
            "The znodes that were recovered are renamed with the ID of the slave cluster appended with the name of the dead server.",
            "Index at which the RM Delegation Token ids will be split so that the delegation token znodes stored in the zookeeper RM state store will be stored as two different znodes (parent-child).",
            "It contains one znode per peer cluster (if 5 slave clusters, 5 znodes are created), and each of these contain a queue of WALs to process."
        ],
        "useful": true
    },
    {
        "concept": "Findbugs",
        "explanation": [
            "Findbugs is used to detect common bugs pattern."
        ],
        "useful": true
    },
    {
        "concept": "Service Instance",
        "explanation": [
            "A service instance is running if the instances the components which for the service are running.",
            "Core Hadoop service instances must be able to register their service endpoints.",
            "The registry will allow a service instance can only be registered under the path where it has permissions.",
            "Since coprocessor Service instances are associated with individual regions within the table, the client RPC calls must ultimately identify which regions should be used in the Service method invocations.",
            "These service instances need to be discovered by clients; traditionally their IP added is registered in DNS or in some configuration file \u2014but that is not feasible in YARN-deployed applications when neither the hostname nor network ports can be known in advance.",
            "Each Hadoop Service instance must be configured with its Kerberos principal and keytab file location.",
            "The service instance could not be created: 57."
        ],
        "useful": true
    },
    {
        "concept": "Heap usage",
        "explanation": [
            "Adds cumulative cpu usage and total heap usage to task counters.",
            "As a result, the actual heap usage of the memstore before being flushed may increase by up to 100%.",
            "Heap usage emulator is designed in such a way that it only emulates at specific progress boundaries of the task."
        ],
        "useful": true
    },
    {
        "concept": "Execution Types",
        "explanation": [
            "If there are available resources, the execution of the container starts immediately, irrespective of its execution type."
        ],
        "useful": true
    },
    {
        "concept": "access token",
        "explanation": [
            "Obtain the access token that should be added to https connection's header."
        ],
        "useful": true
    },
    {
        "concept": "fsck",
        "explanation": [
            "Fsck now checks permissions as directories are traversed.",
            "fsck can be run on the whole file system or on a subset of files.",
            "By default fsck ignores open files but provides an option to select all files during reporting.",
            "The output of hdfs fsck now also contains information about decommissioning replicas.",
            "hdfs fsck <path> // only show decommission state hdfs fsck <path> -maintenance // include maintenance state \u00a9 2018 Apache Software Foundation - Privacy Policy."
        ],
        "useful": true
    },
    {
        "concept": "Partition cluster",
        "explanation": [
            "Partition cluster - each node can be assigned one label, so the cluster will be divided to several smaller disjoint partitions."
        ],
        "useful": true
    },
    {
        "concept": "ACID",
        "explanation": [
            "Utility method for ACID to normalize logging info."
        ],
        "useful": true
    },
    {
        "concept": "row group",
        "explanation": [
            "In a row group (rows having the same key), rows are also sorted by their tags."
        ],
        "useful": true
    },
    {
        "concept": "node cluster",
        "explanation": [
            "Here is an example basic configuration for a distributed ten node cluster: * The nodes are named example0, example1, etc., through node example9 in this example."
        ],
        "useful": true
    },
    {
        "concept": "output paths",
        "explanation": [
            "Thus the output of the job is: < Bye, 1> < Goodbye, 1> < Hadoop, 2> < Hello, 2> The main method specifies various facets of the job, such as the input/output paths (passed via the command line), key/value types, input/output formats etc., in the Job."
        ],
        "useful": true
    },
    {
        "concept": "referenced files",
        "explanation": [
            "Those reference files will point to the parent region's files.",
            "Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further.",
            "The reference file is used just like a regular data file, but only half of the records are considered."
        ],
        "useful": true
    },
    {
        "concept": "Cluster Nodes",
        "explanation": [
            "If your cluster nodes use OS X, see the section, SSH: Setting up Remote Desktop and Enabling Self-Login on the Hadoop wiki."
        ],
        "useful": true
    },
    {
        "concept": "metainfo",
        "explanation": [
            "However, every time your cluster starts, META is scanned to ensure that it does not need to be converted."
        ],
        "useful": true
    },
    {
        "concept": "info key",
        "explanation": [
            "The info key is a string but value can be any object."
        ],
        "useful": true
    },
    {
        "concept": "deprecated name",
        "explanation": [
            "If name is deprecated or there is a deprecated name associated to it, it sets the value to both names."
        ],
        "useful": true
    },
    {
        "concept": "YARN",
        "explanation": [
            "In order to scale YARN beyond few thousands nodes, YARN supports the notion of Federation via the YARN Federation feature.",
            "YARN supports a rudimentary registry which allows YARN Application Masters to register a web URL and an IPC address.",
            "YARN Nodes could be decommissioned NORMAL or GRACEFUL.",
            "YARN Applications and containers publishing their management and metrics bindings.",
            "OSS YARN has been known to scale up to about few thousand nodes.",
            "YARN is known to scale to thousands of nodes.",
            "YARN remains responsible for the management and allocation of resources.",
            "Hadoop YARN allows applications to run on the Hadoop cluster.",
            "In a typical cluster HDFS and YARN services will be launched as the system hdfs and yarn users respectively.",
            "YARN Containers registering their public service endpoints.",
            "YARN will create directories with appropriate permissions for users where YARN deployed services can be registered by a user.",
            "YARN containers in a secure cluster use the operating system facilities to offer execution isolation for containers.",
            "YARN deployed services instances must be able register their bindings and be discovered by clients.",
            "YARN will deploy them across the cluster depending on the individual each component requirements and server availability.",
            "YARN has an option parsing framework that employs parsing generic options as well as running classes.",
            "For a completely functional NM restart, YARN relies on any auxiliary service configured to also support recovery.",
            "YARN services SHOULD be registered using the following convention: /users/{username}/{serviceclass}/{instancename}.",
            "YARN applications should propagate the user name of the user launching an application by setting this environment variable.",
            "YARN commands are invoked by the bin/yarn script.",
            "YARN distributed shell: in hadoop-yarn-applications-distributedshell project after you set up your development environment.",
            "Hence other than the five defined enums, YARN can consider other integers also.",
            "YARN applications that attempt to use new APIs (including new fields in data structures) that have not yet been deployed to the cluster can expect link exceptions.",
            "Use case: In a secure cluster, YARN requires the user hdfs delegation-tokens to do localization and log-aggregation on behalf of the user.",
            "If the resource is a tgz, zip, or jar - you can have YARN unzip it.",
            "DEFAULT : While submitting a job, if the user is not specifying priority, YARN has the capability to pick the default priority as per its config."
        ],
        "useful": true
    },
    {
        "concept": "blocks size",
        "explanation": [
            "The size of those indexes is a factor of the block size (64KB by default), the size of your keys and the amount of data you are storing.",
            "All blocks in a file except the last block are the same size, while users can start a new block without filling out the last block to the configured block size after the support for variable length block was added to append and hsync.",
            "Larger block size is preferred if files are primarily for sequential access.",
            "The block size and replication factor are configurable per file.",
            "The default blocks size prior to this change was 64MB.",
            "-p[rbugp] Preserve status r: replication number b: block size u: user g: group p: permission -p alone is equivalent to -prbugp."
        ],
        "useful": true
    },
    {
        "concept": "checksum type",
        "explanation": [
            "The checksum type is automatically inferred by issuing a read of the first byte of each block."
        ],
        "useful": true
    },
    {
        "concept": "colname",
        "explanation": [
            "colname indicates the column on which to sample each row in the table."
        ],
        "useful": true
    },
    {
        "concept": "service state",
        "explanation": [
            "The service state is checked before the operation begins."
        ],
        "useful": true
    },
    {
        "concept": "read error",
        "explanation": [
            "If the configuration is false, read errors are treated similar to connection errors."
        ],
        "useful": true
    },
    {
        "concept": "NULL",
        "explanation": [
            "The NULL in the union should be silently removed.",
            "NOT NULL is not inferred on UNIQUE and needs to be explicitly declared."
        ],
        "useful": true
    },
    {
        "concept": "reinitialization",
        "explanation": [
            "Once the reinitialization has been committed, It cannot be rolled back."
        ],
        "useful": true
    },
    {
        "concept": "table descriptor",
        "explanation": [
            "Check that the table descriptor for the snapshot is a valid table descriptor.",
            "Construct a table descriptor by cloning the descriptor passed as a parameter."
        ],
        "useful": true
    },
    {
        "concept": "side files",
        "explanation": [
            "When file permissions are thwarted, unlike \"side files\", there are no standard tools that can expose the protected credentials - even with the password known."
        ],
        "useful": true
    },
    {
        "concept": "Namit",
        "explanation": [
            "Permalink Delete comments Namit Jain Depends on what the use case is.",
            "Namit offered to host the next contrib meeting at Facebook.",
            "I think Namit meant this in reference to the design option of using a single database and using scripts for quota management.",
            "Carl raised concerns about potential dependency creep preventing future Hive refactoring, and Namit Jain proposed reworking the approach to restrict it to pre/post-analysis hooks (limiting the dependencies exposed) rather than full-blown analyzer pluggability+subclassing."
        ],
        "useful": true
    },
    {
        "concept": "part files",
        "explanation": [
            "All output part files are created regardless of whether the corresponding task has output."
        ],
        "useful": true
    },
    {
        "concept": "Specified key",
        "explanation": [
            "Check if the specified key is contained in the bloom filter.",
            "Determines wether a specified key belongs to this filter.",
            "Invariant: nothing happens if the specified key does not belong to this counter Bloom filter."
        ],
        "useful": true
    },
    {
        "concept": "YARN service",
        "explanation": [
            "The YARN service registry is built on top of Apache Zookeeper.",
            "Any YARN service intended to run for an extended period of time must have a strategy for renewing credentials."
        ],
        "useful": true
    },
    {
        "concept": "protobuf",
        "explanation": [
            "Convert a protobuf token into a rpc token and set its service."
        ],
        "useful": true
    },
    {
        "concept": "ALTER",
        "explanation": [
            "ALTER TABLE ADD|REPLACE COLUMNS with CASCADE command changes the columns of a table's metadata, and cascades the same change to all the partition metadata.",
            "However, ALTER INDEX requires an index name that was created with lowercase letters (see HIVE-2752)."
        ],
        "useful": true
    },
    {
        "concept": "SMB",
        "explanation": [
            "SMB joins are used wherever the tables are sorted and bucketed."
        ],
        "useful": true
    },
    {
        "concept": "datafile",
        "explanation": [
            "All files in the filesystem are migrated by re-writing the block metadata - no datafiles are touched."
        ],
        "useful": true
    },
    {
        "concept": "JDO",
        "explanation": [
            "Java Data Objects (JDO) is a standard way to access persistent data in databases, using plain old Java objects (POJO) to represent persistent data."
        ],
        "useful": true
    },
    {
        "concept": "batch key",
        "explanation": [
            "Generate optimized results when entire batch key is repeated and it matched the hash map."
        ],
        "useful": true
    },
    {
        "concept": "Snapshottable Directory",
        "explanation": [
            "If there are snapshots in a snapshottable directory, the directory can be neither deleted nor renamed before all the snapshots are deleted.",
            "A snapshottable directory is able to accommodate 65,536 simultaneous snapshots."
        ],
        "useful": true
    },
    {
        "concept": "step number",
        "explanation": [
            "Each step is labeled with its step number."
        ],
        "useful": true
    },
    {
        "concept": "Scan filter",
        "explanation": [
            "This makes this filter unsuitable as a Scan filter."
        ],
        "useful": true
    },
    {
        "concept": "start time",
        "explanation": [
            "Suppose time T is the rolling upgrade start time and the upgrade is terminated by downgrade."
        ],
        "useful": true
    },
    {
        "concept": "Remote directory",
        "explanation": [
            "The remote directory for the jobs is remote/a and the local directory for storing output is local/output."
        ],
        "useful": true
    },
    {
        "concept": "Registry service",
        "explanation": [
            "Registry service properties: The registry must be highly available."
        ],
        "useful": true
    },
    {
        "concept": "flow name",
        "explanation": [
            "If the flow context is not specified, defaults are supplied for these attributes: Flow name: the YARN application name (or the application id if the name is not set).",
            "user, flow name and run id are not mandatory but if specified in query param can preclude the need for an additional operation to fetch flow context information based on cluster and app id."
        ],
        "useful": true
    },
    {
        "concept": "group mapping",
        "explanation": [
            "The groups of a user is determined by a group mapping service provider."
        ],
        "useful": true
    },
    {
        "concept": "Backticked names",
        "explanation": [
            "Backticked names are interpreted as regular expressions."
        ],
        "useful": true
    },
    {
        "concept": "artifact directory",
        "explanation": [
            "On certain Jenkins machines, the artifact directory sometimes gets deleted from outside the test-patch script."
        ],
        "useful": true
    },
    {
        "concept": "whole table",
        "explanation": [
            "Each compaction task handles 1 partition (or whole table if the table is unpartitioned).",
            "for a query of type: select expr from T where limit 100; Most probably, the whole table T need not be scanned."
        ],
        "useful": true
    },
    {
        "concept": "batch mutation",
        "explanation": [
            "This will be called for every batch mutation operation happening at the server."
        ],
        "useful": true
    },
    {
        "concept": "dependent table",
        "explanation": [
            "The dependent table does not have a location."
        ],
        "useful": true
    },
    {
        "concept": "resource value",
        "explanation": [
            "This method assumes resource value is positive."
        ],
        "useful": false
    },
    {
        "concept": "root location",
        "explanation": [
            "By default, this means the root location is stored at /hbase/root-region-server."
        ],
        "useful": true
    },
    {
        "concept": "YARN application",
        "explanation": [
            "A YARN application is deployed.",
            "An YARN application may be passed the token to connect with the ZK service when launched.",
            "YARN supports a rudimentary registry which allows YARN Application Masters to register a web URL and an IPC address.",
            "To execute code in the cluster, a YARN application must: A list of files in the cluster's filesystem to be \"localized\"."
        ],
        "useful": true
    },
    {
        "concept": "specified procedure",
        "explanation": [
            "The specified procedure was executed, and the new state should be written to the store."
        ],
        "useful": true
    },
    {
        "concept": "column family",
        "explanation": [
            "If the column family is omitted, all MOB-enabled column families are compacted.",
            "If only one column family is busy with writes, only that column family accomulates memory.",
            "The column family prefix must be composed of printable characters.",
            "Because tunings and storage specifications are done at the column family level, it is advised that all column family members have the same general access pattern and size characteristics.",
            "When a column family Store is created, it says what memstore type is in effect.",
            "Physically, all column family members are stored together on the filesystem.",
            "same row, column family, qualifier and timestamp \u2014 regardless of which arrived first.",
            "The column family must already exist in your table // schema.",
            "The column family names are stored for every value (ignoring prefix encoding).",
            "When many column families exist the flushing and compaction interaction can make for a bunch of needless i/o (To be addressed by changing flushing and compaction to work on a per column family basis).",
            "If multiple column families are involved, the columns may be spread across them.",
            "Serialize column family to block size map to configuration.",
            "Serialize column family to data block encoding map to configuration.",
            "To enable this setup, alter your table and for each column family set BLOCKCACHE \u21d2 'false'.",
            "Column families must be declared up front at schema definition time whereas columns do not need to be defined at schema time but can be conjured on the fly while the table is up and running.",
            "Each column family has a set of storage properties, such as whether its values should be cached in memory, how its data is compressed or its row keys are encoded, and others.",
            "To check which column families are using incompatible data block encoding you can use Pre-Upgrade Validator.",
            "Per Column Family keys facilitate low impact incremental key rotation and reduce the scope of any external leak of key material.",
            "Column Family time ranges take precedence over the global time range."
        ],
        "useful": true
    },
    {
        "concept": "scheduler state",
        "explanation": [
            "Fair Scheduler state dumps can potentially generate large amount of log data."
        ],
        "useful": true
    },
    {
        "concept": "Minimum period",
        "explanation": [
            "Minimum period of time (in milliseconds) to keep metadata (may only be applied when a prune command is manually run)."
        ],
        "useful": true
    },
    {
        "concept": "bucket number",
        "explanation": [
            "This is similar to hash bucketing currently, where the bucket number determines the file number."
        ],
        "useful": true
    },
    {
        "concept": "table named",
        "explanation": [
            "The table name of the table we are putting data into .",
            "Notice that table names, rows, columns all must be enclosed in quote characters.",
            "If this option is not provided, the original table name is used.",
            "* * rowID : a string identification the statistic to be gathered, possibly the table name + the partition specs.",
            "No regular-expression or wildcard support is present; all table names must be explicitly listed."
        ],
        "useful": true
    },
    {
        "concept": "s3a",
        "explanation": [
            "S3A now supports use of AWS Security Token Service temporary credentials for authentication to S3.",
            "S3A now supports configuration of multiple credential provider classes for authenticating to S3.",
            "S3A does not really enforce any authorization checks on these stub permissions.",
            "S3A then performs optimizations tailored to that access pattern.",
            "S3A is now the recommended client for working with S3 objects.",
            "S3A supports configuration via the standard AWS environment variables.",
            "Currently, S3A only supports S3's Server Side Encryption for at rest data encryption.",
            "S3A now supports read access to a public S3 bucket even if the client does not configure any AWS credentials.",
            "S3A has added support for configurable input policies.",
            "S3A can connect to different regions \u2014the tests support this.",
            "S3A now includes the current Hadoop version in the User-Agent string passed through the AWS SDK to the S3 service.",
            "If a region is not configured, S3A will assume that it is in the same region as the S3 bucket.",
            "This is the default: S3A does not need to be configured."
        ],
        "useful": true
    },
    {
        "concept": "auth",
        "explanation": [
            "Hadoop Auth is a Java library consisting of a client and a server components to enable Kerberos SPNEGO authentication for HTTP.",
            "here so the connection header is encoded if auth enabled.",
            "Hadoop Auth enforces authentication on protected resources, once authentiation has been established it sets a signed HTTP Cookie that contains an authentication token with the user name, user principal, authentication type and expiration time.",
            "Then the auth will pass for the notification related calls from those hosts.",
            "Hadoop Auth also supports additional authentication mechanisms on the client and the server side via 2 simple interfaces.",
            "If the auths contain secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within."
        ],
        "useful": true
    },
    {
        "concept": "permission values",
        "explanation": [
            "The exact permission values in the new child's access ACL are subject to filtering by the mode parameter."
        ],
        "useful": true
    },
    {
        "concept": "serialized value",
        "explanation": [
            "All serialized values are 9 bytes in length.",
            "All serialized values are 5 bytes in length."
        ],
        "useful": true
    },
    {
        "concept": "leveldb",
        "explanation": [
            "Default leveldb read cache size if no configuration is specified."
        ],
        "useful": true
    },
    {
        "concept": "output results",
        "explanation": [
            "At the same time, some output results need to be formed in the overflow batch."
        ],
        "useful": true
    },
    {
        "concept": "tasktracker",
        "explanation": [
            "Get the tasktracker expiry interval for the cluster.",
            "of task failures exceeds this, the tasktracker is blacklisted for this job."
        ],
        "useful": true
    },
    {
        "concept": "Heartbeat messages",
        "explanation": [
            "Heartbeat checks occur during the processing of scans to determine whether or not the server should stop scanning in order to send back a heartbeat message to the client.",
            "Heartbeat messages are used to keep the client-server connection alive during long running scans."
        ],
        "useful": true
    },
    {
        "concept": "Storage Type",
        "explanation": [
            "Storage type quota on DISK are of limited use except when DISK is not the dominant storage medium.",
            "Fallback storage types are used if the preferred storage types are not available.",
            "The default storage type will be DISK if the directory does not have a storage type tagged explicitly.",
            "Storage type quota can be configured even though the specific storage type is unavailable (or available but not configured properly with storage type information).",
            "However, overall space quota is recommended in this case as the storage type information is either unavailable or inaccurate for storage type quota enforcement.",
            "To set storage type quota on a directory, storage policies must be configured on the directory in order to allow files to be stored in different storage types according to the storage policy.",
            "Remove storage type quota specified for each directory.",
            "If specific storage types are given after -t option, only quota and remaining quota of the types specified will be displayed.",
            "These storage types are used sequentially for successive block replicas.",
            "The storage type quota is a hard limit on the usage of specific storage type (SSD, DISK, ARCHIVE) by files in the tree rooted at the directory."
        ],
        "useful": true
    },
    {
        "concept": "Inodes",
        "explanation": [
            "Inodes record the file type (regular file or directory) and the list of blocks.",
            "For a single file /dir1/file1 which takes two blocks of storage, the file structure in S3 would be something like this: / /dir1 /dir1/file1 block-6415776850131549260 block-3026438247347758425 Inodes start with a leading /, while blocks are prefixed with block-."
        ],
        "useful": true
    },
    {
        "concept": "PMC",
        "explanation": [
            "The PMC votes to make a contributor a committer based on an assessment of their contributions to the project.",
            "When the current chair of the PMC resigns, the PMC votes to recommend a new chair using lazy consensus, but the decision must be ratified by the Apache board.",
            "While the initiative is active the PMC may grant commit rights on the branch to its consistent contributors.",
            "When the chair is rotated or if the current chair of the PMC resigns, the PMC votes to recommend a new chair using Single Transferable Vote (STV) voting.",
            "Where necessary, PMC voting may take place on the private Hive PMC mailing list.",
            "PMC members, please read this WIP doc on policy voting for a release candidate, Release Policy.",
            "Contributors often ask Hive PMC members the question, \"What do I need to do in order to become a committer?\" The simple (though frustrating) answer to this question is, \"If you want to become a committer, behave like a committer.\" If you follow this advice, then rest assured that the PMC will notice, and committership will seek you out rather than the other way around."
        ],
        "useful": true
    },
    {
        "concept": "filter name",
        "explanation": [
            "The filter name must be a single word."
        ],
        "useful": true
    },
    {
        "concept": "accumulo",
        "explanation": [
            "Accumulo doc on how to contribute and develop is also good read to understand development workflow."
        ],
        "useful": true
    },
    {
        "concept": "udf",
        "explanation": [
            "This should be done before the UDF is initialized.",
            "Certain optimizations should not be applied if UDF is not deterministic.",
            "Generic UDF params utility class.",
            "String UDFs can be created instead, and the varchar values will be converted to strings and passed to the UDF.",
            "Generic UDF to generate Bloom Filter.",
            "By extending from this class these UDFs will automatically support decimals as well.",
            "Only Hive code and blessed UDFs are accepted in LLAP.",
            "The UDFs are schema agnostic - no XML validation is performed.",
            "If the UDF is not deterministic, or if it is stateful, it is necessary to annotate it as such for correctness.",
            "This UDF has no way of detecting failures or rolling back a transaction.",
            "UDF to determine whether to enforce restriction of information schema.",
            "The example provide was using the RLIKE operator but LIKE or a generic boolean UDF could be used as well.",
            "UDF to determine the current authorizer (class name of the authorizer) This is intended for internal usage only.",
            "This UDF is useful for exporting small to medium summaries that have a unique key.",
            "An exception may be thrown if the UDF doesn't know what to do with this data.",
            "If set to empty, then treated as wildcard character \u2013 all UDFs will be allowed.",
            "This UDF accepts arbitrary number of String arguments, so we use String[]."
        ],
        "useful": true
    },
    {
        "concept": "JobTracker",
        "explanation": [
            "Jobtracker was modified to cleanup reservations created on tasktracker nodes to support high RAM jobs, when the nodes are blacklisted."
        ],
        "useful": true
    },
    {
        "concept": "column vector",
        "explanation": [
            "Bytes column vectors allow a by reference entry to bytes."
        ],
        "useful": true
    },
    {
        "concept": "Queue name",
        "explanation": [
            "In the examples below, <queue> is the queue name.",
            "Name of the queue to which jobs will be submitted, if no queue name is mentioned."
        ],
        "useful": true
    },
    {
        "concept": "show table",
        "explanation": [
            "SHOW TABLE EXTENDED will list information for all tables matching the given regular expression."
        ],
        "useful": true
    },
    {
        "concept": "old format",
        "explanation": [
            "If the value is not specified, old format fsimage will not be saved in checkpoint."
        ],
        "useful": true
    },
    {
        "concept": "jobid",
        "explanation": [
            "JobID represents the immutable and unique identifier for the job.",
            "First part represents the jobtracker identifier, so that jobID to jobtracker map is defined.",
            "JobID consists of two parts.",
            "JOBID is set by the framework."
        ],
        "useful": true
    },
    {
        "concept": "jvms",
        "explanation": [
            "The usual cause is that their JVM is untuned and they are running into long GC pauses.",
            "More data is being generated than in the JVM than it can upload to S3 \u2014and so much data has been buffered that the JVM has run out of memory.",
            "Symptoms of this problem include JVM crashes with a stack trace inside these functions.",
            "Amount of time the JVM spent in garbage collection while executing tasks.",
            "A JVM without the Java Cryptography Extensions installed does not support such a key length.",
            "JVM metrics published to Ganglia now include the process name as part of the gmetric name.",
            "Make sure you don't swap, the JVM never behaves well under swapping.",
            "If the JVM does not support this length, the command will fail."
        ],
        "useful": true
    },
    {
        "concept": "memory estimation",
        "explanation": [
            "However, memory estimation could be tricky especially considering interned string."
        ],
        "useful": true
    },
    {
        "concept": "Batch size",
        "explanation": [
            "The batch size and decaying factor are provided with the constructor.",
            "The rule of thumb is that, the smaller the number of cells out of sync (lower probability of finding a diff), larger batch size values can be determined.",
            "During hive streaming connection creation, transaction batch size can be specified via builder API."
        ],
        "useful": true
    },
    {
        "concept": "bucket name",
        "explanation": [
            "The same bucket name can be used for all tests."
        ],
        "useful": true
    },
    {
        "concept": "Table Properties",
        "explanation": [
            "All the table properties/parameters will be that of table that was used in EXPORT to generate the archive.",
            "The advantage of this over external tables is that all the table properties (bucketing/sorting/list bucketing) for the underlying tables are used."
        ],
        "useful": true
    },
    {
        "concept": "STRING column",
        "explanation": [
            "The standard output of the user script will be treated as TAB-separated STRING columns, any cell containing only \\N will be re-interpreted as a NULL, and then the resulting STRING column will be cast to the data type specified in the table declaration in the usual way.",
            "If your STRING column contains tabs, an identity transformer will not give you back what you started with!"
        ],
        "useful": true
    },
    {
        "concept": "table state",
        "explanation": [
            "Set table state to provided.",
            "Table locking from master prevents concurrent schema modifications to corrupt table state."
        ],
        "useful": true
    },
    {
        "concept": "given cell",
        "explanation": [
            "Deep clones the given cell if the cell supports deep cloning."
        ],
        "useful": true
    },
    {
        "concept": "ZooKeeper",
        "explanation": [
            "Zookeeper has a default limit of 1MB/node.",
            "If using a setup with multiple KMS instances, 'zookeeper' should be used."
        ],
        "useful": true
    },
    {
        "concept": "table called",
        "explanation": [
            "For this example, assume that a table called normal exists with a single double column called val, containing a large number of random number drawn from the standard normal distribution.",
            "hive> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING); creates a table called invites with two columns and a partition column called ds."
        ],
        "useful": true
    },
    {
        "concept": "DataNode",
        "explanation": [
            "Setting this to 0 fails requests right away if the datanode is not yet registered with the namenode.",
            "Any other value will throw an exception when namenode and datanodes are starting up.",
            "the datanode will be transitioned to DECOMMISSIONED state.",
            "HDFS datanodes simply see a stream of encrypted bytes.",
            "Check if all datanodes are healthy after restart.",
            "Repeat the above steps until all datanodes in the cluster are upgraded.",
            "Previously a datanode resigned if any volume failed.",
            "HDFS Rolling Upgrade document explains how the datanodes can be upgraded in a rolling fashion without downtime.",
            "Each element maps to one datanode and each datanode can have multiple properties.",
            "After the timeout, the datanode will be transitioned out of maintenance state automatically by HDFS without human intervention.",
            "The datanode now performs 4MB readahead by default when reading data from its disks, if the native libraries are present.",
            "Otherwise, datanode will be registered and the default rack will be assigned as the topology path.",
            "This means all datanodes belonging to a specific upgrade domain collectively won't store more than one replica of any block.",
            "On startup, the datanode will automatically upgrade it's storages to this new layout.",
            "Datanodes send periodic heartbeats and block reports.",
            "Datanode can continue if a volume for replica storage fails.",
            "Upon startup, the datanode will automatically change the permissions to match the configured value.",
            "Stale datanodes are avoided, and marked as the last possible target for a read or write operation.",
            "This command can be used for checking if a datanode is alive like the Unix ping command.",
            "The Datanodes are started on the nodes specified in the slaves file.",
            "Simulated Datanodes should not include blocks that are still being written in their block report.",
            "If the DATA blocks fit inside fscache, this alternative may make sense when access is completely random across a very large dataset.",
            "When all the Namenodes finish decommissioning a Datanode, the Datanode is considered decommissioned.",
            "Datanode will report to all the nameservices in this list."
        ],
        "useful": true
    },
    {
        "concept": "Cache size",
        "explanation": [
            "This is very useful specially when the Cache size requirement is high."
        ],
        "useful": true
    },
    {
        "concept": "Row Format",
        "explanation": [
            "The delimited row format specifies how the rows are stored in the hive table.",
            "A native SerDe is used if ROW FORMAT is not specified or ROW FORMAT DELIMITED is specified."
        ],
        "useful": true
    },
    {
        "concept": "Table Scan",
        "explanation": [
            "But, more things will be added here as table scan is invoked as part of local work."
        ],
        "useful": true
    },
    {
        "concept": "space quota",
        "explanation": [
            "When a space quota is set on a namespace, the quota's limit applies to the sum of usage of all tables in that namespace.",
            "However, overall space quota is recommended in this case as the storage type information is either unavailable or inaccurate for storage type quota enforcement.",
            "By default, if a table or namespace is deleted that has a space quota, the quota itself is also deleted.",
            "With the -q option, also report the name quota value set for each directory, the available name quota remaining, the space quota value set, and the available space quota remaining.",
            "The space quota is a hard limit on the number of bytes used by files in the tree rooted at that directory.",
            "In some cases, it may be desirable for the space quota to not be automatically deleted."
        ],
        "useful": true
    },
    {
        "concept": "CREATE TABLE",
        "explanation": [
            "from transactions The CREATE TABLE creates a dummy table which controls how the output of the sort is written.",
            "Insert, select, create table, drop table, create database, add partition, drop partition have been tested.",
            "hive> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING); creates a table called invites with two columns and a partition column called ds."
        ],
        "useful": true
    },
    {
        "concept": "prefix tree",
        "explanation": [
            "Prefix Tree may be appropriate for applications that have high block cache hit ratios."
        ],
        "useful": true
    },
    {
        "concept": "table restore",
        "explanation": [
            "When a table restore operation starts, a two-step process is initiated."
        ],
        "useful": true
    },
    {
        "concept": "cluster replication",
        "explanation": [
            "Replication to secondary region replicas works over standard inter-cluster replication."
        ],
        "useful": false
    },
    {
        "concept": "finish time",
        "explanation": [
            "Because job finish time will include the job scheduling overhead."
        ],
        "useful": true
    },
    {
        "concept": "protoc",
        "explanation": [
            "Protos are hosted by the module that makes use of them."
        ],
        "useful": true
    },
    {
        "concept": "input table",
        "explanation": [
            "The other input tables will be recognize as the small tables during the execution stage and these tables need to be held in the memory."
        ],
        "useful": true
    },
    {
        "concept": "split request",
        "explanation": [
            "A region split request is enqueued if the policy recommends it."
        ],
        "useful": true
    },
    {
        "concept": "meta store",
        "explanation": [
            "NOTE: \u2022 Meta Store would need to answer queries with in clauses."
        ],
        "useful": true
    },
    {
        "concept": "key request",
        "explanation": [
            "If the key is not specified, the default expiry duration specified in the sas-key request takes effect."
        ],
        "useful": false
    },
    {
        "concept": "bloom filter",
        "explanation": [
            "Bloom filters were introduced in HBASE-1200.",
            "Large Bloom filters produce a different performance problem: the first get request that requires a Bloom filter lookup will incur the latency of loading the entire Bloom filter bit array.",
            "A counting Bloom filter is an improvement to standard a Bloom filter as it allows dynamic additions and deletions of set membership information.",
            "A Bloom filter chunk enqueued for writing.",
            "Bloom filters are enabled on a Column Family.",
            "A dynamic Bloom filter (DBF) makes use of a s * m bit matrix but each of the s rows is a standard Bloom filter.",
            "Bloom filter type specified in column family configuration.",
            "If an active Bloom filter is found, the key is inserted and nr is incremented by one.",
            "(A bit field or bloom filter could also be included.) Row index entries provide offsets that enable seeking to the right compression block and byte within a decompressed block.",
            "Enabling Bloom Filters can save your having to go to disk and can help improve read latencies.",
            "A Bloom filter, named for its creator, Burton Howard Bloom, is a data structure which is designed to predict whether a given element is a member of a set of data.",
            "The Bloom filter is a data structure that was introduced in 1970 and that has been adopted by the networking research community in the past decade thanks to the bandwidth efficiencies that it offers for the transmission of set membership information between networked hosts.",
            "Unless you have one column per row, row+column Bloom filters require more space, in order to store more keys.",
            "Bloom filters are designed to be \"accurate enough\" for sets of data which are so large that conventional hashing mechanisms would be impractical.",
            "A Bloom filter is active when the number of recorded keys, nr, is strictly less than the current cardinality of A, n.",
            "Bloom filters need to be rebuilt upon deletion, so may not be appropriate in environments with a large number of deletions.",
            "Bloom filter blocks and index blocks (we call these \"inline blocks\") become interspersed with data blocks, and as a side effect we can no longer rely on the difference between block offsets to determine data block length, as it was done in version 1."
        ],
        "useful": true
    },
    {
        "concept": "table links",
        "explanation": [
            "I agree that table links are a degenerate case of views.",
            "I don't think dynamic table links satisfy the use case covered by Team 2's access requirements for table T1.",
            "If we assume that it's possible to make this change then from a quota management standpoint I don't think the table links makes quota management easier.",
            "I don't think table links make this process atomic, and as I mentioned above the process of maintaining this linked set of partitions actually seems easier if you use views instead."
        ],
        "useful": true
    },
    {
        "concept": "Pradeep",
        "explanation": [
            "Pradeep Kamath gave an update on progress with Howl."
        ],
        "useful": true
    },
    {
        "concept": "client service",
        "explanation": [
            "intra-cluster traffic between two services may be routed over a private interface but the client service looked up its public hostname."
        ],
        "useful": true
    },
    {
        "concept": "wait period",
        "explanation": [
            "Such wait period gives RM a chance to settle down resyncing with NMs in the cluster on recovery, before assigning new containers to applications."
        ],
        "useful": true
    },
    {
        "concept": "bucket size",
        "explanation": [
            "Bucket size is 1 / error rate."
        ],
        "useful": true
    },
    {
        "concept": "Distcp",
        "explanation": [
            "DistCp now has a \"-basedir\" option that allows you to set the sufix of the source path that will be copied to the destination.",
            "Distcp will launch a mapreduce job to handle copying the files in a distributed fashion.",
            "Legacy DistCp works by figuring out what files need to be actually copied to target before the copy-job is launched, and then launching as many maps as required for copy.",
            "Note: Distcp works in this situation because the cluster is down and there are no in-flight edits to files.",
            "DistCp may alternatively be sub-classed to fine-tune behaviour.",
            "Rather than to permit this conflict, DistCp will abort.",
            "DistCp is the main driver-class for DistCpV2.",
            "(Also, in this example, the distcp should be run using the configuraton of the new filesystem.).",
            "Distcp will no longer start jobs that move no data.",
            "By default, DistCp makes an attempt to size each map comparably so that each copies roughly the same number of bytes.",
            "When DistCp -update is used with object stores, generally only the modification time and length of the individual files are compared, not any checksums.",
            "DistCp can be used to upload data."
        ],
        "useful": true
    },
    {
        "concept": "given types",
        "explanation": [
            "Check if the given type is numeric."
        ],
        "useful": false
    },
    {
        "concept": "source row",
        "explanation": [
            "In this case the source row would never appear in the results."
        ],
        "useful": false
    },
    {
        "concept": "LLAP",
        "explanation": [
            "Obviously, LLAP level of support depends on each individual execution engine (starting with Tez)."
        ],
        "useful": true
    },
    {
        "concept": "storage capacity",
        "explanation": [
            "Archival Storage is a solution to decouple growing storage capacity from compute capacity."
        ],
        "useful": true
    },
    {
        "concept": "keyspace",
        "explanation": [
            "Also, if you are pre-splitting regions and all your data is still winding up in a single region even though your keys aren't monotonically increasing, confirm that your keyspace actually works with the split strategy."
        ],
        "useful": true
    },
    {
        "concept": "given principal",
        "explanation": [
            "Get the grant information of roles the given principal belongs to."
        ],
        "useful": true
    },
    {
        "concept": "index size",
        "explanation": [
            "This is optimized for point-lookups in the case where a value typically occurs more than once in nearby rows; the index size is kept small since there are many fewer blocks than rows."
        ],
        "useful": true
    },
    {
        "concept": "skewed keys",
        "explanation": [
            "The skewed keys in A are only read and processed by the Mapper, and not sent to the reducer."
        ],
        "useful": true
    },
    {
        "concept": "high latency",
        "explanation": [
            "This can cause high latency in job submission as well as incur some AWS network transmission costs."
        ],
        "useful": true
    },
    {
        "concept": "Typed bytes",
        "explanation": [
            "Typed bytes are sequences of bytes in which the first byte is a type code."
        ],
        "useful": true
    },
    {
        "concept": "edit files",
        "explanation": [
            "In case there is some problem with hadoop cluster and the edits file is corrupted it is possible to save at least part of the edits file that is correct.",
            "One edit file may contain several transactions (edits)."
        ],
        "useful": true
    },
    {
        "concept": "DN",
        "explanation": [
            "This setting controls how much DN volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced."
        ],
        "useful": true
    },
    {
        "concept": "data discovery",
        "explanation": [
            "The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse."
        ],
        "useful": true
    },
    {
        "concept": "WAL",
        "explanation": [
            "Register WAL files as eligible for deletion.",
            "Skips WAL edits for all System tables including META.",
            "This causes the WAL to be a performance bottleneck.",
            "If deferred log flush is used, WAL edits are kept in memory until the flush period.",
            "By default, WAL tag compression is turned on when WAL compression is enabled.",
            "As bulk loading bypasses the write path, the WAL doesn't get written to as part of the process.",
            "WALs are retained when enabling or disabling replication as long as peers exist.",
            "The WALs for each region server must be kept in HDFS as long as they are needed to replicate data to any slave cluster.",
            "The WAL is filtered to this set of tables.",
            "Already-processed WALs are stored in /hbase/oldWALs/ and corrupt WALs are stored in /hbase/.corrupt/ for examination.",
            "The WAL can be replayed for a set of tables or all tables, and a timerange can be provided (in milliseconds)."
        ],
        "useful": true
    },
    {
        "concept": "specified table",
        "explanation": [
            "Get if the specified table is partitioned table or not.",
            "Checks if the specified table exists.",
            "Suspend the procedure if the specified table is already locked."
        ],
        "useful": true
    },
    {
        "concept": "NPE",
        "explanation": [
            "NPE occurred in the following cases - a blacklisted tracker is either decommissioned or expires."
        ],
        "useful": true
    },
    {
        "concept": "YARN resource",
        "explanation": [
            "The YARN Resource Manager gets a new token for the node managers, if needed.",
            "YARN Resource Managers (RMs) and Node Managers (NMs) co-operate to execute the user's application with the identity and hence access rights of that user."
        ],
        "useful": true
    },
    {
        "concept": "webapps",
        "explanation": [
            "If only a host is provided as the value, the webapp will be served on a random port."
        ],
        "useful": true
    },
    {
        "concept": "Resource reserved",
        "explanation": [
            "Resource reserved for the allocation."
        ],
        "useful": false
    },
    {
        "concept": "Login",
        "explanation": [
            "Did the login happen via keytab.",
            "If login details were provided in the filesystem URI, a warning is printed and then the username and password extracted for the AWS key and secret respectively.",
            "Did the login happen via ticket cache.",
            "This method assumes that login had happened already."
        ],
        "useful": true
    },
    {
        "concept": "HEAD",
        "explanation": [
            "GET,POST,HEAD Comma separated list of headers that are allowed for web services needing cross-origin (CORS) support."
        ],
        "useful": true
    },
    {
        "concept": "Variable Substitution",
        "explanation": [
            "All paths must be absolute and no environment variable substitution will be performed."
        ],
        "useful": true
    },
    {
        "concept": "Bloom information",
        "explanation": [
            "Bloom information from the cell is retrieved."
        ],
        "useful": false
    },
    {
        "concept": "Column name",
        "explanation": [
            "Skewed column name should be a valid column defined.",
            "Vectorization: Partition column names are not picked up.",
            "Column names in this string are unqualified references to the columns of the table over which the filter operates, as they are known in the Hive metastore.",
            "Only column names appear in PARTITIONED ON; no types etc.",
            "Note that skewed column name matches skewed value in order.",
            "Skewed column name and value should match.",
            "As part of this step, the column names are verified and expansions like * are performed.",
            "Any column name that is specified within backticks (`) is treated literally.",
            "(Initially, all column names are allowed.).",
            "Value can be \"none\" or \"column\".column: Column names can contain any Unicode character."
        ],
        "useful": true
    },
    {
        "concept": "table created",
        "explanation": [
            "If a table created using the PARTITIONED BY clause, a query can do partition pruning and scan only a fraction of the table relevant to the partitions specified by the query.",
            "The table created by CTAS is atomic, meaning that the table is not seen by other users until all the query results are populated."
        ],
        "useful": true
    },
    {
        "concept": "service entry",
        "explanation": [
            "If the service entry is found, the client should attempt to communicate with the AM on its channel.",
            "If the registered service entry cannot be found, the container MAY do one of: exit."
        ],
        "useful": true
    },
    {
        "concept": "table definition",
        "explanation": [
            "Called before a new table definition is added to the metastore during CREATE TABLE.",
            "Called before a table definition is removed from the metastore during DROP TABLE."
        ],
        "useful": true
    },
    {
        "concept": "delta directory",
        "explanation": [
            "For every transaction batch a delta directory will be created which will impact when compaction will trigger."
        ],
        "useful": true
    },
    {
        "concept": "MVCC",
        "explanation": [
            "Set the mvcc read point to -1 which means do not use it."
        ],
        "useful": true
    },
    {
        "concept": "temp table",
        "explanation": [
            "This has to be done after the temp table is populated and all necessary Partition objects exist in the metastore."
        ],
        "useful": false
    },
    {
        "concept": "Column Statistics",
        "explanation": [
            "Column statistics are fetched from the metastore.",
            "Based on the provided column statistics and number of rows, this method infers if the column can be primary key.",
            "However, the base column statistics is not cached and needs to fetch from SQL database everytime needed.",
            "Basic and column statistics can have one of the following states COMPLETE, PARTIAL, NONE."
        ],
        "useful": true
    },
    {
        "concept": "compressed block",
        "explanation": [
            "Compressed blocks can be jumped over without first having to be decompressed for scanning.",
            "Each compressed block requires one compression/decompression codec for I/O."
        ],
        "useful": true
    },
    {
        "concept": "mob files",
        "explanation": [
            "Checks if the mob file is expired.",
            "Currently, there are only two compact types: NORMAL means do store files compaction; MOB means do mob files compaction.",
            "Checks whether the referenced mob file exists.",
            "The MOB file name uses only the date part of the file creation time in it."
        ],
        "useful": true
    },
    {
        "concept": "destination directories",
        "explanation": [
            "When a file is renamed, its modification time is not changed, but the source and destination directories have their modification times updated."
        ],
        "useful": true
    },
    {
        "concept": "Health Checker",
        "explanation": [
            "The health checker script is not supposed to give ERROR if only some of the local disks become bad."
        ],
        "useful": true
    },
    {
        "concept": "table coprocessor",
        "explanation": [
            "In this version, the coprocessor is loaded dynamically (table coprocessor for the flowrun table).",
            "If 'false' (disabled), any table coprocessor attributes in table descriptors will be ignored."
        ],
        "useful": true
    },
    {
        "concept": "failovers",
        "explanation": [
            "Even if automatic failover is configured, you may initiate a manual failover using the same hdfs haadmin command.",
            "RM Failover Recovering previous active-RM's state."
        ],
        "useful": true
    },
    {
        "concept": "storage statistics",
        "explanation": [
            "Stores global storage statistics objects."
        ],
        "useful": false
    },
    {
        "concept": "filter operates",
        "explanation": [
            "When only include patterns are specified, the filter operates in the white listing mode, where only matched sources are included."
        ],
        "useful": true
    },
    {
        "concept": "fileSystem",
        "explanation": [
            "Other filesystems are skipped unless there is a specific configuration to the remote server providing the filesystem.",
            "If the filesystem has multiple partitions, the use and capacity of the root partition is reflected.",
            "Case: Underlying filesystem doesn't behave in a way that matches Hadoop's expectations.",
            "New filesystem shell command -df reports capacity, space used and space free.",
            "It can then be declared that a path has no parent in which case it is the root directory, or it MUST have a parent that is a directory: Because the parent directories of all directories must themselves satisfy this criterion, it is implicit that only leaf nodes may be files or symbolic links: Furthermore, because every filesystem contains the root path, every filesystem must contain at least one directory.",
            "It also checks that the filesystem isn't in a read-only state.",
            "If set, when the filesystem is instantiated then all outstanding uploads older than the purge age will be terminated -across the entire bucket.",
            "If the operations are interrupted, the filesystem is left in an intermediate state.",
            "Filesystem commands which list permission and user/group details, usually simulate these details."
        ],
        "useful": true
    },
    {
        "concept": "key part",
        "explanation": [
            "The key part of the cell is taken for comparison which includes row, family, qualifier, timestamp and type."
        ],
        "useful": true
    },
    {
        "concept": "Cluster connection",
        "explanation": [
            "Cluster connection to be shared by services."
        ],
        "useful": true
    },
    {
        "concept": "Active Directory",
        "explanation": [
            "Azure Active Directory (Azure AD) is Microsoft's multi-tenant cloud based directory and identity management service."
        ],
        "useful": true
    },
    {
        "concept": "hbase",
        "explanation": [
            "We do this indirection so hbase core can evolve its protobuf version independent of whatever our dependencies rely on.",
            "In this case, you would do the following to link the hadoop native lib so hbase could find them.",
            "The hbase:meta table (previously called .META.) keeps a list of all regions in the system.",
            "Here are others that you may have to take into account: The hbase:meta table is forced into the block cache and have the in-memory priority which means that they are harder to evict."
        ],
        "useful": false
    },
    {
        "concept": "Memstore",
        "explanation": [
            "Called before the memstore is flushed to disk.",
            "Called after the memstore is flushed to disk.",
            "Number of milliseconds updates have been blocked so the memstore can be flushed.",
            "When it is enabled, memstores will step allocate memory in MSLAB 2MB chunks even if the memstore has zero or just a few small elements.",
            "For write-heavy workload, memstore fraction can be increased in configuration at the expense of block cache; this will also allow one to have more regions."
        ],
        "useful": true
    },
    {
        "concept": "Hadoop",
        "explanation": [
            "Hadoop streaming is a utility that comes with the Hadoop distribution.",
            "Hadoop YARN allows applications to run on the Hadoop cluster.",
            "Hadoop now requires Java 6.",
            "The date that Hadoop was compiled.",
            "Hadoop has a library package called Aggregate.",
            "Unlike the released documentation, which is part of Hadoop source tree, Hadoop Wiki is regularly edited by Hadoop Community.",
            "Hadoop now supports integration with Azure Storage as an alternative Hadoop Compatible File System.",
            "Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes.",
            "Once the provider is set in the Hadoop configuration, hadoop commands work exactly as if the secrets were in an XML file.",
            "This is the hadoop copied local so can fix bugs and make hbase-specific optimizations.",
            "Hadoop supports shell-like commands to interact with HDFS directly.",
            "You must choose which Hadoop to build against.",
            "Finally: Apache Hadoop is an open source project.",
            "Hadoop will send multiple IP addresses to ARGV when forking the topology script.",
            "Hadoop uses URIs to refer to files within a filesystem.",
            "Hadoop components are rack-aware.",
            "Hadoop has an option parsing framework that employs parsing generic options as well as running classes.",
            "Hadoop is written in Java and is supported on all major platforms.",
            "When Hadoop is configured to run in secure mode, each Hadoop service and each user must be authenticated by Kerberos.",
            "Hadoop should be capable of generating serialization code in multiple target languages and should be C++ and Java.",
            "Hadoop daemons obtain the rack information of the slaves in the cluster by invoking an administrator configured module.",
            "Hadoop has upgraded its dependency so that this class is deprecated.",
            "Then Hadoop system will look for a mount table with the name \"clusterX\" in the Hadoop configuration files.",
            "This .20S workaround should cease to exist when Hadoop supports token store.",
            "Hadoop Auth is a Java library consisting of a client and a server components to enable Kerberos SPNEGO authentication for HTTP.",
            "By doing this, Hadoop will reuse the entries in the distributed cache.",
            "Over last few years Apache Hadoop has become the de facto platform for distributed data processing using commodity hardware.",
            "In this configuration, Hadoop jobs run with the privileges of the invoking user.",
            "Thus Hive and Hive Web Interface cannot enforce more stringent security then Hadoop can.",
            "Hadoop is able to read/write such files using the S3N filesystem.",
            "check whether current hadoop supports sticky bit.",
            "hadoop trace -list shows list of loaded span receivers associated with the id.",
            "Hadoop metrics sent to Ganglia over multicast now support optional configuration of socket TTL.",
            "To facilitate this, Hadoop supports a notion of a default file system.",
            "Hadoop is setup in secure mode with the authentication type set to kerberos.",
            "Hadoop, including HDFS, is well suited for distributed storage and distributed processing using commodity hardware.",
            "Unit tests are only flagged as necessary with native or Java code, since Hadoop has no framework in place yet for other types of unit tests.",
            "For each target language Hadoop defines very simple input and output stream interfaces.",
            "JDK classes, hadoop classes and resources, and some select third-party classes are considered system classes, and are not loaded by the application classloader.",
            "In particular, a user wishing to interact with Hadoop or Hive requires access to many ports.",
            "Hadoop users have to describe their data in a simple data description language.",
            "Optionally, you may now configure the default path for Hadoop clients to use the new HA-enabled logical URI.",
            "Hadoop Auth also supports additional authentication mechanisms on the client and the server side via 2 simple interfaces.",
            "Hadoop should include as primitives commonly used builtin types from programming languages we intend to support.",
            "Hadoop generates code for serializing and deserializing record types to abstract streams.",
            "Hadoop provides an optional mode of execution in which the bad records are detected and skipped in further attempts.",
            "If Hadoop cannot authenticate with the S3 service endpoint, the client retries a number of times before eventually failing.",
            "By default, Hadoop is configured to run in a non-distributed mode, as a single Java process.",
            "Hadoop doesn't have a Company-Private classification, which is meant for APIs which are intended to be used by other projects within the company, since it doesn't apply to opensource projects.",
            "But Hadoop also supports special group mapping mechanisms through LDAP and composition of LDAP and operating system group name resolution, which require additional configurations.",
            "Alternatively Hadoop provides a block based file system using S3 as a backing store.",
            "Hadoop should support widely used composite types such as structs and vectors.",
            "Hadoop 2 added iterative listing to handle the challenge of listing directories with millions of entries without buffering at the cost of consistency.",
            "Hadoop 2 is strongly encouraged (faster but also has fixes that help MTTR).",
            "hadoop s3guard destroy -meta dynamodb://ireland-team -region eu-west-1 Clean up a table, s3guard prune.",
            "By default, Hadoop makes two LDAP queries per user if this value is empty.",
            "Kosmos FS (KFS) is no longer maintained and Hadoop support has been removed.",
            "Required command line arguments: Optional command line arguments: Hadoop offline edits viewer.",
            "Hadoop provides an option where a certain set of bad input records can be skipped when processing map inputs.",
            "Hadoop will call us multiple times in the event of failure.",
            "Hadoop recommends that this value should be less than the ZKFC session timeout value.",
            "Hadoop comes configured with a single mandatory queue, called 'default'.",
            "Hadoop set this to 1 by default, whereas Hive uses -1 as its default value.",
            "Hadoop assumes that filesystems are consistent; that creation, updates and deletions are immediately visible, and that the results of listing a directory are current with respect to the files within that directory.",
            "Hadoop Auth enforces authentication on protected resources, once authentiation has been established it sets a signed HTTP Cookie that contains an authentication token with the user name, user principal, authentication type and expiration time.",
            "Hadooop should include support in the form of headers, libraries, packages for supported target languages that enable easy inclusion and use of generated code in applications.",
            "If set, Hadoop will attempt to resolve group names from this attribute, instead of making the second LDAP query to get group objects.",
            "Typically, Hadoop resolves a user's group names by making two LDAP queries: the first query gets the user object, and the second query uses the user's Distinguished Name to find the groups."
        ],
        "useful": true
    },
    {
        "concept": "Load Generator",
        "explanation": [
            "The Synthetic Load Generator complements the extensive nature of SLS-native and RUMEN traces, by providing a distribution-driven generation of load."
        ],
        "useful": true
    },
    {
        "concept": "max count",
        "explanation": [
            "There are lazily created buffers and the count is the max count to be pooled."
        ],
        "useful": false
    },
    {
        "concept": "memory copy",
        "explanation": [
            "Indicates which memory copy is used in building cell."
        ],
        "useful": false
    },
    {
        "concept": "auth method",
        "explanation": [
            "Note: This auth method is suitable for running interactive tools, but will not work for jobs submitted to a cluster."
        ],
        "useful": true
    },
    {
        "concept": "Javadocs",
        "explanation": [
            "Javadoc warnings are checked during precommit."
        ],
        "useful": false
    },
    {
        "concept": "results list",
        "explanation": [
            "The results list can contain expressions based on the input columns and also the matched Path."
        ],
        "useful": true
    },
    {
        "concept": "byte range",
        "explanation": [
            "Limits the byte range upto a specified value."
        ],
        "useful": false
    },
    {
        "concept": "rr",
        "explanation": [
            "When the read pattern is a random row read load and each of the rows are smaller in size compared to this 64 KB, try reducing this."
        ],
        "useful": true
    },
    {
        "concept": "Timeframes",
        "explanation": [
            "Timeframes can be expressed in the following units: sec, min, hour, day."
        ],
        "useful": true
    },
    {
        "concept": "Storage accounts",
        "explanation": [
            "A storage account may have multiple containers."
        ],
        "useful": true
    },
    {
        "concept": "PUT request",
        "explanation": [
            "A PUT request uploads an object/\"Blob\"; a GET request retrieves it; ranged GET operations permit portions of a blob to retrieved.",
            "The put request will be buffered by its corresponding buffer queue.",
            "Put requests are used to modify the scheduler configuration."
        ],
        "useful": true
    },
    {
        "concept": "foreign keys",
        "explanation": [
            "All foreign keys for a particular table can be fetched by passing null for the last two arguments."
        ],
        "useful": true
    },
    {
        "concept": "input scan",
        "explanation": [
            "Configure input scan with proper ranges, iterators, and columns based on serde properties for Hive table."
        ],
        "useful": true
    },
    {
        "concept": "intermediate files",
        "explanation": [
            "The intermediate files in the temporary directory will not be cleaned up."
        ],
        "useful": true
    },
    {
        "concept": "Predicate Pushdown",
        "explanation": [
            "Predicate pushdown is a term borrowed from relational databases even though for Hive it is predicate pushup."
        ],
        "useful": true
    },
    {
        "concept": "Data moved",
        "explanation": [
            "Data moved to the .Trash directory can be purged using the expunge command."
        ],
        "useful": true
    },
    {
        "concept": "every row",
        "explanation": [
            "every row gets its own set of rows to process the UDAF on."
        ],
        "useful": true
    },
    {
        "concept": "row matches",
        "explanation": [
            "A row matches if it has the same tag names and values as record, but it may also have additional tags."
        ],
        "useful": true
    },
    {
        "concept": "http methods",
        "explanation": [
            "Possible values are: false: All HTTP methods are permitted - GET/PUT/POST/DELETE."
        ],
        "useful": true
    },
    {
        "concept": "given scan",
        "explanation": [
            "Checks whether the given scan passes the Bloom filter (if present)."
        ],
        "useful": true
    },
    {
        "concept": "Templeton name",
        "explanation": [
            "The Templeton name is taken from a character in the award-winning children's novel Charlotte's Web, by E."
        ],
        "useful": true
    },
    {
        "concept": "index information",
        "explanation": [
            "The metadata and index information can be cached even for data that is not currently cached."
        ],
        "useful": true
    },
    {
        "concept": "input keys",
        "explanation": [
            "Input keys and values are swapped.",
            "Input keys must not be altered."
        ],
        "useful": true
    },
    {
        "concept": "storage policies",
        "explanation": [
            "Ensure Storage Policies are enabled.",
            "A storage policy specifies the placement of block replicas on specific storage types.",
            "To set storage type quota on a directory, storage policies must be configured on the directory in order to allow files to be stored in different storage types according to the storage policy.",
            "More formally, a storage policy consists of the following fields: Policy ID."
        ],
        "useful": true
    },
    {
        "concept": "reduced table",
        "explanation": [
            "The reduced table is then joined on c."
        ],
        "useful": true
    },
    {
        "concept": "ASCII",
        "explanation": [
            "ASCII code for 'A'."
        ],
        "useful": false
    },
    {
        "concept": "DynamoDB",
        "explanation": [
            "See DynamoDB documents for more information.",
            "The IO load of clients of the (shared) DynamoDB table was exceeded."
        ],
        "useful": true
    },
    {
        "concept": "jiras",
        "explanation": [
            "This jira introduces backward incompatibility.",
            "JIRA comments now use much more markup to improve readability.",
            "JIRA sorts attached files by the time they were attached, and has no problem with multiple attachments with the same name.",
            "Even if it doesn't get fixed, the JIRA is a public record of it, and will help others out if they run into a similar issue in the future.",
            "The jira made the following changes: 1.",
            "This jira only allows providing paths using back slash as separator on Windows.",
            "This JIRA makes following change: Change Router metrics context from 'router' to 'dfs'.",
            "This jira changes the default block size to 128MB."
        ],
        "useful": true
    },
    {
        "concept": "system directory",
        "explanation": [
            "The mapred system directory generated by HOD is cleaned up at cluster deallocation time."
        ],
        "useful": true
    },
    {
        "concept": "Cluster topology",
        "explanation": [
            "Cluster topology is used as follows : To reconstruct the splits and make sure that the distances/latencies seen in the actual run are modeled correctly."
        ],
        "useful": true
    },
    {
        "concept": "appId",
        "explanation": [
            "Such apps will not be retried by the RM on app attempt failure.",
            "Once app is created, note down the \"Appplication ID\" of the app.",
            "This is useful when the app only wants to aggregate logs of a subset of containers.",
            "When other apps are submitted, resources that free up are assigned to the new apps, so that each app eventually on gets roughly the same amount of resources.",
            "user: the app is placed into a queue with the name of the user who submitted it.",
            "Once app is created, go to \"keys\" under \"settings\" for the app.",
            "This could be null if no app is associated.",
            "If \"fifo\", apps with earlier submit times are given preference for containers, but apps submitted later may run concurrently if there is leftover space on the cluster after satisfying the earlier app's requests.",
            "You can confirm that the app is killed by repeating the PUT request until you get a 200, querying the state using the GET method or querying for app information and checking the state.",
            "Valid rules are: specified: the app is placed into the queue it requested.",
            "Unlike the default Hadoop scheduler, which forms a queue of apps, this lets short apps finish in reasonable time while not starving long-lived apps.",
            "If an app specifically lists a queue in a container resource request, the request is submitted to that queue.",
            "When there is a single app running, that app uses the entire cluster.",
            "default: the app is placed into the queue specified in the 'queue' attribute of the default rule.",
            "App should not make concurrent allocate requests.",
            "If the app cannot use the container or wants to give up the container then it can release them.",
            "So even after the remove request the app must be prepared to receive an allocation for the previous request even after the remove request."
        ],
        "useful": true
    },
    {
        "concept": "leaf level",
        "explanation": [
            "Intermediate levels can only be present if leaf level blocks are present Optionally, version 2 leaf levels, stored in the non%root format inline with data blocks."
        ],
        "useful": false
    },
    {
        "concept": "table write",
        "explanation": [
            "Get the table write Ids that are valid for the current transaction."
        ],
        "useful": false
    },
    {
        "concept": "created directory",
        "explanation": [
            "Newly created directories have no associated quota."
        ],
        "useful": true
    },
    {
        "concept": "table file",
        "explanation": [
            "External table files can be accessed and managed by processes outside of Hive."
        ],
        "useful": true
    },
    {
        "concept": "block pool",
        "explanation": [
            "Each Block Pool is managed independently.",
            "A Namespace and its block pool together are called Namespace Volume.",
            "A Block Pool is a set of blocks that belong to a single namespace.",
            "When a Namenode/namespace is deleted, the corresponding block pool at the Datanodes is deleted."
        ],
        "useful": true
    },
    {
        "concept": "changed files",
        "explanation": [
            "Otherwise, changed files may be missed/copied too often."
        ],
        "useful": true
    },
    {
        "concept": "slave node",
        "explanation": [
            "One node will be used as the master and as such the cluster will have 9 slave nodes.",
            "Slave nodes then use this reader context to read data."
        ],
        "useful": true
    },
    {
        "concept": "metrics related",
        "explanation": [
            "The following metrics have been removed: Metrics related to the Distributed Log Replay feature are no longer present."
        ],
        "useful": false
    },
    {
        "concept": "data tens",
        "explanation": [
            "Rewriting the same data tens of times is the last thing you want."
        ],
        "useful": false
    },
    {
        "concept": "Big table",
        "explanation": [
            "\u2022 Even if the Big Table columns are not partitioned, the set of values generated from small tables could be pushed down as a predicate on the big table.",
            "For the Map Join, the query processor should know which input table the big table is.",
            "Columns that are identified from small table has following characteristics: \u2022 Column is the other side of predicate in the join condition and Big Table column is identified as a target for partition pruning.",
            "During the probe phase, the big table S is scanned sequentially and for each row of S, the hash table is probed for matching rows."
        ],
        "useful": true
    },
    {
        "concept": "key series",
        "explanation": [
            "The key series will be positioned to the beginning.",
            "The key series is logically indexed."
        ],
        "useful": true
    },
    {
        "concept": "master cluster",
        "explanation": [
            "The master cluster relies on randomization to attempt to balance the stream of replication on the slave clusters.",
            "So if the master cluster crashes, the slave cluster may not have the newest data."
        ],
        "useful": true
    },
    {
        "concept": "RDD",
        "explanation": [
            "While RDD extension seems easy in Scala, this can be challenging as Spark's Java APIs lack such capability."
        ],
        "useful": true
    },
    {
        "concept": "access keys",
        "explanation": [
            "The access key is a secret that protects access to your storage account."
        ],
        "useful": true
    },
    {
        "concept": "Secret manager",
        "explanation": [
            "The secret manager is responsible for generating and accepting the password for each token.",
            "Setting to true always allows the DT secret manager to be used, even if security is disabled."
        ],
        "useful": true
    },
    {
        "concept": "vaidya",
        "explanation": [
            "Introduced Vaidya rule based performance diagnostic tool for Map/Reduce jobs."
        ],
        "useful": true
    },
    {
        "concept": "Cache files",
        "explanation": [
            "Clearly the cache files should not be modified by the application or externally while the job is executing.",
            "The cache file into which container token is written.",
            "Adjust this property can make cache file be available for the time as you want.",
            "The cache files are checked at the client side for public/private access on the file system, and that information is passed in the configuration.",
            "Cached file metadata (or overrides as the case may be)."
        ],
        "useful": true
    },
    {
        "concept": "cache entry",
        "explanation": [
            "If successful the cache entry will be set to valid status and be usable for cached queries.",
            "Wait for the cache entry to go from PENDING to VALID status."
        ],
        "useful": true
    },
    {
        "concept": "Rowkey",
        "explanation": [
            "The rowkey also has to be defined in details as a column (col0), which has a specific cf (rowkey).",
            "The rowkey [hostname][log-event][timestamp] is a candidate if there is a large-ish number of hosts to spread the writes and reads across the keyspace.",
            "In the Hostname In The Rowkey Lead Position example, it might look like this: Composite Rowkey With Hashes: [MD5 hash of hostname] = 16 bytes."
        ],
        "useful": true
    },
    {
        "concept": "Cloudera",
        "explanation": [
            "Cloudera provides some AMIs that bundle Hive with Hadoop - although the choice in terms of Hive and Hadoop versions may be restricted.",
            "Cloudera will take the lead on that.",
            "Cloudera will host the next meeting."
        ],
        "useful": true
    },
    {
        "concept": "Hortonworks",
        "explanation": [
            "Alan said that Hortonworks would like to contribute to this, and is interested in collaborating with others on it."
        ],
        "useful": true
    },
    {
        "concept": "snapshot file",
        "explanation": [
            "The snapshot should not be deleted while there are jobs reading from snapshot files.",
            "Blocks in datanodes are not copied: the snapshot files record the block list and the file size."
        ],
        "useful": true
    },
    {
        "concept": "Region state",
        "explanation": [
            "Region state is distributed and hard to reason about and test.",
            "Let a region state have some vintage before we act on it (one second currently)."
        ],
        "useful": true
    },
    {
        "concept": "HTTP verbs",
        "explanation": [
            "They also strive for simple non-POSIX APIs: the HTTP verbs are the operations allowed."
        ],
        "useful": true
    },
    {
        "concept": "Time ranges",
        "explanation": [
            "The WAL can be replayed for a set of tables or all tables, and a time range can be provided (in milliseconds)."
        ],
        "useful": true
    },
    {
        "concept": "drop table",
        "explanation": [
            "DROP TABLE removes metadata and data for this table.",
            "Insert, select, create table, drop table, create database, add partition, drop partition have been tested."
        ],
        "useful": true
    },
    {
        "concept": "classpath",
        "explanation": [
            "If called without arguments, then prints the classpath set up by the command scripts, which is likely to contain wildcards in the classpath entries.",
            "If named by a String, then the classpath is examined for a file with that name.",
            "Classpath is usually the first problem."
        ],
        "useful": true
    },
    {
        "concept": "exit codes",
        "explanation": [
            "Previously the exit code was success.",
            "Non 0 exit code indicates failure.",
            "If unsuccessful, exit codes are.",
            "Exit code when a control-C, kill -3, signal was picked up: 3."
        ],
        "useful": true
    },
    {
        "concept": "side cache",
        "explanation": [
            "Both client side and server side cache needs to be a singleton and shared within the JVM.",
            "In the worst case, the caller may get up to (server-side cache size + client-side cache size) number of old EEKs, or until both caches expire."
        ],
        "useful": false
    },
    {
        "concept": "lock table",
        "explanation": [
            "Set table state to provided but only if table in specified states Caller should lock table on write."
        ],
        "useful": false
    },
    {
        "concept": "RDL",
        "explanation": [
            "If the RDL is different, the reservation will be rejected, and the request will be unsuccessful."
        ],
        "useful": true
    },
    {
        "concept": "rdbms",
        "explanation": [
            "RDBMS products are more advanced in this regard to handle alternative index management out of the box.",
            "Whether the RDBMS has a bug in join and filter operation order described in DERBY-6358.",
            "An RDBMS can scale well, but only up to a point - specifically, the size of a single database server - and for the best performance requires specialized hardware and storage devices.",
            "Whether the RDBMS has restrictions on IN list size (explicit, or poor perf-based)."
        ],
        "useful": true
    },
    {
        "concept": "max number",
        "explanation": [
            "Defines the max number of applications could be fetched using REST API or application history protocol and shown in timeline This property should never be set to false.",
            "Defines the max number of applications could be fetched using REST API or application history protocol and shown in timeline server web ui."
        ],
        "useful": true
    },
    {
        "concept": "RPC",
        "explanation": [
            "This is because RPC commands are stateless.",
            "Logs if a RPC is really slow compared to rest of RPCs.",
            "Check if RPC is in asynchronous mode or not.",
            "When the feature is enabled, RPC server will no longer block on the processing of RPC requests when RPC call queue is full.",
            "The master retries sending the close request to the server until the RPC goes through or the master runs out of retries.",
            "RPC can use Avro serialization.",
            "Improve how RPC server reads and writes large buffers.",
            "Does RPC against a cluster.",
            "The time after which a RPC will timeout.",
            "Called after getting if is rpc throttle enabled.",
            "rpc version is introduced which should change when the format of rpc messages is changed.",
            "If the RPC threw an exception, the source will retry 10 times before trying to find a different sink.",
            "If the RPC was successful, the source determines whether the current file has been emptied or it contains more data which needs to be read."
        ],
        "useful": true
    },
    {
        "concept": "Data Encryption",
        "explanation": [
            "Get the current working directory of the Trash Policy This API does not work with files deleted from encryption zone when HDFS data encryption at rest feature is enabled as rename file between encryption zones or encryption zone and non-encryption zone is not allowed.",
            "Data encryption is required by a number of different government, financial, and regulatory entities."
        ],
        "useful": true
    },
    {
        "concept": "rpc call",
        "explanation": [
            "When the feature is enabled, RPC server will no longer block on the processing of RPC requests when RPC call queue is full."
        ],
        "useful": true
    },
    {
        "concept": "capacity numbers",
        "explanation": [
            "</description> Attempting to perform more IO than the capacity requested simply throttles the IO; small capacity numbers are recommended when initially experimenting with S3Guard."
        ],
        "useful": true
    },
    {
        "concept": "block file",
        "explanation": [
            "If a block file is specified, we will verify that the checksums in the metadata file match the block file.",
            "If a block file is specified, we will compute the checksums from the block file, and save it to the specified output metadata file.",
            "Only use as a last measure, and when you are 100% certain the block file is good."
        ],
        "useful": true
    },
    {
        "concept": "struct",
        "explanation": [
            "Structs are not nullable but their component fields may be.",
            "Struct treats the right-most nullable field members as special.",
            "Rather than writing null values to the output buffer, Struct omits those records all together.",
            "A struct is a sequence of <member> elements.",
            "A Struct may be used as a member of another Struct.",
            "The set of fields this struct contains, along with convenience methods for finding and manipulating them."
        ],
        "useful": true
    },
    {
        "concept": "threshold number",
        "explanation": [
            "When threshold number of the nodemanager-local-directories or threshold number of the nodemanager-log-directories become bad."
        ],
        "useful": true
    },
    {
        "concept": "deserializer",
        "explanation": [
            "In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree."
        ],
        "useful": true
    },
    {
        "concept": "output table",
        "explanation": [
            "Checks if the output table exists and is enabled."
        ],
        "useful": true
    },
    {
        "concept": "complete row",
        "explanation": [
            "If the version you specified when deleting a row is larger than the version of any value in the row, then you can consider the complete row to be deleted."
        ],
        "useful": false
    },
    {
        "concept": "nameNode",
        "explanation": [
            "Each namenode has its own namespace.",
            "A namenode prior to this change will not be able to communicate with a namenode after this change.",
            "Each Namenode decommissions its Block Pool.",
            "Any other value will throw an exception when namenode and datanodes are starting up.",
            "Specifies a rolling upgrade already started so that the namenode should allow image directories with different layout versions during startup.",
            "If the namenode ID is not configured it is determined automatically by matching the local node's address with the configured address.",
            "A namenode belongs to one and only one cluster.",
            "The issue HBASE-8354 forces Namenode into loop with lease recovery requests is messy but has a bunch of good discussion toward the end on low timeouts and how to cause faster recovery including citation of fixes added to HDFS.",
            "If set to false, the Namenode uses the traditional synchronous edit logs.",
            "Any Namenode in the cluster can be used to access this web page."
        ],
        "useful": true
    },
    {
        "concept": "unrecognized value",
        "explanation": [
            "An unrecognized value was silently assumed to mean authentication."
        ],
        "useful": true
    },
    {
        "concept": "group key",
        "explanation": [
            "If the grouping key is a superset of the bucketing and sorting keys of the underlying table in the same order, the group by can be be performed on the map-side completely."
        ],
        "useful": true
    },
    {
        "concept": "input files",
        "explanation": [
            "'LOCAL' signifies that the input file is on the local file system."
        ],
        "useful": true
    },
    {
        "concept": "Acid table",
        "explanation": [
            "External tables cannot be made ACID tables since the changes on external tables are beyond the control of the compactor (HIVE-13175).Reading/writing to an ACID table from a non-ACID session is not allowed."
        ],
        "useful": true
    },
    {
        "concept": "txn",
        "explanation": [
            "For implicit txn, the stmt that triggered/started the txn is the first statement.",
            "Each statement writing data within a multi statement txn should have a unique WriteId."
        ],
        "useful": true
    },
    {
        "concept": "Table instances",
        "explanation": [
            "Table instances are not thread-safe."
        ],
        "useful": true
    },
    {
        "concept": "consistency level",
        "explanation": [
            "Consistency defines the expected consistency level for an operation."
        ],
        "useful": true
    },
    {
        "concept": "maximum download",
        "explanation": [
            "Get the current maximum download bandwidth."
        ],
        "useful": false
    },
    {
        "concept": "Plugins",
        "explanation": [
            "This plugin is run when you specify the site goal as in when you run mvn site.",
            "In our build we use a maven plugin for convenience; however, the plugin may not be able to retrieve appropriate binaries for all platforms.",
            "Plugins for checkstyle, shellcheck, and whitespace now execute as necessary.",
            "The plugin can decide how two keys are joined.",
            "If not, click Available Plugins, select it, and click Install."
        ],
        "useful": true
    },
    {
        "concept": "log directories",
        "explanation": [
            "Fixed a bug in tasklog servlet which displayed wrong error message about job ACLs - an access control error instead of the expected log files gone error - after task logs directory is deleted."
        ],
        "useful": false
    },
    {
        "concept": "given Table",
        "explanation": [
            "Check whether the given table is contained in a sync replication peer which can pass the state checker."
        ],
        "useful": true
    },
    {
        "concept": "CPU",
        "explanation": [
            "Once resources (CPU, memory, etc.) have been obtained from YARN for a specific workload, the execution engine can choose to delegate these resources to LLAP, or to launch Hive executors in separate processes."
        ],
        "useful": false
    },
    {
        "concept": "startcode",
        "explanation": [
            "The startcode distinguishes restarted servers on same hostname and port (startcode is usually timestamp of server startup).",
            "What to use if no startcode supplied."
        ],
        "useful": true
    },
    {
        "concept": "LZ4",
        "explanation": [
            "LZ4 support is bundled with Hadoop."
        ],
        "useful": true
    },
    {
        "concept": "LCE",
        "explanation": [
            "The LCE requires that container-executor binary be owned by root:hadoop and have 6050 permissions.",
            "The LCE also provides enhanced security and is required when deploying a secure cluster.",
            "If so, the command will be overridden when LCE launches the image with YARN's container launch script.",
            "When the LCE launches a YARN container to execute in a Docker container, the application can specify the Docker image to be used.",
            "If running in non-secure mode, by default, the LCE runs all jobs as user \"nobody\"."
        ],
        "useful": true
    },
    {
        "concept": "KFS",
        "explanation": [
            "KFS has been replaced by QFS (HADOOP-8885).",
            "Kosmos FS (KFS) is no longer maintained and Hadoop support has been removed."
        ],
        "useful": true
    },
    {
        "concept": "data block",
        "explanation": [
            "Data block encoder used for data blocks.",
            "Data block index reader keeping the root data index in memory.",
            "Default data block encoding algorithm.",
            "This size is approximate, because Bloom blocks can only be inserted at data block boundaries, and the number of keys per data block varies.",
            "Encapsulates a data block compressed using a particular encoding algorithm.",
            "Bloom filter blocks and index blocks (we call these \"inline blocks\") become interspersed with data blocks, and as a side effect we can no longer rely on the difference between block offsets to determine data block length, as it was done in version 1.",
            "Smaller blocks are good for random access, but require more memory to hold the block index, and may be slower to create (because we must flush the compressor stream at the conclusion of each data block, which leads to an FS I/O flush).",
            "If the DATA blocks fit inside fscache, this alternative may make sense when access is completely random across a very large dataset."
        ],
        "useful": true
    },
    {
        "concept": "History file",
        "explanation": [
            "Every job history file consists of events.",
            "Post HADOOP-4372, empty job history files caused NPE.",
            "Parses history file and calls call back functions.",
            "The job history file at this path may or may not be existing depending on the job completion state."
        ],
        "useful": true
    },
    {
        "concept": "specified nodes",
        "explanation": [
            "Creates the specified node, iff the node does not exist."
        ],
        "useful": false
    },
    {
        "concept": "submission code",
        "explanation": [
            "If \"disabled\" is specified then the job submission code will not use the shared cache."
        ],
        "useful": true
    },
    {
        "concept": "data nodes",
        "explanation": [
            "Failure and recovery is simplified because any data node can still be used to process any fragment of the input data.",
            "This is useful when data node decommissioning is blocked by slow writers.",
            "A stale data node is avoided during lease/block recovery.",
            "The Data Nodes will flush in-memory data to disk asynchronously thus removing expensive disk IO and checksum computations from the performance-sensitive IO path, hence we call such writes Lazy Persist writes.",
            "Following a scheduled reboot, one data node began exhibiting unusual behavior."
        ],
        "useful": true
    },
    {
        "concept": "sequence file",
        "explanation": [
            "Existing sequence files can be appended.",
            "Text files require an extension, whereas others, like sequence files, do not.",
            "When two sequence files, which have same Key type but different Value types, are mapped out to reduce, multiple Value types is not allowed."
        ],
        "useful": true
    },
    {
        "concept": "encryption algorithm",
        "explanation": [
            "Encryption algorithm and key used."
        ],
        "useful": false
    },
    {
        "concept": "CSRF",
        "explanation": [
            "In this case, CSRF is not a potential attack vector, so the prevention is not enforced.",
            "If the incoming User-Agent matches any of these regular expressions, then the request is considered to be sent by a browser, and therefore CSRF prevention is enforced."
        ],
        "useful": true
    },
    {
        "concept": "ipc",
        "explanation": [
            "Address where the localizer IPC is.",
            "Address where the collector service IPC is."
        ],
        "useful": true
    },
    {
        "concept": "lock time",
        "explanation": [
            "Lock times out on master: Can happen because of network issues, GC pauses, etc."
        ],
        "useful": true
    },
    {
        "concept": "StoreFile size",
        "explanation": [
            "Archived Storefile Size is the Storefile size in Archive.",
            "Shared Storefile Size is the Storefile size shared between snapshots and active tables."
        ],
        "useful": true
    },
    {
        "concept": "replication count",
        "explanation": [
            "The replication count of all files is \"1\"."
        ],
        "useful": true
    },
    {
        "concept": "metric info",
        "explanation": [
            "More metrics info can see Router RPC Metrics and State Store Metrics."
        ],
        "useful": true
    },
    {
        "concept": "Container request",
        "explanation": [
            "The execution types are the following: GUARANTEED - this container is guaranteed to start its execution, once the corresponding start container request is received by an NM."
        ],
        "useful": false
    },
    {
        "concept": "Hash aggregation",
        "explanation": [
            "Hash aggregation is an optimization in hive to reduce the number of rows shuffled between map and reduce stage.",
            "The number of rows emitted from map-side will vary if hash aggregation is enabled throughout execution or disabled."
        ],
        "useful": true
    },
    {
        "concept": "text value",
        "explanation": [
            "Text Encoding Each text value begins with a single byte of 0x33 and ends with a single byte of 0x00."
        ],
        "useful": true
    },
    {
        "concept": "mutation request",
        "explanation": [
            "Controls whether a mutation request is allowed."
        ],
        "useful": false
    },
    {
        "concept": "tag list",
        "explanation": [
            "Whether the tag list has a mob reference tag."
        ],
        "useful": true
    },
    {
        "concept": "Compactor",
        "explanation": [
            "A compactor is a compaction algorithm associated a given policy.",
            "When the compactor kicks in, these delta files get rewritten into read- and storage-optimized ORC format (enable dictionary encoding, indexes and compression)."
        ],
        "useful": true
    },
    {
        "concept": "column types",
        "explanation": [
            "Things can go wrong if the bucketing column type is different during the insert and on read, or if you manually cluster by a value that's different from the table definition."
        ],
        "useful": true
    },
    {
        "concept": "destination cluster",
        "explanation": [
            "Create tables with the same names and column families on both the source and destination clusters, so that the destination cluster knows where to store data it will receive."
        ],
        "useful": true
    },
    {
        "concept": "max period",
        "explanation": [
            "Also note that the configured max period must be divisible by the recurrence expression if expressed as a long."
        ],
        "useful": true
    },
    {
        "concept": "PTF",
        "explanation": [
            "PTFs are black boxes, we assume all columns are needed."
        ],
        "useful": true
    },
    {
        "concept": "gpg",
        "explanation": [
            "In case the GPG is not-available, cluster operations will continue as of the last time the GPG published policies, and while a long-term unavailability might mean some of the desirable properties of balance, optimal cluster utilization and global invariants might drift away, compute and access to data will not be compromised.",
            "The GPG operates continuously but out-of-band from all cluster operations, and provide us with a unique vantage point, that allows to enforce global invariants, affect load balancing, trigger draining of sub-clusters that will undergo maintenance, etc."
        ],
        "useful": true
    },
    {
        "concept": "NN",
        "explanation": [
            "On start, this NN will not enter the standby state as usual in an HA setup.",
            "The active NN at the time this happens will perform the finalization of the shared log, and the NN whose local storage directories contain the previous FS state will delete its local state.",
            "To perform a rollback of an upgrade, both NNs should first be shut down.",
            "Rather, this NN will immediately enter the active state, perform an upgrade of its local storage dirs, and also perform an upgrade of the shared edit log."
        ],
        "useful": true
    },
    {
        "concept": "Prasad",
        "explanation": [
            "Prasad pointed out that in the future, for materialized views, we may need the view definition to be tracked at the partition level as well, so that when we change the view definition, we don't have to discard existing materialized partitions if the new view result can be derived from the old one.",
            "Update 30-Dec-2009: Prasad pointed out that even without supporting materialized views, it may be necessary to provide users with metadata about data dependencies between views and underlying table partitions so that users can avoid seeing inconsistent results during the window when not all partitions have been refreshed with the latest data."
        ],
        "useful": true
    },
    {
        "concept": "table exists",
        "explanation": [
            "Thrown when a table exists but should not.",
            "Check if table exists or not.",
            "If target table exists and is not partitioned, it must be empty.",
            "Check whether a table exists in the default catalog.",
            "Throw an exception if the table exists on peer cluster but descriptors are not same.",
            "If the region is not set, verify that the table exists in the same region as the bucket being used."
        ],
        "useful": true
    },
    {
        "concept": "column list",
        "explanation": [
            "The PARTITIONED BY clause may be used to specify a subset of the table's partitioning columns (this column list may be empty to indicate that the index spans all partitions of the table)."
        ],
        "useful": true
    },
    {
        "concept": "root directory",
        "explanation": [
            "The root directory, \"/\", is always a directory, and cannot be overwritten by a file write operation.",
            "The root directory, \"/\", always exists, and cannot be renamed."
        ],
        "useful": true
    },
    {
        "concept": "output key",
        "explanation": [
            "The key in an output key/value pair encode two pieces of information: aggregation type and aggregation id.",
            "The map output keys of the above Map/Reduce job normally have four fields separated by \".\".",
            "In this case, the map output key will consist of fields 6, 5, 1, 2, and 3."
        ],
        "useful": true
    },
    {
        "concept": "checksum verification",
        "explanation": [
            "Checksum verification by HDFS will be internally disabled on hfile streams when this flag is set."
        ],
        "useful": true
    },
    {
        "concept": "selected directory",
        "explanation": [
            "If the selected directory does not exist, an attempt is made to create it."
        ],
        "useful": true
    },
    {
        "concept": "table access",
        "explanation": [
            "Once created, table access is via an instance of Table."
        ],
        "useful": true
    },
    {
        "concept": "AMRM",
        "explanation": [
            "Get the AMRM token of the application."
        ],
        "useful": true
    },
    {
        "concept": "jstack",
        "explanation": [
            "jstack is one of the most important tools when trying to figure out what a java process is doing apart from looking at the logs."
        ],
        "useful": true
    },
    {
        "concept": "reported values",
        "explanation": [
            "If the directory does not have a quota set, the reported values are none and inf."
        ],
        "useful": true
    },
    {
        "concept": "Trace values",
        "explanation": [
            "Trace values retrived from the database."
        ],
        "useful": false
    },
    {
        "concept": "input Paths",
        "explanation": [
            "If any input path points to an empty table or partition a dummy file in the scratch dir is instead created and added to the list."
        ],
        "useful": true
    },
    {
        "concept": "Table Creation",
        "explanation": [
            "Unpartitioned tables effectively have one default partition that must be created at table creation time."
        ],
        "useful": true
    },
    {
        "concept": "memory limits",
        "explanation": [
            "Now, different memory limits can be set for map and reduce tasks of a job, in MB."
        ],
        "useful": true
    },
    {
        "concept": "log roll",
        "explanation": [
            "Sync slots after log roll failed, abort."
        ],
        "useful": true
    },
    {
        "concept": "scratch directory",
        "explanation": [
            "If the lock is available, the scratch directory will be cleared.",
            "hive --service cleardanglingscratchdir [-r] [-v] [-s scratchdir] -r dry-run mode, which produces a list on console -v verbose mode, which prints extra debugging information The tool tests if a scratch directory is in use, and if not, will remove it."
        ],
        "useful": true
    },
    {
        "concept": "Metadata Store",
        "explanation": [
            "Metadata store URIs include a scheme that designates the backing store.",
            "The DynamoDB metadata store takes advantage of the fact that the DynamoDB service uses the same authentication mechanisms as S3.",
            "The Metadata Store to use in production is bonded to Amazon's DynamoDB database service.",
            "Object stores with cached metadata databases (for example: AWS S3 with an in-memory or a DynamoDB metadata store) may have timestamps generated from the local system clock, rather than that of the service.",
            "A DynamoDB metadata store can be initialized with additional parameters pertaining to Provisioned Throughput: Example 1."
        ],
        "useful": true
    },
    {
        "concept": "column keys",
        "explanation": [
            "While rows and column keys are expressed as bytes, the version is specified using a long integer."
        ],
        "useful": true
    },
    {
        "concept": "Retry times",
        "explanation": [
            "This places a limit even if the retry times and interval limit, combined with the backoff policy, result in a long retry period."
        ],
        "useful": true
    },
    {
        "concept": "Load Balancer",
        "explanation": [
            "The Master runs several background threads: Periodically, and when there are no regions in transition, a load balancer will run and move regions around to balance the cluster's load.",
            "The Load Balancer ensures that the region replicas are not co-hosted in the same region servers and also in the same rack (if possible)."
        ],
        "useful": true
    },
    {
        "concept": "Field numbers",
        "explanation": [
            "Field numbers are cheap and changing and reusing is not a good idea."
        ],
        "useful": true
    },
    {
        "concept": "empty key",
        "explanation": [
            "If the first character on a line is the separator, empty key is assumed, and the whole line is the value (due to a bug this was not the case)."
        ],
        "useful": true
    },
    {
        "concept": "memory compaction",
        "explanation": [
            "Called before in memory compaction started.",
            "No memory compaction, when size threshold is exceeded data is flushed to disk.",
            "In-memory compaction works best when high data churn; overwrites or over-versions can be eliminated while the data is still in memory."
        ],
        "useful": true
    },
    {
        "concept": "specified master",
        "explanation": [
            "Wait for the specified master to stop."
        ],
        "useful": false
    },
    {
        "concept": "config",
        "explanation": [
            "The conf is also passed to the file system for its configuration.",
            "If hive conf is manually created, metastore uri has to be set correctly.",
            "conf: contains the configuration files for the resource estimator service.",
            "Only configs matching any of the prefixes will be retrieved.",
            "Now when the job retires, the conf is deleted.",
            "These configs all need doc on when you'd change them.",
            "If the config is not set then default parallelism of 1 will be assumed.",
            "If confstoretrieve is specified, configs will be retrieved irrespective of whether CONFIGS is specified in fields query param or not.",
            "In cases of connection retries, conf will usually contain modified values.",
            "If that config is set, and it points to an address that is different then the RM web interface then a separate proxy server needs to be launched.",
            "This config can take values from 0 to 4.",
            "If this config is true, only pushed down filters remain in the operator tree, and the original filter is removed."
        ],
        "useful": true
    },
    {
        "concept": "RUNNING state",
        "explanation": [
            "Beyond all the groundwork that has been done in Phase 1 to ensure the persistency of application state and reload that state on recovery, Phase 2 primarily focuses on re-constructing the entire running state of YARN cluster, the majority of which is the state of the central scheduler inside RM which keeps track of all containers' life-cycle, applications' headroom and resource requests, queues' resource usage etc."
        ],
        "useful": true
    },
    {
        "concept": "INDEX",
        "explanation": [
            "An index can be dropped at any time with DROP INDEX.",
            "However, ALTER INDEX requires an index name that was created with lowercase letters (see HIVE-2752)."
        ],
        "useful": true
    },
    {
        "concept": "separate table",
        "explanation": [
            "If you leave it unset/empty, a separate table will be created for each S3 bucket you access, and that bucket's name will be used for the name of the DynamoDB table."
        ],
        "useful": true
    },
    {
        "concept": "TEZ",
        "explanation": [
            "We let Tez handle the situation.",
            "Tez allows for small datasets to be handled entirely in memory, while no such optimization is available in map-reduce.",
            "Also Tez does not restrict the job to be only Map followed by Reduce; this implies all of the query execution can be done in a single job without having to cross job boundaries."
        ],
        "useful": true
    },
    {
        "concept": "WASB",
        "explanation": [
            "WASB does not enforce this guarantee internally.",
            "WASB can operate in secure mode where the Storage access keys required to communicate with Azure storage does not have to be in the same address space as the process using WASB.",
            "WASB passes User-Agent header to the Azure back-end.",
            "The Azure Blob Storage file system (WASB) now includes optional support for use of the append API by a single writer on a path."
        ],
        "useful": true
    },
    {
        "concept": "relative path",
        "explanation": [
            "All relative paths will be resolved relative to it.",
            "Please keep in mind that all paths should be fully specified (no relative paths).",
            "Hence Hadoop path names can be one of: fully qualified URI: scheme://authority/path slash relative names: /path relative to the default file system wd-relative names: path relative to the working dir Relative paths with scheme (scheme:foo/bar) are illegal.",
            "Relative paths can be used.",
            "Relative paths path Resolves to [Y'][path]."
        ],
        "useful": true
    },
    {
        "concept": "table permission",
        "explanation": [
            "Updates the internal table permissions cache for specified table."
        ],
        "useful": true
    },
    {
        "concept": "storage locations",
        "explanation": [
            "Then, if one storage location is corrupt, you can read the metadata from one of the other storage locations.",
            "A datanode storage location /grid/dn/disk0 on DISK should be configured with [DISK]file:///grid/dn/disk0 A datanode storage location /grid/dn/archive0 on ARCHIVE should be configured with [ARCHIVE]file:///grid/dn/archive0 The default storage type of a datanode storage location will be DISK if it does not have a storage type tagged explicitly."
        ],
        "useful": true
    },
    {
        "concept": "AVG",
        "explanation": [
            "AVG uses a STRUCT with count and sum for partial aggregation data.",
            "On the other hand avg would have an evaluator only for the double type.",
            "The avg, min, or max can also be used."
        ],
        "useful": true
    },
    {
        "concept": "Alter table",
        "explanation": [
            "ALTER TABLE ADD|REPLACE COLUMNS with CASCADE command changes the columns of a table's metadata, and cascades the same change to all the partition metadata.",
            "For eg: alter table Tdependent add partition (ds='1') depends on table T1 partition (ds='1'); Something that can be achieved by external tables currently.",
            "By signaling an error via this message, the table is left in a good state and the incorrect value can be corrected with a call to alter table T set TBLPROPERTIES."
        ],
        "useful": true
    },
    {
        "concept": "sequence ID",
        "explanation": [
            "If the sequence of the last edit is greater than or equal to the sequence ID included in the file name, it is clear that all writes from the edit file have been completed."
        ],
        "useful": true
    },
    {
        "concept": "TTLs",
        "explanation": [
            "Cell TTLs are submitted as an attribute on mutation requests (Appends, Increments, Puts, etc.) using Mutation#setTTL."
        ],
        "useful": true
    },
    {
        "concept": "Retry policy",
        "explanation": [
            "If \"false\", retry policy is turned off."
        ],
        "useful": false
    },
    {
        "concept": "Procedure code",
        "explanation": [
            "Execute may be called multiple times in the case of failure or restart, so Procedure code must be idempotent yielding the same result each time it run."
        ],
        "useful": true
    },
    {
        "concept": "Task Counters",
        "explanation": [
            "Counters holds per job/task counters, defined either by the Map-Reduce framework or applications."
        ],
        "useful": true
    },
    {
        "concept": "local copies",
        "explanation": [
            "This local copy is required to display on the webui.",
            "Your local copy of Hive should work by running build/dist/bin/hive from the Hive root directory, and you should have some tables of data loaded into your local instance for testing whatever UDAF you have in mind."
        ],
        "useful": true
    },
    {
        "concept": "column reference",
        "explanation": [
            "In such cases the column references are replaced by the corresponding expression in the input data."
        ],
        "useful": true
    },
    {
        "concept": "cache value",
        "explanation": [
            "Higher caching values will enable faster scanners but will use more memory."
        ],
        "useful": true
    },
    {
        "concept": "equi",
        "explanation": [
            "Rule that merges a join with multijoin/join children if the equi compared the same set of input columns."
        ],
        "useful": false
    },
    {
        "concept": "cached block",
        "explanation": [
            "The cached blocks map is a hash map which uses chained hashing."
        ],
        "useful": true
    },
    {
        "concept": "slave cluster",
        "explanation": [
            "So if the master cluster crashes, the slave cluster may not have the newest data.",
            "When the slave cluster is finally available, the buffer is applied in the same way as during normal processing.",
            "If a slave cluster does run out of room, or is inaccessible for other reasons, it throws an error and the master retains the WAL and retries the replication at intervals.",
            "It is expected that the slave cluster has storage capacity to hold the replicated data, as well as any data it is responsible for ingesting."
        ],
        "useful": true
    },
    {
        "concept": "output column",
        "explanation": [
            "All the input parameters and output column types are string.",
            "In this variation, column information is ordered by the output column number."
        ],
        "useful": true
    },
    {
        "concept": "Output pairs",
        "explanation": [
            "Output pairs need not be of the same types as input pairs."
        ],
        "useful": true
    },
    {
        "concept": "right level",
        "explanation": [
            "The right level of parallelism for maps seems to be around 10-100 maps per-node, although it has been set up to 300 maps for very cpu-light map tasks."
        ],
        "useful": true
    },
    {
        "concept": "object store",
        "explanation": [
            "If the operation is interrupted, the object store will be in an undefined state.",
            "An object store is a data storage service, usually accessed over HTTP/HTTPS.",
            "If an object store is eventually consistent, then any operation which overwrites existing objects may not be immediately visible to all clients/queries.",
            "Some object store connectors offer an option for in-memory buffering of output \u2014for example the S3A connector.",
            "Different object store clients may support these commands: do consult the documentation and test against the target store.",
            "Smaller values results in faster test runs, especially when the object store is a long way away."
        ],
        "useful": true
    },
    {
        "concept": "encryption zone",
        "explanation": [
            "Each encryption zone is associated with a single encryption zone key which is specified when the zone is created.",
            "An encryption zone is a special directory whose contents will be transparently encrypted upon write and transparently decrypted upon read.",
            "Therefore, if the encryption zone key is compromised, it is important to identify all vulnerable files and re-encrypt them.",
            "To comply with the above rule, each encryption zone has its own .Trash directory under the \"zone directory\".",
            "When the entire encryption zone is deleted, the \"zone directory\" will be moved to the .Trash directory under the user's home directory."
        ],
        "useful": true
    },
    {
        "concept": "entire row",
        "explanation": [
            "Get whether entire row should be filtered if column is not found.",
            "If true, the entire row will be skipped if the column is not found.",
            "Otherwise, if the column is found, the entire row will be emitted only if the value passes."
        ],
        "useful": false
    },
    {
        "concept": "client scanner",
        "explanation": [
            "A client scanner for a region opened for read-only on the client side."
        ],
        "useful": true
    },
    {
        "concept": "write request",
        "explanation": [
            "If issued against a non-decommissioing data node, all current writers will be stopped, but new write requests will continue to be served.",
            "As write requests are handled by the region server, they accumulate in an in-memory storage system called the memstore."
        ],
        "useful": true
    },
    {
        "concept": "Forrest",
        "explanation": [
            "To commit documentation changes you must have Forrest installed and the forrest executable on your $PATH."
        ],
        "useful": true
    },
    {
        "concept": "connection creation",
        "explanation": [
            "Streaming connection creation, begin/commit/abort transactions, write and close has to be called in the same thread."
        ],
        "useful": true
    },
    {
        "concept": "ORDER",
        "explanation": [
            "Update 30-Dec-2009: Based on feedback in JIRA, ORDER BY is important as forward-looking to materialized views."
        ],
        "useful": false
    },
    {
        "concept": "Data capture",
        "explanation": [
            "Data capture boundaries should coincide with activity boundaries."
        ],
        "useful": true
    },
    {
        "concept": "asf",
        "explanation": [
            "The Apache Software Foundation defines generic guidelines for what it means to be a committer.",
            "See ASF board reporting for more information."
        ],
        "useful": true
    },
    {
        "concept": "mvn",
        "explanation": [
            "Clean the checkout dir $ mvn clean $ git clean -f -x -d."
        ],
        "useful": false
    },
    {
        "concept": "root value",
        "explanation": [
            "The root value is prepended to all registry paths so as to create the absolute path."
        ],
        "useful": true
    },
    {
        "concept": "table already",
        "explanation": [
            "So the attempt to create the table is made even if the table already exists.",
            "-o (Optional) Overwrites the target table for the restore if the table already exists.",
            "Throw an exception if the table already has replication enabled on any of the column families."
        ],
        "useful": false
    },
    {
        "concept": "puzzle files",
        "explanation": [
            "Puzzle files have a line per a row and columns separated by spaces."
        ],
        "useful": true
    },
    {
        "concept": "owner name",
        "explanation": [
            "If an owner is specified, that owner name overrides that of the caller."
        ],
        "useful": true
    },
    {
        "concept": "service definition",
        "explanation": [
            "A reference to this service would use the rackspaceuk service name: Because the public endpoint is used, if this service definition is used within the London datacenter, all accesses will be billed at the public upload/download rates, irrespective of where the Hadoop cluster is.",
            "This service definition is for use in a Hadoop cluster deployed within Rackspace's US infrastructure."
        ],
        "useful": true
    },
    {
        "concept": "Rm",
        "explanation": [
            "If true, then the RM will not allocate a container for the AM and start it.",
            "When a container finishes its execution at a node, the RM gets notified that there are available resources through the next NM-RM heartbeat, then the RM schedules a new container at that node, the AM gets notified through the next AM-RM heartbeat, and finally the AM launches the new container at the node.",
            "In the meantime, AM needs to re-send the outstanding resource requests to RM because RM may lose the unfulfilled requests when it shuts down.",
            "If the application stops before the flag is true then the RM may retry the application.",
            "RM set next Heartbeat interval for NM.",
            "RM drains the ATS events dispatcher when stopping.",
            "Whether the RM should enable Reservation System.",
            "The RM retains state on all running applications.",
            "While demand persists, the RM will repeat its request; applications should not interpret each message as a request for additional resources on top of previous messages.",
            "Allow the RM to create per-user regions of the registration space 1.",
            "The RM gathers this information from all nodes and determines the least loaded ones.",
            "RM will use these configurations for renewing tokens.",
            "Conversely, an RM may request a different profile of containers in subsequent requests.",
            "Get if the RM should manage the execution of the AM.",
            "If the option is disabled, the RM does not provide any registry support at all.",
            "Note that the RM may have an inconsistent view of the resources owned by the AM.",
            "RM will reload this information from state-store upon restart and re-kick the previously running applications.",
            "In this way, RM doesn't need to kill the AM and re-run the application from scratch as it is done in Phase 1.",
            "Operability Node labels and node labels mapping can be recovered across RM restart Update node labels - admin can update labels on nodes and labels on queues when RM is running.",
            "Which would mean the RM could end up giving it a lot of new allocated containers.",
            "This configuration is added for below scenario: User needs to run distcp jobs across two clusters, but the RM does not have necessary hdfs configurations to connect to the remote hdfs cluster.",
            "To determine the number of least loaded nodes that will be used when scheduling opportunistic containers and how often this list will be refreshed, we use the following parameters: As discussed in the node load rebalancing section above, at regular intervals, the RM gathers all NM queue lengths and computes their mean value (avg) and standard deviation (stdev), as well as the value avg + k*stdev (where k a float).",
            "Besides, RM also saves the credentials like security keys, tokens to work in a secure environment.",
            "The RM shall publish a Release Plan on the dev@hive list stating the branch from which they intend to make a Release Candidate, at least one week before they do so.",
            "The RM runs as a trusted user, and people visiting that web address will treat it, and links it provides to them as trusted, when in reality the AM is running as a non-trusted user, and the links it gives to the RM could point to anything malicious or otherwise."
        ],
        "useful": true
    },
    {
        "concept": "input trace",
        "explanation": [
            "Both generation of Distributed Cache files and emulation of Distributed Cache load are disabled if: input trace comes from the standard input-stream instead of file, or."
        ],
        "useful": true
    },
    {
        "concept": "Max precision",
        "explanation": [
            "Max precision guaranteed to fit into a long."
        ],
        "useful": true
    },
    {
        "concept": "heap sizes",
        "explanation": [
            "As the heap size is increased, the operation rate linearly increases."
        ],
        "useful": true
    },
    {
        "concept": "output duration",
        "explanation": [
            "Folding essentially means that the output duration of the resulting trace is fixed and job timelines are adjusted to respect the final output duration."
        ],
        "useful": true
    },
    {
        "concept": "entire cluster",
        "explanation": [
            "Thus if you have 2 regions for 16GB data, on a 20 node machine your data will be concentrated on just a few machines - nearly the entire cluster will be idle."
        ],
        "useful": false
    },
    {
        "concept": "http policy",
        "explanation": [
            "When the deprecated configuration properties are still configured, currently http policy is decided based on the following rules: 1."
        ],
        "useful": false
    },
    {
        "concept": "input split",
        "explanation": [
            "Get the input split for this map.",
            "Get the size of the split, so that the input splits can be sorted by size."
        ],
        "useful": true
    },
    {
        "concept": "Hftp",
        "explanation": [
            "Hftp changed to store the tokens as HFTP and renew them over http.",
            "Note that WebHDFS and HFTP use the HTTP port of the namenode but not the RPC port.",
            "Note that HFTP is read-only so the destination must be an HDFS filesystem.",
            "HFTP is wire-compatible even between different versions of HDFS.",
            "HFTP is a read-only filesystem, and will throw exceptions if you try to use it to write data or modify the filesystem state.",
            "HFTP can now serve a specific byte range from a file.",
            "HFTP is primarily useful if you have multiple HDFS clusters with different versions and you need to move data from one to another."
        ],
        "useful": true
    },
    {
        "concept": "input rows",
        "explanation": [
            "Map outputs table rows IF the input row has columns that have content."
        ],
        "useful": true
    },
    {
        "concept": "Httpclient",
        "explanation": [
            "HTTP POST is used for configuring the loggers.",
            "The HTTP result code, response headers, and body of a HTTP response.",
            "HttpFS can be used to access data in HDFS using HTTP utilities (such as curl and wget) and HTTP libraries Perl from other languages than Java.",
            "This HTTP Cookie has an expiration time, after which it will trigger a new authentication sequence."
        ],
        "useful": false
    },
    {
        "concept": "completion time",
        "explanation": [
            "Remove the sentinels that are marked as finished and the completion time has exceeded the removal timeout."
        ],
        "useful": true
    },
    {
        "concept": "DROP",
        "explanation": [
            "DROP TABLE removes metadata and data for this table."
        ],
        "useful": true
    },
    {
        "concept": "Row keys",
        "explanation": [
            "Rows are sorted alphabetically by the row key as they are stored.",
            "This effectively randomizes row keys, but sacrifices row ordering properties.",
            "Row keys are uninterpreted bytes.",
            "If the row key is made of arbitrary bytes, the charset ISO-8859-1 is recommended.",
            "However, poorly designed row keys are a common source of hotspotting.",
            "Your first schema is \"tall\": each row represents one value for one user, and so there are many rows in the table for each user; the row key is user + valueid, and there would be (presumably) a single column qualifier that means \"the value\"."
        ],
        "useful": true
    },
    {
        "concept": "delta files",
        "explanation": [
            "Once a delta file has been closed it cannot be reopened.",
            "If the delta file already exists, delete it.",
            "When the compactor kicks in, these delta files get rewritten into read- and storage-optimized ORC format (enable dictionary encoding, indexes and compression).",
            "An acid delta file is created for each combination partition, and bucket id (a single write id is implied)."
        ],
        "useful": true
    },
    {
        "concept": "Metrics system",
        "explanation": [
            "A default metrics system is provided to marshal metrics from sources to sinks based on (per source/sink) configuration options."
        ],
        "useful": true
    },
    {
        "concept": "table quotas",
        "explanation": [
            "When a table with a quota exists in a namespace with a quota, the table quota takes priority over the namespace quota.",
            "Because there is no table quota on 'ns1:t1', this table can grow up to 100TB, but only if 'ns1:t2' and 'ns1:t3' have a usage of zero bytes."
        ],
        "useful": true
    },
    {
        "concept": "Write number",
        "explanation": [
            "Write number and whether write has completed given out at start of a write transaction."
        ],
        "useful": true
    },
    {
        "concept": "column level",
        "explanation": [
            "In order to accurately compute the average row size, column level statistics is required."
        ],
        "useful": true
    },
    {
        "concept": "header byte",
        "explanation": [
            "It uses 0x38 as the header byte, and is terminated by 0x00 in the DESCENDING case."
        ],
        "useful": true
    },
    {
        "concept": "deleted files",
        "explanation": [
            "Furthermore, the deleted files will continue to incur storage costs.",
            "Deleted files will remain encrypted and they will be moved to a \".Trash\" subdirectory under the root of the encryption zone, prefixed by $USER/current."
        ],
        "useful": true
    },
    {
        "concept": "Hadoop distributed file system",
        "explanation": [
            "It is a synonym for hdfs dfs when HDFS is in use.",
            "HDFS is on our CLASSPATH.",
            "In a typical cluster HDFS and YARN services will be launched as the system hdfs and yarn users respectively.",
            "If HDFS is being used, hdfs dfs is a synonym.",
            "Projects that access HDFS can depend on the hadoop-hdfs-client module instead of the hadoop-hdfs module to avoid pulling in unnecessary dependency.",
            "HDFS Federation addresses this limitation by adding support for multiple Namenodes/namespaces to HDFS.",
            "HDFS access is authorized through the use of HDFS permissions.",
            "If required, HDFS could be placed in Safemode explicitly using bin/hdfs dfsadmin -safemode command.",
            "HDFS supports the fsck command to check for various inconsistencies.",
            "HDFS datanodes simply see a stream of encrypted bytes.",
            "The default HDFS location is /hbase/coprocessor.",
            "HDFS now supports the option to configure AES encryption for block data transfer.",
            "HDFS is the primary distributed storage used by Hadoop applications.",
            "HDFS supports a traditional hierarchical file organization.",
            "HDFS has to be treated as correct in its behavior.",
            "HDFS encryption is able to provide good performance and existing Hadoop applications are able to run transparently on encrypted data.",
            "HDFS does not support this.",
            "HDFS is a distributed file system that is well suited for the storage of large files.",
            "In this example, HDFS is running on the localhost at port 8020.",
            "In Hadoop v2, HDFS supports highly-available (HA) namenode services and wire compatibility.",
            "HDFS supports user quotas and access permissions.",
            "Added HDFS file access times.",
            "However, the normative specification of the behavior of this class is actually HDFS: if HDFS does not behave the way these Javadocs or the specification in the Hadoop documentations define, assume that the documentation is incorrect.",
            "In either case HDFS daemons will bind to a single IP address making the daemons unreachable from other networks.",
            "Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines.",
            "HDFS supports writing to off-heap memory managed by the Data Nodes.",
            "Previously hdfs client was using commons-logging as the logging framework.",
            "hdfs fsck <path> // only show decommission state hdfs fsck <path> -maintenance // include maintenance state \u00a9 2018 Apache Software Foundation - Privacy Policy.",
            "HDFS also has more context than traditional filesystems when it comes to making policy decisions.",
            "HDFS is designed to reliably store very large files across machines in a large cluster.",
            "By default HDFS endpoints are specified as either hostnames or IP addresses.",
            "For instance, HDFS serializes using protobuf.",
            "HDFS is highly configurable with a default configuration well suited for many installations.",
            "HDFS provides best-effort persistence guarantees for Lazy Persist Writes.",
            "HDFS stores the data on the local hard disks, avoiding network traffic if the code can be executed on that host.",
            "HDFS allows user data to be organized in the form of files and directories.",
            "HDFS does not support hard links or soft links.",
            "HDFS takes a while to mark a node as dead.",
            "Compute HDFS blocks distribution of a given file, or a portion of the file.",
            "HDFS will create a \".Trash\" subdirectory when creating a new encryption zone to support soft delete for files deleted within the encryption zone.",
            "HDFS replicates data for faster query performance.",
            "HDFS will deduct quotas from both target storage type based on storage policy and the overall space quota.",
            "HDFS append internally guarantees that only a single writer may append to a path at a given time.",
            "HDFS will no longer support upgrades from versions without CRCs for block data.",
            "HDFS supports extended attributes out of the box, without additional configuration.",
            "Instead, HDFS moves it to a trash directory (each user has its own trash directory under /user/<username>/.Trash).",
            "HDFS persists metadata (the image and edit logs) in a particular format.",
            "To support this strong guarantee without losing the flexibility of using different encryption zone keys in different parts of the filesystem, HDFS allows nested encryption zones.",
            "and HDFS will still contain references to the parent region.",
            "By default, HDFS does not mark a node as dead until it is unreachable for 630 seconds.",
            "You might consider this profile when you are intent on a simple deploy profile, the loading is light, but the HDFS where data is replicated ensures the latter.",
            "HDFS (as of this writing) writes checksums to a separate file than the data file necessitating extra seeks.",
            "HDFS now supports ACLs (Access Control Lists).",
            "However, HDFS will continue to run without any impact.",
            "Most recent deleted files are moved to the current trash directory (/user/<username>/.Trash/Current), and in a configurable interval, HDFS creates checkpoints (under /user/<username>/.Trash/<date>) for files in current trash directory and deletes old checkpoints when they are expired.",
            "HDFS will make a best effort to lazily write these files to persistent storage, however file contents may be lost at any time due to process/ node restarts, hence there is no guarantee of data durability.",
            "While HDFS meets all these requirements directly, eventually consistent object stores may not -hence these tests.",
            "While MR and HDFS are always released in sync today, they may change down the road.",
            "HDFS Snapshots are read-only point-in-time copies of the file system.",
            "That is, if it has an out of date HDFS token \u2014that token is not renewed.",
            "HDFS applications need a write-once-read-many access model for files.",
            "HDFS now can choose to append data to a new block instead of end of the last partial block."
        ],
        "useful": true
    },
    {
        "concept": "root queue",
        "explanation": [
            "All queues in the system are children of the root queue."
        ],
        "useful": true
    },
    {
        "concept": "xattr",
        "explanation": [
            "This xattr can be set and accessed by any user, assuming normal filesystem permissions.",
            "raw xattrs are preserved based solely on whether /.reserved/raw prefixes are supplied.",
            "Administrators could potentially be interested in the options limiting the number of xattrs per inode and the size of xattrs, since xattrs increase the on-disk and in-memory space consumption of an inode.",
            "This xattr does not allow a value to be set.",
            "If the xattr exists already, exception will be thrown.",
            "This xattr can only be set on files, and it will prevent the superuser from reading the file's contents.",
            "Determination of whether raw.* namespace xattrs are preserved is independent of the -p (preserve) flag.",
            "This xattr is also write-once, and cannot be removed once set."
        ],
        "useful": true
    },
    {
        "concept": "row schema",
        "explanation": [
            "The new Row Schema can only be a subset of this TS schema."
        ],
        "useful": true
    },
    {
        "concept": "MYSQL",
        "explanation": [
            "However, many DBMS's ignore this rule; for example, MySQL allows ORDER BY, but ignores it in the case where it is superceded by an ORDER BY in the query."
        ],
        "useful": true
    },
    {
        "concept": "trash directory",
        "explanation": [
            "Deleted delete/test2 We can see now that the Trash directory contains only file test1."
        ],
        "useful": true
    },
    {
        "concept": "kill request",
        "explanation": [
            "If it's already running, a kill request will be sent to it."
        ],
        "useful": true
    },
    {
        "concept": "todo",
        "explanation": [
            "TODO: remove once HBASE-11555 is fixed.",
            "TODO: need to turn on rules that's commented out and add more if necessary.",
            "TODO: rename files \"execute\" option may be supplied in both modes to have the utility automatically execute the equivalent of the generated commands \"location\" option may be supplied followed by a path to set the location for the generated scripts.",
            "TODO Currently List type only support non nested case.",
            "TODO: this method is relied upon by custom input formats to set jobconf properties.",
            "The client should only hold one open transaction at any given time (TODO: enforce this)."
        ],
        "useful": false
    },
    {
        "concept": "Tablename",
        "explanation": [
            "hive> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING); creates a table called invites with two columns and a partition column called ds.",
            "DROP TABLE removes metadata and data for this table.",
            "ALTER TABLE ADD|REPLACE COLUMNS with CASCADE command changes the columns of a table's metadata, and cascades the same change to all the partition metadata.",
            "DB and TABLENAME are DOT-separated."
        ],
        "useful": true
    },
    {
        "concept": "NLM",
        "explanation": [
            "NLM is not supported so mount option \"nolock\" is needed."
        ],
        "useful": true
    },
    {
        "concept": "mmaped",
        "explanation": [
            "mmaped files also use virtual address space."
        ],
        "useful": true
    },
    {
        "concept": "skewed table",
        "explanation": [
            "The table will be a skewed table."
        ],
        "useful": false
    },
    {
        "concept": "network topology",
        "explanation": [
            "The network topology of the cluster would determine the number of components in the network path."
        ],
        "useful": true
    },
    {
        "concept": "end store",
        "explanation": [
            "The back end store is responsible for maintaining and persisting metadata about the shared cache."
        ],
        "useful": true
    },
    {
        "concept": "specified lock",
        "explanation": [
            "Called when the specified lock has failed."
        ],
        "useful": false
    },
    {
        "concept": "metric names",
        "explanation": [
            "All metrics names start with capitals."
        ],
        "useful": true
    },
    {
        "concept": "time errors",
        "explanation": [
            "If a compile time or run time error occurs that appears related to vectorization, please file a Hive JIRA."
        ],
        "useful": true
    },
    {
        "concept": "table integrity",
        "explanation": [
            "Table integrity problems can require repairs that deal with overlaps."
        ],
        "useful": true
    },
    {
        "concept": "Corrupted Files",
        "explanation": [
            "Corrupt files should probably be removed."
        ],
        "useful": true
    },
    {
        "concept": "optimal value",
        "explanation": [
            "The optimal value depends on the jobs characteristics, the cluster configuration and the target utilization."
        ],
        "useful": true
    },
    {
        "concept": "output recovery",
        "explanation": [
            "If task output recovery is supported, job restart can be done more efficiently."
        ],
        "useful": true
    },
    {
        "concept": "Key Version",
        "explanation": [
            "Because keys can be rolled, a key can have multiple key versions, where each key version has its own key material (the actual secret bytes used during encryption and decryption)."
        ],
        "useful": true
    },
    {
        "concept": "arbitrary names",
        "explanation": [
            "There is no notion of a directory; arbitrary names can be assigned to objects \u2014 within the limitations of the naming scheme imposed by the service's provider."
        ],
        "useful": true
    },
    {
        "concept": "Table table",
        "explanation": [
            "Error \"DynamoDB table TABLE does not exist in region REGION; auto-creation is turned off\"."
        ],
        "useful": false
    }
]